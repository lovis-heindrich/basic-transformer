{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlovisheindrich\u001b[0m (\u001b[33mlovis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.model import Transformer, TransformerConfig\n",
    "from src.load_data import load_data, download_data, create_word_dicts, create_dataset\n",
    "from src.train import train, eval\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "from tqdm.notebook import trange\n",
    "import os\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1195401 words\n",
      "6083 words that occur >= 10 times\n",
      "22188 words that occur < 10 times\n",
      "Vocabulary size: 6084\n"
     ]
    }
   ],
   "source": [
    "download_data()\n",
    "words = load_data()\n",
    "word_to_index, index_to_word = create_word_dicts(words, min_occurrences=10)\n",
    "\n",
    "data = {\n",
    "    \"words\": words,\n",
    "    \"word_to_index\": word_to_index,\n",
    "    \"index_to_word\": index_to_word\n",
    "}\n",
    "\n",
    "# Initialize run data directory\n",
    "run_data_path = \"./run_data\"\n",
    "if os.path.exists(run_data_path):\n",
    "    shutil.rmtree(run_data_path)\n",
    "os.mkdir(run_data_path)\n",
    "os.mkdir(run_data_path+\"/checkpoints\")\n",
    "\n",
    "with open(run_data_path+\"/word_data.json\", 'w') as outfile:\n",
    "    outfile.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\heind\\Documents\\workspace\\basic-transformer\\wandb\\run-20230420_163906-sbhf1168</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lovis/basic-transformer/runs/sbhf1168' target=\"_blank\">usual-dragon-11</a></strong> to <a href='https://wandb.ai/lovis/basic-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lovis/basic-transformer' target=\"_blank\">https://wandb.ai/lovis/basic-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lovis/basic-transformer/runs/sbhf1168' target=\"_blank\">https://wandb.ai/lovis/basic-transformer/runs/sbhf1168</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens per batch 1920\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": len(index_to_word),\n",
    "    \"max_input_length\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_size\": 128,\n",
    "    \"num_blocks\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_epochs\": 200,\n",
    "    \"val_split\": 0.2,\n",
    "    \"warmup_steps\": 4000,\n",
    "    \"lr_scale\": 1\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"basic-transformer\",\n",
    "    config=config,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "model_artifact = wandb.Artifact('models', 'model')\n",
    "word_artifact = wandb.Artifact('word_dicts', 'dataset')\n",
    "word_artifact.add_file(local_path=run_data_path+\"/word_data.json\")\n",
    "wandb.log_artifact(word_artifact)\n",
    "\n",
    "with open(run_data_path+\"/config.json\", 'w') as outfile:\n",
    "    outfile.write(json.dumps(config))\n",
    "\n",
    "# Paper used 25000 tokens per batch\n",
    "print(\"Total tokens per batch\", config[\"batch_size\"]*config[\"max_input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1573632\n",
      "Total training steps: 99800\n"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl = create_dataset(words, word_to_index, index_to_word, batch_size=config[\"batch_size\"], val_split=config[\"val_split\"], max_input_length=config[\"max_input_length\"])\n",
    "\n",
    "transformer = Transformer(TransformerConfig(vocab_size=config[\"vocabulary_size\"], max_input_length=config[\"max_input_length\"], num_heads=config[\"num_heads\"], num_blocks=config[\"num_blocks\"], embedding_size=config[\"embedding_size\"]), apply_softmax=False)\n",
    "transformer.to(device)\n",
    "wandb.watch(transformer, log_freq=1000)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "steps_per_epoch = len(train_dl)\n",
    "\n",
    "def transformer_lr(step, d_model=config[\"embedding_size\"], warmup_steps=config[\"warmup_steps\"], lr_scale=config[\"lr_scale\"]):\n",
    "    if step==0:\n",
    "        return transformer_lr(1, d_model, warmup_steps)\n",
    "    return lr_scale*((d_model) ** -0.5)*min(step**-0.5, step*(warmup_steps**-1.5))\n",
    "\n",
    "initial_lr = transformer_lr(1)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "lr_per_epoch = lambda epoch: transformer_lr(epoch*steps_per_epoch) / initial_lr\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_per_epoch)\n",
    "\n",
    "transformer_params = 0\n",
    "for param in transformer.parameters():\n",
    "    transformer_params += param.nelement()\n",
    "print(\"Total parameters:\", transformer_params)\n",
    "print(\"Total training steps:\", steps_per_epoch*config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_schedule():\n",
    "    optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_per_epoch)\n",
    "    lrs = []\n",
    "    for i in range(300):\n",
    "        lrs.append(lr_scheduler.optimizer.param_groups[0]['lr'])\n",
    "        lr_scheduler.step()\n",
    "    import seaborn as sns\n",
    "    print(initial_lr, max(lrs))\n",
    "    sns.lineplot(lrs)\n",
    "\n",
    "#plot_lr_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_epochs, start_epoch=0, model_checkpoint_freq=100):\n",
    "    for e in trange(num_epochs):\n",
    "\n",
    "        train(transformer, loss_fn=loss_fn, optim=optim, device=device, dl=train_dl)\n",
    "\n",
    "        train_loss, train_acc = eval(transformer=transformer, loss_fn=loss_fn, device=device, dl=train_dl)\n",
    "        val_loss, val_acc = eval(transformer=transformer, loss_fn=loss_fn, device=device, dl=val_dl)\n",
    "\n",
    "        lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "        step = (e+start_epoch)*steps_per_epoch\n",
    "        print(f\"\\nEpoch {e+start_epoch}, lr = {lr:.6f}\")\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Training loss {train_loss:.4f}, accuracy {train_acc:.4f}\")\n",
    "        print(f\"Eval loss {val_loss:.4f}, accuracy {val_acc:.4f}\")\n",
    "        if (e+1)%model_checkpoint_freq == 0:\n",
    "            checkpoint_name = f\"/checkpoints/checkpoint_{e+start_epoch+1}.pt\"\n",
    "            torch.save(transformer.state_dict(), run_data_path + checkpoint_name)\n",
    "            model_artifact.add_file(run_data_path + checkpoint_name, name =checkpoint_name)\n",
    "        wandb.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, \"learning_rate\": lr, \"step\": step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f8de7019dd42188e6eb784cbae3b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, lr = 0.000000\n",
      "Training loss 37.4343, accuracy 0.0002\n",
      "Eval loss 37.4371, accuracy 0.0002\n",
      "\n",
      "Epoch 1, lr = 0.000174\n",
      "Training loss 9.6564, accuracy 0.0780\n",
      "Eval loss 9.6541, accuracy 0.0777\n",
      "\n",
      "Epoch 2, lr = 0.000349\n",
      "Training loss 6.3628, accuracy 0.1036\n",
      "Eval loss 6.3717, accuracy 0.1033\n",
      "\n",
      "Epoch 3, lr = 0.000523\n",
      "Training loss 5.8232, accuracy 0.1249\n",
      "Eval loss 5.8338, accuracy 0.1250\n",
      "\n",
      "Epoch 4, lr = 0.000697\n",
      "Training loss 5.6627, accuracy 0.1316\n",
      "Eval loss 5.6874, accuracy 0.1319\n",
      "\n",
      "Epoch 5, lr = 0.000872\n",
      "Training loss 5.4346, accuracy 0.1505\n",
      "Eval loss 5.4685, accuracy 0.1489\n",
      "\n",
      "Epoch 6, lr = 0.001046\n",
      "Training loss 5.2375, accuracy 0.1706\n",
      "Eval loss 5.2765, accuracy 0.1680\n",
      "\n",
      "Epoch 7, lr = 0.001220\n",
      "Training loss 5.0944, accuracy 0.1735\n",
      "Eval loss 5.1452, accuracy 0.1706\n",
      "\n",
      "Epoch 8, lr = 0.001395\n",
      "Training loss 4.9693, accuracy 0.1804\n",
      "Eval loss 5.0302, accuracy 0.1764\n",
      "\n",
      "Epoch 9, lr = 0.001319\n",
      "Training loss 4.8585, accuracy 0.1923\n",
      "Eval loss 4.9327, accuracy 0.1875\n",
      "\n",
      "Epoch 10, lr = 0.001251\n",
      "Training loss 4.7890, accuracy 0.1969\n",
      "Eval loss 4.8780, accuracy 0.1912\n",
      "\n",
      "Epoch 11, lr = 0.001193\n",
      "Training loss 4.7291, accuracy 0.2009\n",
      "Eval loss 4.8264, accuracy 0.1945\n",
      "\n",
      "Epoch 12, lr = 0.001142\n",
      "Training loss 4.6900, accuracy 0.2035\n",
      "Eval loss 4.8021, accuracy 0.1952\n",
      "\n",
      "Epoch 13, lr = 0.001097\n",
      "Training loss 4.6447, accuracy 0.2073\n",
      "Eval loss 4.7624, accuracy 0.1987\n",
      "\n",
      "Epoch 14, lr = 0.001058\n",
      "Training loss 4.6139, accuracy 0.2091\n",
      "Eval loss 4.7438, accuracy 0.2007\n",
      "\n",
      "Epoch 15, lr = 0.001022\n",
      "Training loss 4.5774, accuracy 0.2109\n",
      "Eval loss 4.7215, accuracy 0.2005\n",
      "\n",
      "Epoch 16, lr = 0.000989\n",
      "Training loss 4.5517, accuracy 0.2140\n",
      "Eval loss 4.7055, accuracy 0.2028\n",
      "\n",
      "Epoch 17, lr = 0.000960\n",
      "Training loss 4.5223, accuracy 0.2155\n",
      "Eval loss 4.6878, accuracy 0.2038\n",
      "\n",
      "Epoch 18, lr = 0.000933\n",
      "Training loss 4.4951, accuracy 0.2178\n",
      "Eval loss 4.6775, accuracy 0.2047\n",
      "\n",
      "Epoch 19, lr = 0.000908\n",
      "Training loss 4.4737, accuracy 0.2181\n",
      "Eval loss 4.6639, accuracy 0.2053\n",
      "\n",
      "Epoch 20, lr = 0.000885\n",
      "Training loss 4.4560, accuracy 0.2205\n",
      "Eval loss 4.6606, accuracy 0.2053\n",
      "\n",
      "Epoch 21, lr = 0.000863\n",
      "Training loss 4.4327, accuracy 0.2230\n",
      "Eval loss 4.6379, accuracy 0.2073\n",
      "\n",
      "Epoch 22, lr = 0.000844\n",
      "Training loss 4.4164, accuracy 0.2236\n",
      "Eval loss 4.6371, accuracy 0.2066\n",
      "\n",
      "Epoch 23, lr = 0.000825\n",
      "Training loss 4.3927, accuracy 0.2260\n",
      "Eval loss 4.6242, accuracy 0.2079\n",
      "\n",
      "Epoch 24, lr = 0.000808\n",
      "Training loss 4.3753, accuracy 0.2268\n",
      "Eval loss 4.6170, accuracy 0.2084\n",
      "\n",
      "Epoch 25, lr = 0.000791\n",
      "Training loss 4.3596, accuracy 0.2279\n",
      "Eval loss 4.6118, accuracy 0.2092\n",
      "\n",
      "Epoch 26, lr = 0.000776\n",
      "Training loss 4.3457, accuracy 0.2292\n",
      "Eval loss 4.6016, accuracy 0.2089\n",
      "\n",
      "Epoch 27, lr = 0.000761\n",
      "Training loss 4.3273, accuracy 0.2310\n",
      "Eval loss 4.5939, accuracy 0.2106\n",
      "\n",
      "Epoch 28, lr = 0.000748\n",
      "Training loss 4.3174, accuracy 0.2316\n",
      "Eval loss 4.5919, accuracy 0.2108\n",
      "\n",
      "Epoch 29, lr = 0.000735\n",
      "Training loss 4.3059, accuracy 0.2322\n",
      "Eval loss 4.5846, accuracy 0.2108\n",
      "\n",
      "Epoch 30, lr = 0.000722\n",
      "Training loss 4.2871, accuracy 0.2341\n",
      "Eval loss 4.5816, accuracy 0.2113\n",
      "\n",
      "Epoch 31, lr = 0.000711\n",
      "Training loss 4.2787, accuracy 0.2349\n",
      "Eval loss 4.5753, accuracy 0.2116\n",
      "\n",
      "Epoch 32, lr = 0.000699\n",
      "Training loss 4.2638, accuracy 0.2359\n",
      "Eval loss 4.5704, accuracy 0.2121\n",
      "\n",
      "Epoch 33, lr = 0.000689\n",
      "Training loss 4.2548, accuracy 0.2363\n",
      "Eval loss 4.5681, accuracy 0.2122\n",
      "\n",
      "Epoch 34, lr = 0.000679\n",
      "Training loss 4.2425, accuracy 0.2367\n",
      "Eval loss 4.5657, accuracy 0.2123\n",
      "\n",
      "Epoch 35, lr = 0.000669\n",
      "Training loss 4.2281, accuracy 0.2384\n",
      "Eval loss 4.5613, accuracy 0.2125\n",
      "\n",
      "Epoch 36, lr = 0.000659\n",
      "Training loss 4.2199, accuracy 0.2396\n",
      "Eval loss 4.5523, accuracy 0.2139\n",
      "\n",
      "Epoch 37, lr = 0.000650\n",
      "Training loss 4.2085, accuracy 0.2400\n",
      "Eval loss 4.5568, accuracy 0.2121\n",
      "\n",
      "Epoch 38, lr = 0.000642\n",
      "Training loss 4.2014, accuracy 0.2408\n",
      "Eval loss 4.5588, accuracy 0.2133\n",
      "\n",
      "Epoch 39, lr = 0.000634\n",
      "Training loss 4.1924, accuracy 0.2421\n",
      "Eval loss 4.5485, accuracy 0.2126\n",
      "\n",
      "Epoch 40, lr = 0.000626\n",
      "Training loss 4.1827, accuracy 0.2425\n",
      "Eval loss 4.5504, accuracy 0.2124\n",
      "\n",
      "Epoch 41, lr = 0.000618\n",
      "Training loss 4.1728, accuracy 0.2423\n",
      "Eval loss 4.5479, accuracy 0.2132\n",
      "\n",
      "Epoch 42, lr = 0.000611\n",
      "Training loss 4.1623, accuracy 0.2444\n",
      "Eval loss 4.5439, accuracy 0.2137\n",
      "\n",
      "Epoch 43, lr = 0.000603\n",
      "Training loss 4.1529, accuracy 0.2450\n",
      "Eval loss 4.5438, accuracy 0.2136\n",
      "\n",
      "Epoch 44, lr = 0.000597\n",
      "Training loss 4.1465, accuracy 0.2454\n",
      "Eval loss 4.5397, accuracy 0.2139\n",
      "\n",
      "Epoch 45, lr = 0.000590\n",
      "Training loss 4.1405, accuracy 0.2462\n",
      "Eval loss 4.5361, accuracy 0.2143\n",
      "\n",
      "Epoch 46, lr = 0.000583\n",
      "Training loss 4.1333, accuracy 0.2462\n",
      "Eval loss 4.5357, accuracy 0.2144\n",
      "\n",
      "Epoch 47, lr = 0.000577\n",
      "Training loss 4.1244, accuracy 0.2476\n",
      "Eval loss 4.5370, accuracy 0.2141\n",
      "\n",
      "Epoch 48, lr = 0.000571\n",
      "Training loss 4.1156, accuracy 0.2477\n",
      "Eval loss 4.5346, accuracy 0.2151\n",
      "\n",
      "Epoch 49, lr = 0.000565\n",
      "Training loss 4.1102, accuracy 0.2481\n",
      "Eval loss 4.5367, accuracy 0.2135\n",
      "\n",
      "Epoch 50, lr = 0.000560\n",
      "Training loss 4.1011, accuracy 0.2493\n",
      "Eval loss 4.5334, accuracy 0.2148\n",
      "\n",
      "Epoch 51, lr = 0.000554\n",
      "Training loss 4.0941, accuracy 0.2499\n",
      "Eval loss 4.5346, accuracy 0.2145\n",
      "\n",
      "Epoch 52, lr = 0.000549\n",
      "Training loss 4.0875, accuracy 0.2505\n",
      "Eval loss 4.5318, accuracy 0.2153\n",
      "\n",
      "Epoch 53, lr = 0.000544\n",
      "Training loss 4.0868, accuracy 0.2499\n",
      "Eval loss 4.5298, accuracy 0.2144\n",
      "\n",
      "Epoch 54, lr = 0.000538\n",
      "Training loss 4.0758, accuracy 0.2515\n",
      "Eval loss 4.5301, accuracy 0.2160\n",
      "\n",
      "Epoch 55, lr = 0.000534\n",
      "Training loss 4.0673, accuracy 0.2522\n",
      "Eval loss 4.5251, accuracy 0.2151\n",
      "\n",
      "Epoch 56, lr = 0.000529\n",
      "Training loss 4.0627, accuracy 0.2521\n",
      "Eval loss 4.5259, accuracy 0.2157\n",
      "\n",
      "Epoch 57, lr = 0.000524\n",
      "Training loss 4.0585, accuracy 0.2525\n",
      "Eval loss 4.5322, accuracy 0.2146\n",
      "\n",
      "Epoch 58, lr = 0.000520\n",
      "Training loss 4.0514, accuracy 0.2535\n",
      "Eval loss 4.5269, accuracy 0.2153\n",
      "\n",
      "Epoch 59, lr = 0.000515\n",
      "Training loss 4.0461, accuracy 0.2536\n",
      "Eval loss 4.5303, accuracy 0.2158\n",
      "\n",
      "Epoch 60, lr = 0.000511\n",
      "Training loss 4.0414, accuracy 0.2541\n",
      "Eval loss 4.5256, accuracy 0.2156\n",
      "\n",
      "Epoch 61, lr = 0.000507\n",
      "Training loss 4.0343, accuracy 0.2547\n",
      "Eval loss 4.5267, accuracy 0.2160\n",
      "\n",
      "Epoch 62, lr = 0.000503\n",
      "Training loss 4.0315, accuracy 0.2551\n",
      "Eval loss 4.5236, accuracy 0.2154\n",
      "\n",
      "Epoch 63, lr = 0.000499\n",
      "Training loss 4.0270, accuracy 0.2555\n",
      "Eval loss 4.5170, accuracy 0.2160\n",
      "\n",
      "Epoch 64, lr = 0.000495\n",
      "Training loss 4.0234, accuracy 0.2557\n",
      "Eval loss 4.5190, accuracy 0.2155\n",
      "\n",
      "Epoch 65, lr = 0.000491\n",
      "Training loss 4.0135, accuracy 0.2561\n",
      "Eval loss 4.5208, accuracy 0.2156\n",
      "\n",
      "Epoch 66, lr = 0.000487\n",
      "Training loss 4.0091, accuracy 0.2565\n",
      "Eval loss 4.5257, accuracy 0.2163\n",
      "\n",
      "Epoch 67, lr = 0.000483\n",
      "Training loss 4.0051, accuracy 0.2569\n",
      "Eval loss 4.5242, accuracy 0.2153\n",
      "\n",
      "Epoch 68, lr = 0.000480\n",
      "Training loss 4.0004, accuracy 0.2574\n",
      "Eval loss 4.5189, accuracy 0.2158\n",
      "\n",
      "Epoch 69, lr = 0.000476\n",
      "Training loss 3.9961, accuracy 0.2578\n",
      "Eval loss 4.5246, accuracy 0.2162\n",
      "\n",
      "Epoch 70, lr = 0.000473\n",
      "Training loss 3.9884, accuracy 0.2582\n",
      "Eval loss 4.5260, accuracy 0.2161\n",
      "\n",
      "Epoch 71, lr = 0.000470\n",
      "Training loss 3.9851, accuracy 0.2590\n",
      "Eval loss 4.5185, accuracy 0.2167\n",
      "\n",
      "Epoch 72, lr = 0.000466\n",
      "Training loss 3.9805, accuracy 0.2591\n",
      "Eval loss 4.5261, accuracy 0.2160\n",
      "\n",
      "Epoch 73, lr = 0.000463\n",
      "Training loss 3.9787, accuracy 0.2592\n",
      "Eval loss 4.5294, accuracy 0.2155\n",
      "\n",
      "Epoch 74, lr = 0.000460\n",
      "Training loss 3.9700, accuracy 0.2604\n",
      "Eval loss 4.5214, accuracy 0.2162\n",
      "\n",
      "Epoch 75, lr = 0.000457\n",
      "Training loss 3.9638, accuracy 0.2604\n",
      "Eval loss 4.5257, accuracy 0.2163\n",
      "\n",
      "Epoch 76, lr = 0.000454\n",
      "Training loss 3.9615, accuracy 0.2608\n",
      "Eval loss 4.5202, accuracy 0.2162\n",
      "\n",
      "Epoch 77, lr = 0.000451\n",
      "Training loss 3.9556, accuracy 0.2615\n",
      "Eval loss 4.5216, accuracy 0.2158\n",
      "\n",
      "Epoch 78, lr = 0.000448\n",
      "Training loss 3.9541, accuracy 0.2612\n",
      "Eval loss 4.5289, accuracy 0.2163\n",
      "\n",
      "Epoch 79, lr = 0.000445\n",
      "Training loss 3.9496, accuracy 0.2622\n",
      "Eval loss 4.5213, accuracy 0.2163\n",
      "\n",
      "Epoch 80, lr = 0.000442\n",
      "Training loss 3.9456, accuracy 0.2622\n",
      "Eval loss 4.5207, accuracy 0.2170\n",
      "\n",
      "Epoch 81, lr = 0.000440\n",
      "Training loss 3.9409, accuracy 0.2631\n",
      "Eval loss 4.5222, accuracy 0.2161\n",
      "\n",
      "Epoch 82, lr = 0.000437\n",
      "Training loss 3.9379, accuracy 0.2629\n",
      "Eval loss 4.5255, accuracy 0.2169\n",
      "\n",
      "Epoch 83, lr = 0.000434\n",
      "Training loss 3.9337, accuracy 0.2635\n",
      "Eval loss 4.5225, accuracy 0.2164\n",
      "\n",
      "Epoch 84, lr = 0.000432\n",
      "Training loss 3.9287, accuracy 0.2641\n",
      "Eval loss 4.5234, accuracy 0.2162\n",
      "\n",
      "Epoch 85, lr = 0.000429\n",
      "Training loss 3.9269, accuracy 0.2636\n",
      "Eval loss 4.5255, accuracy 0.2160\n",
      "\n",
      "Epoch 86, lr = 0.000427\n",
      "Training loss 3.9230, accuracy 0.2637\n",
      "Eval loss 4.5219, accuracy 0.2163\n",
      "\n",
      "Epoch 87, lr = 0.000424\n",
      "Training loss 3.9205, accuracy 0.2645\n",
      "Eval loss 4.5176, accuracy 0.2166\n",
      "\n",
      "Epoch 88, lr = 0.000422\n",
      "Training loss 3.9168, accuracy 0.2645\n",
      "Eval loss 4.5255, accuracy 0.2156\n",
      "\n",
      "Epoch 89, lr = 0.000419\n",
      "Training loss 3.9115, accuracy 0.2652\n",
      "Eval loss 4.5233, accuracy 0.2154\n",
      "\n",
      "Epoch 90, lr = 0.000417\n",
      "Training loss 3.9056, accuracy 0.2660\n",
      "Eval loss 4.5256, accuracy 0.2162\n",
      "\n",
      "Epoch 91, lr = 0.000415\n",
      "Training loss 3.9021, accuracy 0.2659\n",
      "Eval loss 4.5259, accuracy 0.2161\n",
      "\n",
      "Epoch 92, lr = 0.000413\n",
      "Training loss 3.9011, accuracy 0.2659\n",
      "Eval loss 4.5178, accuracy 0.2176\n",
      "\n",
      "Epoch 93, lr = 0.000410\n",
      "Training loss 3.8950, accuracy 0.2662\n",
      "Eval loss 4.5228, accuracy 0.2168\n",
      "\n",
      "Epoch 94, lr = 0.000408\n",
      "Training loss 3.8940, accuracy 0.2661\n",
      "Eval loss 4.5266, accuracy 0.2169\n",
      "\n",
      "Epoch 95, lr = 0.000406\n",
      "Training loss 3.8882, accuracy 0.2667\n",
      "Eval loss 4.5311, accuracy 0.2164\n",
      "\n",
      "Epoch 96, lr = 0.000404\n",
      "Training loss 3.8847, accuracy 0.2676\n",
      "Eval loss 4.5259, accuracy 0.2166\n",
      "\n",
      "Epoch 97, lr = 0.000402\n",
      "Training loss 3.8837, accuracy 0.2675\n",
      "Eval loss 4.5253, accuracy 0.2164\n",
      "\n",
      "Epoch 98, lr = 0.000400\n",
      "Training loss 3.8803, accuracy 0.2677\n",
      "Eval loss 4.5214, accuracy 0.2169\n",
      "\n",
      "Epoch 99, lr = 0.000398\n",
      "Training loss 3.8741, accuracy 0.2684\n",
      "Eval loss 4.5285, accuracy 0.2160\n",
      "\n",
      "Epoch 100, lr = 0.000396\n",
      "Training loss 3.8744, accuracy 0.2689\n",
      "Eval loss 4.5234, accuracy 0.2162\n",
      "\n",
      "Epoch 101, lr = 0.000394\n",
      "Training loss 3.8670, accuracy 0.2692\n",
      "Eval loss 4.5300, accuracy 0.2160\n",
      "\n",
      "Epoch 102, lr = 0.000392\n",
      "Training loss 3.8692, accuracy 0.2690\n",
      "Eval loss 4.5274, accuracy 0.2157\n",
      "\n",
      "Epoch 103, lr = 0.000390\n",
      "Training loss 3.8632, accuracy 0.2694\n",
      "Eval loss 4.5240, accuracy 0.2162\n",
      "\n",
      "Epoch 104, lr = 0.000388\n",
      "Training loss 3.8610, accuracy 0.2694\n",
      "Eval loss 4.5242, accuracy 0.2174\n",
      "\n",
      "Epoch 105, lr = 0.000386\n",
      "Training loss 3.8544, accuracy 0.2703\n",
      "Eval loss 4.5271, accuracy 0.2166\n",
      "\n",
      "Epoch 106, lr = 0.000384\n",
      "Training loss 3.8545, accuracy 0.2693\n",
      "Eval loss 4.5285, accuracy 0.2163\n",
      "\n",
      "Epoch 107, lr = 0.000383\n",
      "Training loss 3.8539, accuracy 0.2699\n",
      "Eval loss 4.5214, accuracy 0.2166\n",
      "\n",
      "Epoch 108, lr = 0.000381\n",
      "Training loss 3.8494, accuracy 0.2706\n",
      "Eval loss 4.5274, accuracy 0.2162\n",
      "\n",
      "Epoch 109, lr = 0.000379\n",
      "Training loss 3.8468, accuracy 0.2703\n",
      "Eval loss 4.5283, accuracy 0.2165\n",
      "\n",
      "Epoch 110, lr = 0.000377\n",
      "Training loss 3.8395, accuracy 0.2713\n",
      "Eval loss 4.5342, accuracy 0.2165\n",
      "\n",
      "Epoch 111, lr = 0.000376\n",
      "Training loss 3.8414, accuracy 0.2714\n",
      "Eval loss 4.5232, accuracy 0.2169\n",
      "\n",
      "Epoch 112, lr = 0.000374\n",
      "Training loss 3.8378, accuracy 0.2716\n",
      "Eval loss 4.5284, accuracy 0.2156\n",
      "\n",
      "Epoch 113, lr = 0.000372\n",
      "Training loss 3.8326, accuracy 0.2717\n",
      "Eval loss 4.5270, accuracy 0.2163\n",
      "\n",
      "Epoch 114, lr = 0.000371\n",
      "Training loss 3.8304, accuracy 0.2722\n",
      "Eval loss 4.5287, accuracy 0.2163\n",
      "\n",
      "Epoch 115, lr = 0.000369\n",
      "Training loss 3.8300, accuracy 0.2717\n",
      "Eval loss 4.5326, accuracy 0.2158\n",
      "\n",
      "Epoch 116, lr = 0.000367\n",
      "Training loss 3.8236, accuracy 0.2728\n",
      "Eval loss 4.5348, accuracy 0.2160\n",
      "\n",
      "Epoch 117, lr = 0.000366\n",
      "Training loss 3.8215, accuracy 0.2731\n",
      "Eval loss 4.5355, accuracy 0.2155\n",
      "\n",
      "Epoch 118, lr = 0.000364\n",
      "Training loss 3.8203, accuracy 0.2734\n",
      "Eval loss 4.5286, accuracy 0.2158\n",
      "\n",
      "Epoch 119, lr = 0.000363\n",
      "Training loss 3.8168, accuracy 0.2732\n",
      "Eval loss 4.5361, accuracy 0.2161\n",
      "\n",
      "Epoch 120, lr = 0.000361\n",
      "Training loss 3.8130, accuracy 0.2732\n",
      "Eval loss 4.5399, accuracy 0.2161\n",
      "\n",
      "Epoch 121, lr = 0.000360\n",
      "Training loss 3.8139, accuracy 0.2734\n",
      "Eval loss 4.5247, accuracy 0.2172\n",
      "\n",
      "Epoch 122, lr = 0.000358\n",
      "Training loss 3.8075, accuracy 0.2745\n",
      "Eval loss 4.5366, accuracy 0.2153\n",
      "\n",
      "Epoch 123, lr = 0.000357\n",
      "Training loss 3.8051, accuracy 0.2743\n",
      "Eval loss 4.5402, accuracy 0.2156\n",
      "\n",
      "Epoch 124, lr = 0.000355\n",
      "Training loss 3.8036, accuracy 0.2743\n",
      "Eval loss 4.5390, accuracy 0.2159\n",
      "\n",
      "Epoch 125, lr = 0.000354\n",
      "Training loss 3.8027, accuracy 0.2744\n",
      "Eval loss 4.5347, accuracy 0.2159\n",
      "\n",
      "Epoch 126, lr = 0.000353\n",
      "Training loss 3.8004, accuracy 0.2747\n",
      "Eval loss 4.5357, accuracy 0.2163\n",
      "\n",
      "Epoch 127, lr = 0.000351\n",
      "Training loss 3.7960, accuracy 0.2749\n",
      "Eval loss 4.5357, accuracy 0.2163\n",
      "\n",
      "Epoch 128, lr = 0.000350\n",
      "Training loss 3.7946, accuracy 0.2750\n",
      "Eval loss 4.5350, accuracy 0.2153\n",
      "\n",
      "Epoch 129, lr = 0.000348\n",
      "Training loss 3.7905, accuracy 0.2748\n",
      "Eval loss 4.5406, accuracy 0.2167\n",
      "\n",
      "Epoch 130, lr = 0.000347\n",
      "Training loss 3.7925, accuracy 0.2755\n",
      "Eval loss 4.5327, accuracy 0.2164\n",
      "\n",
      "Epoch 131, lr = 0.000346\n",
      "Training loss 3.7871, accuracy 0.2758\n",
      "Eval loss 4.5343, accuracy 0.2170\n",
      "\n",
      "Epoch 132, lr = 0.000344\n",
      "Training loss 3.7835, accuracy 0.2762\n",
      "Eval loss 4.5362, accuracy 0.2161\n",
      "\n",
      "Epoch 133, lr = 0.000343\n",
      "Training loss 3.7809, accuracy 0.2765\n",
      "Eval loss 4.5397, accuracy 0.2160\n",
      "\n",
      "Epoch 134, lr = 0.000342\n",
      "Training loss 3.7781, accuracy 0.2767\n",
      "Eval loss 4.5374, accuracy 0.2159\n",
      "\n",
      "Epoch 135, lr = 0.000341\n",
      "Training loss 3.7759, accuracy 0.2766\n",
      "Eval loss 4.5379, accuracy 0.2161\n",
      "\n",
      "Epoch 136, lr = 0.000339\n",
      "Training loss 3.7750, accuracy 0.2767\n",
      "Eval loss 4.5349, accuracy 0.2170\n",
      "\n",
      "Epoch 137, lr = 0.000338\n",
      "Training loss 3.7698, accuracy 0.2769\n",
      "Eval loss 4.5462, accuracy 0.2157\n",
      "\n",
      "Epoch 138, lr = 0.000337\n",
      "Training loss 3.7690, accuracy 0.2769\n",
      "Eval loss 4.5441, accuracy 0.2167\n",
      "\n",
      "Epoch 139, lr = 0.000336\n",
      "Training loss 3.7652, accuracy 0.2773\n",
      "Eval loss 4.5449, accuracy 0.2160\n",
      "\n",
      "Epoch 140, lr = 0.000334\n",
      "Training loss 3.7669, accuracy 0.2775\n",
      "Eval loss 4.5373, accuracy 0.2152\n",
      "\n",
      "Epoch 141, lr = 0.000333\n",
      "Training loss 3.7607, accuracy 0.2779\n",
      "Eval loss 4.5467, accuracy 0.2162\n",
      "\n",
      "Epoch 142, lr = 0.000332\n",
      "Training loss 3.7598, accuracy 0.2776\n",
      "Eval loss 4.5472, accuracy 0.2164\n",
      "\n",
      "Epoch 143, lr = 0.000331\n",
      "Training loss 3.7606, accuracy 0.2779\n",
      "Eval loss 4.5463, accuracy 0.2156\n",
      "\n",
      "Epoch 144, lr = 0.000330\n",
      "Training loss 3.7544, accuracy 0.2785\n",
      "Eval loss 4.5463, accuracy 0.2168\n",
      "\n",
      "Epoch 145, lr = 0.000329\n",
      "Training loss 3.7546, accuracy 0.2786\n",
      "Eval loss 4.5430, accuracy 0.2157\n",
      "\n",
      "Epoch 146, lr = 0.000327\n",
      "Training loss 3.7499, accuracy 0.2792\n",
      "Eval loss 4.5461, accuracy 0.2160\n",
      "\n",
      "Epoch 147, lr = 0.000326\n",
      "Training loss 3.7487, accuracy 0.2794\n",
      "Eval loss 4.5438, accuracy 0.2161\n",
      "\n",
      "Epoch 148, lr = 0.000325\n",
      "Training loss 3.7458, accuracy 0.2794\n",
      "Eval loss 4.5566, accuracy 0.2155\n",
      "\n",
      "Epoch 149, lr = 0.000324\n",
      "Training loss 3.7446, accuracy 0.2796\n",
      "Eval loss 4.5444, accuracy 0.2162\n",
      "\n",
      "Epoch 150, lr = 0.000323\n",
      "Training loss 3.7454, accuracy 0.2794\n",
      "Eval loss 4.5397, accuracy 0.2166\n",
      "\n",
      "Epoch 151, lr = 0.000322\n",
      "Training loss 3.7419, accuracy 0.2802\n",
      "Eval loss 4.5454, accuracy 0.2160\n",
      "\n",
      "Epoch 152, lr = 0.000321\n",
      "Training loss 3.7404, accuracy 0.2798\n",
      "Eval loss 4.5447, accuracy 0.2155\n",
      "\n",
      "Epoch 153, lr = 0.000320\n",
      "Training loss 3.7380, accuracy 0.2798\n",
      "Eval loss 4.5442, accuracy 0.2157\n",
      "\n",
      "Epoch 154, lr = 0.000319\n",
      "Training loss 3.7351, accuracy 0.2803\n",
      "Eval loss 4.5451, accuracy 0.2165\n",
      "\n",
      "Epoch 155, lr = 0.000318\n",
      "Training loss 3.7307, accuracy 0.2807\n",
      "Eval loss 4.5534, accuracy 0.2149\n",
      "\n",
      "Epoch 156, lr = 0.000317\n",
      "Training loss 3.7312, accuracy 0.2806\n",
      "Eval loss 4.5560, accuracy 0.2156\n",
      "\n",
      "Epoch 157, lr = 0.000316\n",
      "Training loss 3.7295, accuracy 0.2810\n",
      "Eval loss 4.5466, accuracy 0.2160\n",
      "\n",
      "Epoch 158, lr = 0.000315\n",
      "Training loss 3.7274, accuracy 0.2809\n",
      "Eval loss 4.5531, accuracy 0.2159\n",
      "\n",
      "Epoch 159, lr = 0.000314\n",
      "Training loss 3.7244, accuracy 0.2815\n",
      "Eval loss 4.5522, accuracy 0.2164\n",
      "\n",
      "Epoch 160, lr = 0.000313\n",
      "Training loss 3.7226, accuracy 0.2811\n",
      "Eval loss 4.5623, accuracy 0.2156\n",
      "\n",
      "Epoch 161, lr = 0.000312\n",
      "Training loss 3.7235, accuracy 0.2813\n",
      "Eval loss 4.5505, accuracy 0.2152\n",
      "\n",
      "Epoch 162, lr = 0.000311\n",
      "Training loss 3.7220, accuracy 0.2815\n",
      "Eval loss 4.5506, accuracy 0.2157\n",
      "\n",
      "Epoch 163, lr = 0.000310\n",
      "Training loss 3.7197, accuracy 0.2805\n",
      "Eval loss 4.5538, accuracy 0.2160\n",
      "\n",
      "Epoch 164, lr = 0.000309\n",
      "Training loss 3.7174, accuracy 0.2820\n",
      "Eval loss 4.5439, accuracy 0.2165\n",
      "\n",
      "Epoch 165, lr = 0.000308\n",
      "Training loss 3.7139, accuracy 0.2819\n",
      "Eval loss 4.5468, accuracy 0.2166\n",
      "\n",
      "Epoch 166, lr = 0.000307\n",
      "Training loss 3.7127, accuracy 0.2823\n",
      "Eval loss 4.5480, accuracy 0.2159\n",
      "\n",
      "Epoch 167, lr = 0.000306\n",
      "Training loss 3.7093, accuracy 0.2830\n",
      "Eval loss 4.5529, accuracy 0.2154\n",
      "\n",
      "Epoch 168, lr = 0.000305\n",
      "Training loss 3.7084, accuracy 0.2829\n",
      "Eval loss 4.5565, accuracy 0.2154\n",
      "\n",
      "Epoch 169, lr = 0.000304\n",
      "Training loss 3.7084, accuracy 0.2829\n",
      "Eval loss 4.5531, accuracy 0.2160\n",
      "\n",
      "Epoch 170, lr = 0.000303\n",
      "Training loss 3.7043, accuracy 0.2830\n",
      "Eval loss 4.5624, accuracy 0.2153\n",
      "\n",
      "Epoch 171, lr = 0.000303\n",
      "Training loss 3.7030, accuracy 0.2834\n",
      "Eval loss 4.5497, accuracy 0.2167\n",
      "\n",
      "Epoch 172, lr = 0.000302\n",
      "Training loss 3.7024, accuracy 0.2837\n",
      "Eval loss 4.5453, accuracy 0.2165\n",
      "\n",
      "Epoch 173, lr = 0.000301\n",
      "Training loss 3.7000, accuracy 0.2834\n",
      "Eval loss 4.5486, accuracy 0.2164\n",
      "\n",
      "Epoch 174, lr = 0.000300\n",
      "Training loss 3.6989, accuracy 0.2836\n",
      "Eval loss 4.5496, accuracy 0.2163\n",
      "\n",
      "Epoch 175, lr = 0.000299\n",
      "Training loss 3.6953, accuracy 0.2840\n",
      "Eval loss 4.5554, accuracy 0.2163\n",
      "\n",
      "Epoch 176, lr = 0.000298\n",
      "Training loss 3.6950, accuracy 0.2846\n",
      "Eval loss 4.5466, accuracy 0.2156\n",
      "\n",
      "Epoch 177, lr = 0.000297\n",
      "Training loss 3.6909, accuracy 0.2840\n",
      "Eval loss 4.5576, accuracy 0.2157\n",
      "\n",
      "Epoch 178, lr = 0.000297\n",
      "Training loss 3.6887, accuracy 0.2843\n",
      "Eval loss 4.5580, accuracy 0.2158\n",
      "\n",
      "Epoch 179, lr = 0.000296\n",
      "Training loss 3.6895, accuracy 0.2845\n",
      "Eval loss 4.5533, accuracy 0.2161\n",
      "\n",
      "Epoch 180, lr = 0.000295\n",
      "Training loss 3.6889, accuracy 0.2844\n",
      "Eval loss 4.5545, accuracy 0.2162\n",
      "\n",
      "Epoch 181, lr = 0.000294\n",
      "Training loss 3.6845, accuracy 0.2848\n",
      "Eval loss 4.5538, accuracy 0.2157\n",
      "\n",
      "Epoch 182, lr = 0.000293\n",
      "Training loss 3.6831, accuracy 0.2851\n",
      "Eval loss 4.5580, accuracy 0.2157\n",
      "\n",
      "Epoch 183, lr = 0.000292\n",
      "Training loss 3.6844, accuracy 0.2852\n",
      "Eval loss 4.5547, accuracy 0.2155\n",
      "\n",
      "Epoch 184, lr = 0.000292\n",
      "Training loss 3.6806, accuracy 0.2852\n",
      "Eval loss 4.5554, accuracy 0.2155\n",
      "\n",
      "Epoch 185, lr = 0.000291\n",
      "Training loss 3.6820, accuracy 0.2849\n",
      "Eval loss 4.5537, accuracy 0.2164\n",
      "\n",
      "Epoch 186, lr = 0.000290\n",
      "Training loss 3.6785, accuracy 0.2853\n",
      "Eval loss 4.5546, accuracy 0.2163\n",
      "\n",
      "Epoch 187, lr = 0.000289\n",
      "Training loss 3.6764, accuracy 0.2855\n",
      "Eval loss 4.5607, accuracy 0.2152\n",
      "\n",
      "Epoch 188, lr = 0.000289\n",
      "Training loss 3.6770, accuracy 0.2854\n",
      "Eval loss 4.5512, accuracy 0.2161\n",
      "\n",
      "Epoch 189, lr = 0.000288\n",
      "Training loss 3.6705, accuracy 0.2862\n",
      "Eval loss 4.5629, accuracy 0.2162\n",
      "\n",
      "Epoch 190, lr = 0.000287\n",
      "Training loss 3.6716, accuracy 0.2860\n",
      "Eval loss 4.5610, accuracy 0.2151\n",
      "\n",
      "Epoch 191, lr = 0.000286\n",
      "Training loss 3.6709, accuracy 0.2860\n",
      "Eval loss 4.5588, accuracy 0.2154\n",
      "\n",
      "Epoch 192, lr = 0.000286\n",
      "Training loss 3.6662, accuracy 0.2863\n",
      "Eval loss 4.5616, accuracy 0.2161\n",
      "\n",
      "Epoch 193, lr = 0.000285\n",
      "Training loss 3.6628, accuracy 0.2870\n",
      "Eval loss 4.5648, accuracy 0.2153\n",
      "\n",
      "Epoch 194, lr = 0.000284\n",
      "Training loss 3.6631, accuracy 0.2866\n",
      "Eval loss 4.5606, accuracy 0.2155\n",
      "\n",
      "Epoch 195, lr = 0.000283\n",
      "Training loss 3.6619, accuracy 0.2864\n",
      "Eval loss 4.5665, accuracy 0.2166\n",
      "\n",
      "Epoch 196, lr = 0.000283\n",
      "Training loss 3.6623, accuracy 0.2867\n",
      "Eval loss 4.5608, accuracy 0.2161\n",
      "\n",
      "Epoch 197, lr = 0.000282\n",
      "Training loss 3.6604, accuracy 0.2866\n",
      "Eval loss 4.5619, accuracy 0.2157\n",
      "\n",
      "Epoch 198, lr = 0.000281\n",
      "Training loss 3.6574, accuracy 0.2871\n",
      "Eval loss 4.5646, accuracy 0.2155\n",
      "\n",
      "Epoch 199, lr = 0.000280\n",
      "Training loss 3.6540, accuracy 0.2876\n",
      "Eval loss 4.5673, accuracy 0.2155\n"
     ]
    }
   ],
   "source": [
    "training_loop(config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▆█▇▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.00028</td></tr><tr><td>step</td><td>99301</td></tr><tr><td>train_acc</td><td>0.28758</td></tr><tr><td>train_loss</td><td>3.65399</td></tr><tr><td>val_acc</td><td>0.21551</td></tr><tr><td>val_loss</td><td>4.56732</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">usual-dragon-11</strong> at: <a href='https://wandb.ai/lovis/basic-transformer/runs/sbhf1168' target=\"_blank\">https://wandb.ai/lovis/basic-transformer/runs/sbhf1168</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230420_163906-sbhf1168\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(transformer.state_dict(), run_data_path + \"/model.pt\")\n",
    "model_artifact.add_file(run_data_path + \"/model.pt\")\n",
    "wandb.log_artifact(model_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
