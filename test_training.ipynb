{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlovisheindrich\u001b[0m (\u001b[33mlovis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.model import Transformer, TransformerConfig\n",
    "from src.load_data import load_data, download_data, create_word_dicts, create_dataset\n",
    "from src.train import train, eval\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "from tqdm.notebook import trange\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1195401 words\n",
      "6083 words that occur >= 10 times\n",
      "22188 words that occur < 10 times\n",
      "Vocabulary size: 6084\n"
     ]
    }
   ],
   "source": [
    "download_data()\n",
    "words = load_data()\n",
    "word_to_index, index_to_word = create_word_dicts(words, min_occurrences=10)\n",
    "\n",
    "data = {\n",
    "    \"words\": words,\n",
    "    \"word_to_index\": word_to_index,\n",
    "    \"index_to_word\": index_to_word\n",
    "}\n",
    "\n",
    "# Initialize run data directory\n",
    "run_data_path = \"./run_data\"\n",
    "if os.path.exists(run_data_path):\n",
    "    shutil.rmtree(run_data_path)\n",
    "os.mkdir(run_data_path)\n",
    "os.mkdir(run_data_path+\"/checkpoints\")\n",
    "\n",
    "with open(run_data_path+\"/word_data.json\", 'w') as outfile:\n",
    "    outfile.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\heind\\Documents\\workspace\\basic-transformer\\wandb\\run-20230420_102236-qh607w2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lovis/basic-transformer/runs/qh607w2f' target=\"_blank\">devoted-eon-7</a></strong> to <a href='https://wandb.ai/lovis/basic-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lovis/basic-transformer' target=\"_blank\">https://wandb.ai/lovis/basic-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lovis/basic-transformer/runs/qh607w2f' target=\"_blank\">https://wandb.ai/lovis/basic-transformer/runs/qh607w2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens per batch 1920\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": len(index_to_word),\n",
    "    \"max_input_length\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_size\": 256,\n",
    "    \"num_blocks\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_epochs\": 300,\n",
    "    \"val_split\": 0.2,\n",
    "    \"warmup_steps\": 4000,\n",
    "    \"lr_scale\": 5\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"basic-transformer\",\n",
    "    config=config,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "word_artifact = wandb.Artifact('word_dicts', 'dataset')\n",
    "word_artifact.add_file(local_path=run_data_path+\"/word_data.json\")\n",
    "wandb.log_artifact(word_artifact)\n",
    "\n",
    "model_artifact = wandb.Artifact('models', 'model')\n",
    "\n",
    "\n",
    "# Paper used 25000 tokens per batch\n",
    "print(\"Total tokens per batch\", config[\"batch_size\"]*config[\"max_input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(run_data_path+\"/config.json\", 'w') as outfile:\n",
    "    outfile.write(json.dumps(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = create_dataset(words, word_to_index, index_to_word, batch_size=config[\"batch_size\"], val_split=config[\"val_split\"], max_input_length=config[\"max_input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4720128\n",
      "Total training steps: 149700\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(TransformerConfig(vocab_size=config[\"vocabulary_size\"], max_input_length=config[\"max_input_length\"], num_heads=config[\"num_heads\"], num_blocks=config[\"num_blocks\"], embedding_size=config[\"embedding_size\"]), apply_softmax=False)\n",
    "transformer.to(device)\n",
    "wandb.watch(transformer, log_freq=1000)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "steps_per_epoch = len(train_dl)\n",
    "\n",
    "def transformer_lr(step, d_model=config[\"embedding_size\"], warmup_steps=config[\"warmup_steps\"], lr_scale=config[\"lr_scale\"]):\n",
    "    if step==0:\n",
    "        return transformer_lr(1, d_model, warmup_steps)\n",
    "    return lr_scale*((d_model) ** -0.5)*min(step**-0.5, step*(warmup_steps**-1.5))\n",
    "\n",
    "initial_lr = transformer_lr(1)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "lr_per_epoch = lambda epoch: transformer_lr(epoch*steps_per_epoch) / initial_lr\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_per_epoch)\n",
    "\n",
    "transformer_params = 0\n",
    "for param in transformer.parameters():\n",
    "    transformer_params += param.nelement()\n",
    "print(\"Total parameters:\", transformer_params)\n",
    "print(\"Total training steps:\", steps_per_epoch*config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_schedule():\n",
    "    optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_per_epoch)\n",
    "    lrs = []\n",
    "    for i in range(300):\n",
    "        lrs.append(lr_scheduler.optimizer.param_groups[0]['lr'])\n",
    "        lr_scheduler.step()\n",
    "    import seaborn as sns\n",
    "    print(initial_lr, max(lrs))\n",
    "    sns.lineplot(lrs)\n",
    "\n",
    "#plot_lr_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_epochs, start_epoch=0, model_checkpoint_freq=100):\n",
    "    for e in trange(num_epochs):\n",
    "\n",
    "        train(transformer, loss_fn=loss_fn, optim=optim, device=device, dl=train_dl)\n",
    "\n",
    "        train_loss, train_acc = eval(transformer=transformer, loss_fn=loss_fn, device=device, dl=train_dl)\n",
    "        val_loss, val_acc = eval(transformer=transformer, loss_fn=loss_fn, device=device, dl=val_dl)\n",
    "\n",
    "        lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "        step = (e+start_epoch)*steps_per_epoch\n",
    "        print(f\"\\nEpoch {e+start_epoch}, lr = {lr:.6f}\")\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Training loss {train_loss:.4f}, accuracy {train_acc:.4f}\")\n",
    "        print(f\"Eval loss {val_loss:.4f}, accuracy {val_acc:.4f}\")\n",
    "        if (e+1)%model_checkpoint_freq == 0:\n",
    "            checkpoint_name = f\"/checkpoints/checkpoint_{e+start_epoch+1}.pt\"\n",
    "            torch.save(transformer.state_dict(), run_data_path + checkpoint_name)\n",
    "            model_artifact.add_file(run_data_path + checkpoint_name, name =checkpoint_name)\n",
    "        wandb.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, \"learning_rate\": lr, \"step\": step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4dce5e179444efb37ca9a804cd5050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, lr = 0.000001\n",
      "Training loss 32.7505, accuracy 0.0100\n",
      "Eval loss 32.8000, accuracy 0.0099\n",
      "\n",
      "Epoch 1, lr = 0.000616\n",
      "Training loss 7.0642, accuracy 0.0778\n",
      "Eval loss 7.0852, accuracy 0.0783\n",
      "\n",
      "Epoch 2, lr = 0.001233\n",
      "Training loss 5.8961, accuracy 0.0835\n",
      "Eval loss 5.9437, accuracy 0.0811\n",
      "\n",
      "Epoch 3, lr = 0.001849\n",
      "Training loss 5.1550, accuracy 0.1728\n",
      "Eval loss 5.2229, accuracy 0.1691\n",
      "\n",
      "Epoch 4, lr = 0.002466\n",
      "Training loss 4.9541, accuracy 0.1790\n",
      "Eval loss 5.0377, accuracy 0.1743\n",
      "\n",
      "Epoch 5, lr = 0.003082\n",
      "Training loss 4.8494, accuracy 0.1804\n",
      "Eval loss 4.9352, accuracy 0.1753\n",
      "\n",
      "Epoch 6, lr = 0.003698\n",
      "Training loss 4.7860, accuracy 0.1880\n",
      "Eval loss 4.8760, accuracy 0.1836\n",
      "\n",
      "Epoch 7, lr = 0.004315\n",
      "Training loss 4.7628, accuracy 0.1873\n",
      "Eval loss 4.8662, accuracy 0.1824\n",
      "\n",
      "Epoch 8, lr = 0.004931\n",
      "Training loss 4.7258, accuracy 0.1882\n",
      "Eval loss 4.8356, accuracy 0.1833\n",
      "\n",
      "Epoch 9, lr = 0.004663\n",
      "Training loss 4.6630, accuracy 0.1950\n",
      "Eval loss 4.7848, accuracy 0.1891\n",
      "\n",
      "Epoch 10, lr = 0.004424\n",
      "Training loss 4.6081, accuracy 0.1997\n",
      "Eval loss 4.7488, accuracy 0.1931\n",
      "\n",
      "Epoch 11, lr = 0.004218\n",
      "Training loss 4.5604, accuracy 0.2024\n",
      "Eval loss 4.7145, accuracy 0.1942\n",
      "\n",
      "Epoch 12, lr = 0.004038\n",
      "Training loss 4.5036, accuracy 0.2063\n",
      "Eval loss 4.6800, accuracy 0.1972\n",
      "\n",
      "Epoch 13, lr = 0.003880\n",
      "Training loss 4.4638, accuracy 0.2078\n",
      "Eval loss 4.6575, accuracy 0.1978\n",
      "\n",
      "Epoch 14, lr = 0.003739\n",
      "Training loss 4.4284, accuracy 0.2134\n",
      "Eval loss 4.6333, accuracy 0.2013\n",
      "\n",
      "Epoch 15, lr = 0.003612\n",
      "Training loss 4.3840, accuracy 0.2167\n",
      "Eval loss 4.6123, accuracy 0.2043\n",
      "\n",
      "Epoch 16, lr = 0.003497\n",
      "Training loss 4.3545, accuracy 0.2186\n",
      "Eval loss 4.5951, accuracy 0.2051\n",
      "\n",
      "Epoch 17, lr = 0.003393\n",
      "Training loss 4.3188, accuracy 0.2203\n",
      "Eval loss 4.5875, accuracy 0.2054\n",
      "\n",
      "Epoch 18, lr = 0.003297\n",
      "Training loss 4.2853, accuracy 0.2239\n",
      "Eval loss 4.5755, accuracy 0.2069\n",
      "\n",
      "Epoch 19, lr = 0.003209\n",
      "Training loss 4.2580, accuracy 0.2252\n",
      "Eval loss 4.5676, accuracy 0.2075\n",
      "\n",
      "Epoch 20, lr = 0.003128\n",
      "Training loss 4.2351, accuracy 0.2277\n",
      "Eval loss 4.5616, accuracy 0.2098\n",
      "\n",
      "Epoch 21, lr = 0.003053\n",
      "Training loss 4.2121, accuracy 0.2285\n",
      "Eval loss 4.5539, accuracy 0.2079\n",
      "\n",
      "Epoch 22, lr = 0.002983\n",
      "Training loss 4.1781, accuracy 0.2314\n",
      "Eval loss 4.5383, accuracy 0.2102\n",
      "\n",
      "Epoch 23, lr = 0.002917\n",
      "Training loss 4.1550, accuracy 0.2324\n",
      "Eval loss 4.5364, accuracy 0.2107\n",
      "\n",
      "Epoch 24, lr = 0.002856\n",
      "Training loss 4.1364, accuracy 0.2340\n",
      "Eval loss 4.5382, accuracy 0.2107\n",
      "\n",
      "Epoch 25, lr = 0.002798\n",
      "Training loss 4.1130, accuracy 0.2359\n",
      "Eval loss 4.5328, accuracy 0.2118\n",
      "\n",
      "Epoch 26, lr = 0.002744\n",
      "Training loss 4.0954, accuracy 0.2368\n",
      "Eval loss 4.5351, accuracy 0.2106\n",
      "\n",
      "Epoch 27, lr = 0.002692\n",
      "Training loss 4.0710, accuracy 0.2389\n",
      "Eval loss 4.5386, accuracy 0.2126\n",
      "\n",
      "Epoch 28, lr = 0.002644\n",
      "Training loss 4.0489, accuracy 0.2405\n",
      "Eval loss 4.5365, accuracy 0.2124\n",
      "\n",
      "Epoch 29, lr = 0.002598\n",
      "Training loss 4.0232, accuracy 0.2432\n",
      "Eval loss 4.5431, accuracy 0.2141\n",
      "\n",
      "Epoch 30, lr = 0.002554\n",
      "Training loss 4.0135, accuracy 0.2435\n",
      "Eval loss 4.5314, accuracy 0.2122\n",
      "\n",
      "Epoch 31, lr = 0.002513\n",
      "Training loss 3.9913, accuracy 0.2445\n",
      "Eval loss 4.5363, accuracy 0.2124\n",
      "\n",
      "Epoch 32, lr = 0.002473\n",
      "Training loss 3.9686, accuracy 0.2466\n",
      "Eval loss 4.5514, accuracy 0.2113\n",
      "\n",
      "Epoch 33, lr = 0.002435\n",
      "Training loss 3.9498, accuracy 0.2474\n",
      "Eval loss 4.5358, accuracy 0.2135\n",
      "\n",
      "Epoch 34, lr = 0.002399\n",
      "Training loss 3.9348, accuracy 0.2496\n",
      "Eval loss 4.5452, accuracy 0.2121\n",
      "\n",
      "Epoch 35, lr = 0.002365\n",
      "Training loss 3.9139, accuracy 0.2517\n",
      "Eval loss 4.5420, accuracy 0.2136\n",
      "\n",
      "Epoch 36, lr = 0.002332\n",
      "Training loss 3.8964, accuracy 0.2518\n",
      "Eval loss 4.5497, accuracy 0.2140\n",
      "\n",
      "Epoch 37, lr = 0.002300\n",
      "Training loss 3.8728, accuracy 0.2541\n",
      "Eval loss 4.5492, accuracy 0.2129\n",
      "\n",
      "Epoch 38, lr = 0.002269\n",
      "Training loss 3.8619, accuracy 0.2557\n",
      "Eval loss 4.5474, accuracy 0.2129\n",
      "\n",
      "Epoch 39, lr = 0.002240\n",
      "Training loss 3.8420, accuracy 0.2566\n",
      "Eval loss 4.5508, accuracy 0.2137\n",
      "\n",
      "Epoch 40, lr = 0.002212\n",
      "Training loss 3.8322, accuracy 0.2575\n",
      "Eval loss 4.5520, accuracy 0.2141\n",
      "\n",
      "Epoch 41, lr = 0.002185\n",
      "Training loss 3.8150, accuracy 0.2589\n",
      "Eval loss 4.5693, accuracy 0.2138\n",
      "\n",
      "Epoch 42, lr = 0.002159\n",
      "Training loss 3.8030, accuracy 0.2594\n",
      "Eval loss 4.5721, accuracy 0.2139\n",
      "\n",
      "Epoch 43, lr = 0.002133\n",
      "Training loss 3.7903, accuracy 0.2599\n",
      "Eval loss 4.5770, accuracy 0.2138\n",
      "\n",
      "Epoch 44, lr = 0.002109\n",
      "Training loss 3.7686, accuracy 0.2621\n",
      "Eval loss 4.5709, accuracy 0.2139\n",
      "\n",
      "Epoch 45, lr = 0.002085\n",
      "Training loss 3.7488, accuracy 0.2631\n",
      "Eval loss 4.5954, accuracy 0.2119\n",
      "\n",
      "Epoch 46, lr = 0.002063\n",
      "Training loss 3.7366, accuracy 0.2648\n",
      "Eval loss 4.5821, accuracy 0.2145\n",
      "\n",
      "Epoch 47, lr = 0.002041\n",
      "Training loss 3.7250, accuracy 0.2659\n",
      "Eval loss 4.5943, accuracy 0.2112\n",
      "\n",
      "Epoch 48, lr = 0.002019\n",
      "Training loss 3.7119, accuracy 0.2665\n",
      "Eval loss 4.5995, accuracy 0.2128\n",
      "\n",
      "Epoch 49, lr = 0.001998\n",
      "Training loss 3.7030, accuracy 0.2679\n",
      "Eval loss 4.5924, accuracy 0.2119\n",
      "\n",
      "Epoch 50, lr = 0.001978\n",
      "Training loss 3.6800, accuracy 0.2696\n",
      "Eval loss 4.6038, accuracy 0.2132\n",
      "\n",
      "Epoch 51, lr = 0.001959\n",
      "Training loss 3.6678, accuracy 0.2702\n",
      "Eval loss 4.6193, accuracy 0.2142\n",
      "\n",
      "Epoch 52, lr = 0.001940\n",
      "Training loss 3.6659, accuracy 0.2700\n",
      "Eval loss 4.6066, accuracy 0.2143\n",
      "\n",
      "Epoch 53, lr = 0.001922\n",
      "Training loss 3.6403, accuracy 0.2725\n",
      "Eval loss 4.6304, accuracy 0.2125\n",
      "\n",
      "Epoch 54, lr = 0.001904\n",
      "Training loss 3.6360, accuracy 0.2729\n",
      "Eval loss 4.6304, accuracy 0.2128\n",
      "\n",
      "Epoch 55, lr = 0.001886\n",
      "Training loss 3.6218, accuracy 0.2740\n",
      "Eval loss 4.6271, accuracy 0.2118\n",
      "\n",
      "Epoch 56, lr = 0.001869\n",
      "Training loss 3.6127, accuracy 0.2744\n",
      "Eval loss 4.6217, accuracy 0.2130\n",
      "\n",
      "Epoch 57, lr = 0.001853\n",
      "Training loss 3.6044, accuracy 0.2754\n",
      "Eval loss 4.6200, accuracy 0.2137\n",
      "\n",
      "Epoch 58, lr = 0.001837\n",
      "Training loss 3.5797, accuracy 0.2769\n",
      "Eval loss 4.6527, accuracy 0.2139\n",
      "\n",
      "Epoch 59, lr = 0.001821\n",
      "Training loss 3.5707, accuracy 0.2786\n",
      "Eval loss 4.6614, accuracy 0.2112\n",
      "\n",
      "Epoch 60, lr = 0.001806\n",
      "Training loss 3.5625, accuracy 0.2804\n",
      "Eval loss 4.6697, accuracy 0.2097\n",
      "\n",
      "Epoch 61, lr = 0.001791\n",
      "Training loss 3.5538, accuracy 0.2805\n",
      "Eval loss 4.6477, accuracy 0.2119\n",
      "\n",
      "Epoch 62, lr = 0.001777\n",
      "Training loss 3.5343, accuracy 0.2822\n",
      "Eval loss 4.6622, accuracy 0.2122\n",
      "\n",
      "Epoch 63, lr = 0.001763\n",
      "Training loss 3.5215, accuracy 0.2835\n",
      "Eval loss 4.6779, accuracy 0.2119\n",
      "\n",
      "Epoch 64, lr = 0.001749\n",
      "Training loss 3.5099, accuracy 0.2847\n",
      "Eval loss 4.6798, accuracy 0.2113\n",
      "\n",
      "Epoch 65, lr = 0.001735\n",
      "Training loss 3.5040, accuracy 0.2848\n",
      "Eval loss 4.6722, accuracy 0.2117\n",
      "\n",
      "Epoch 66, lr = 0.001722\n",
      "Training loss 3.4970, accuracy 0.2864\n",
      "Eval loss 4.6797, accuracy 0.2097\n",
      "\n",
      "Epoch 67, lr = 0.001709\n",
      "Training loss 3.4921, accuracy 0.2870\n",
      "Eval loss 4.6863, accuracy 0.2095\n",
      "\n",
      "Epoch 68, lr = 0.001696\n",
      "Training loss 3.4736, accuracy 0.2867\n",
      "Eval loss 4.7079, accuracy 0.2138\n",
      "\n",
      "Epoch 69, lr = 0.001684\n",
      "Training loss 3.4581, accuracy 0.2908\n",
      "Eval loss 4.7051, accuracy 0.2101\n",
      "\n",
      "Epoch 70, lr = 0.001672\n",
      "Training loss 3.4550, accuracy 0.2895\n",
      "Eval loss 4.6894, accuracy 0.2122\n",
      "\n",
      "Epoch 71, lr = 0.001660\n",
      "Training loss 3.4410, accuracy 0.2913\n",
      "Eval loss 4.7129, accuracy 0.2103\n",
      "\n",
      "Epoch 72, lr = 0.001649\n",
      "Training loss 3.4252, accuracy 0.2928\n",
      "Eval loss 4.7337, accuracy 0.2095\n",
      "\n",
      "Epoch 73, lr = 0.001637\n",
      "Training loss 3.4201, accuracy 0.2935\n",
      "Eval loss 4.7265, accuracy 0.2112\n",
      "\n",
      "Epoch 74, lr = 0.001626\n",
      "Training loss 3.4062, accuracy 0.2949\n",
      "Eval loss 4.7367, accuracy 0.2096\n",
      "\n",
      "Epoch 75, lr = 0.001615\n",
      "Training loss 3.4080, accuracy 0.2941\n",
      "Eval loss 4.7259, accuracy 0.2121\n",
      "\n",
      "Epoch 76, lr = 0.001605\n",
      "Training loss 3.3980, accuracy 0.2957\n",
      "Eval loss 4.7465, accuracy 0.2100\n",
      "\n",
      "Epoch 77, lr = 0.001594\n",
      "Training loss 3.3882, accuracy 0.2964\n",
      "Eval loss 4.7352, accuracy 0.2108\n",
      "\n",
      "Epoch 78, lr = 0.001584\n",
      "Training loss 3.3793, accuracy 0.2969\n",
      "Eval loss 4.7213, accuracy 0.2132\n",
      "\n",
      "Epoch 79, lr = 0.001574\n",
      "Training loss 3.3724, accuracy 0.2967\n",
      "Eval loss 4.7502, accuracy 0.2126\n",
      "\n",
      "Epoch 80, lr = 0.001564\n",
      "Training loss 3.3622, accuracy 0.3007\n",
      "Eval loss 4.7579, accuracy 0.2076\n",
      "\n",
      "Epoch 81, lr = 0.001554\n",
      "Training loss 3.3585, accuracy 0.3005\n",
      "Eval loss 4.7319, accuracy 0.2110\n",
      "\n",
      "Epoch 82, lr = 0.001545\n",
      "Training loss 3.3309, accuracy 0.3025\n",
      "Eval loss 4.7664, accuracy 0.2118\n",
      "\n",
      "Epoch 83, lr = 0.001536\n",
      "Training loss 3.3405, accuracy 0.3009\n",
      "Eval loss 4.7725, accuracy 0.2111\n",
      "\n",
      "Epoch 84, lr = 0.001526\n",
      "Training loss 3.3325, accuracy 0.3045\n",
      "Eval loss 4.7513, accuracy 0.2097\n",
      "\n",
      "Epoch 85, lr = 0.001517\n",
      "Training loss 3.3170, accuracy 0.3047\n",
      "Eval loss 4.7642, accuracy 0.2102\n",
      "\n",
      "Epoch 86, lr = 0.001509\n",
      "Training loss 3.3109, accuracy 0.3041\n",
      "Eval loss 4.7777, accuracy 0.2111\n",
      "\n",
      "Epoch 87, lr = 0.001500\n",
      "Training loss 3.2974, accuracy 0.3088\n",
      "Eval loss 4.7854, accuracy 0.2092\n",
      "\n",
      "Epoch 88, lr = 0.001491\n",
      "Training loss 3.2933, accuracy 0.3066\n",
      "Eval loss 4.7919, accuracy 0.2108\n",
      "\n",
      "Epoch 89, lr = 0.001483\n",
      "Training loss 3.2837, accuracy 0.3089\n",
      "Eval loss 4.8011, accuracy 0.2074\n",
      "\n",
      "Epoch 90, lr = 0.001475\n",
      "Training loss 3.2810, accuracy 0.3101\n",
      "Eval loss 4.7830, accuracy 0.2091\n",
      "\n",
      "Epoch 91, lr = 0.001466\n",
      "Training loss 3.2760, accuracy 0.3120\n",
      "Eval loss 4.7968, accuracy 0.2075\n",
      "\n",
      "Epoch 92, lr = 0.001458\n",
      "Training loss 3.2640, accuracy 0.3107\n",
      "Eval loss 4.8029, accuracy 0.2104\n",
      "\n",
      "Epoch 93, lr = 0.001451\n",
      "Training loss 3.2657, accuracy 0.3118\n",
      "Eval loss 4.7885, accuracy 0.2075\n",
      "\n",
      "Epoch 94, lr = 0.001443\n",
      "Training loss 3.2503, accuracy 0.3117\n",
      "Eval loss 4.8140, accuracy 0.2108\n",
      "\n",
      "Epoch 95, lr = 0.001435\n",
      "Training loss 3.2476, accuracy 0.3122\n",
      "Eval loss 4.8152, accuracy 0.2091\n",
      "\n",
      "Epoch 96, lr = 0.001428\n",
      "Training loss 3.2478, accuracy 0.3154\n",
      "Eval loss 4.7874, accuracy 0.2083\n",
      "\n",
      "Epoch 97, lr = 0.001420\n",
      "Training loss 3.2358, accuracy 0.3168\n",
      "Eval loss 4.8144, accuracy 0.2074\n",
      "\n",
      "Epoch 98, lr = 0.001413\n",
      "Training loss 3.2256, accuracy 0.3149\n",
      "Eval loss 4.8299, accuracy 0.2097\n",
      "\n",
      "Epoch 99, lr = 0.001406\n",
      "Training loss 3.2200, accuracy 0.3186\n",
      "Eval loss 4.8003, accuracy 0.2082\n",
      "\n",
      "Epoch 100, lr = 0.001399\n",
      "Training loss 3.2226, accuracy 0.3198\n",
      "Eval loss 4.8271, accuracy 0.2055\n",
      "\n",
      "Epoch 101, lr = 0.001392\n",
      "Training loss 3.2037, accuracy 0.3174\n",
      "Eval loss 4.8419, accuracy 0.2105\n",
      "\n",
      "Epoch 102, lr = 0.001385\n",
      "Training loss 3.1962, accuracy 0.3218\n",
      "Eval loss 4.8494, accuracy 0.2064\n",
      "\n",
      "Epoch 103, lr = 0.001378\n",
      "Training loss 3.1952, accuracy 0.3201\n",
      "Eval loss 4.8199, accuracy 0.2089\n",
      "\n",
      "Epoch 104, lr = 0.001372\n",
      "Training loss 3.1871, accuracy 0.3194\n",
      "Eval loss 4.8477, accuracy 0.2098\n",
      "\n",
      "Epoch 105, lr = 0.001365\n",
      "Training loss 3.1785, accuracy 0.3227\n",
      "Eval loss 4.8452, accuracy 0.2092\n",
      "\n",
      "Epoch 106, lr = 0.001359\n",
      "Training loss 3.1663, accuracy 0.3236\n",
      "Eval loss 4.8713, accuracy 0.2090\n",
      "\n",
      "Epoch 107, lr = 0.001352\n",
      "Training loss 3.1726, accuracy 0.3228\n",
      "Eval loss 4.8419, accuracy 0.2098\n",
      "\n",
      "Epoch 108, lr = 0.001346\n",
      "Training loss 3.1639, accuracy 0.3226\n",
      "Eval loss 4.8479, accuracy 0.2094\n",
      "\n",
      "Epoch 109, lr = 0.001340\n",
      "Training loss 3.1564, accuracy 0.3236\n",
      "Eval loss 4.8624, accuracy 0.2092\n",
      "\n",
      "Epoch 110, lr = 0.001334\n",
      "Training loss 3.1514, accuracy 0.3267\n",
      "Eval loss 4.8607, accuracy 0.2073\n",
      "\n",
      "Epoch 111, lr = 0.001328\n",
      "Training loss 3.1519, accuracy 0.3258\n",
      "Eval loss 4.8487, accuracy 0.2090\n",
      "\n",
      "Epoch 112, lr = 0.001322\n",
      "Training loss 3.1369, accuracy 0.3271\n",
      "Eval loss 4.8582, accuracy 0.2085\n",
      "\n",
      "Epoch 113, lr = 0.001316\n",
      "Training loss 3.1358, accuracy 0.3288\n",
      "Eval loss 4.8636, accuracy 0.2069\n",
      "\n",
      "Epoch 114, lr = 0.001310\n",
      "Training loss 3.1283, accuracy 0.3291\n",
      "Eval loss 4.8818, accuracy 0.2060\n",
      "\n",
      "Epoch 115, lr = 0.001305\n",
      "Training loss 3.1318, accuracy 0.3284\n",
      "Eval loss 4.8506, accuracy 0.2068\n",
      "\n",
      "Epoch 116, lr = 0.001299\n",
      "Training loss 3.1230, accuracy 0.3289\n",
      "Eval loss 4.8732, accuracy 0.2088\n",
      "\n",
      "Epoch 117, lr = 0.001293\n",
      "Training loss 3.1366, accuracy 0.3296\n",
      "Eval loss 4.8345, accuracy 0.2085\n",
      "\n",
      "Epoch 118, lr = 0.001288\n",
      "Training loss 3.1067, accuracy 0.3325\n",
      "Eval loss 4.8809, accuracy 0.2069\n",
      "\n",
      "Epoch 119, lr = 0.001282\n",
      "Training loss 3.1060, accuracy 0.3321\n",
      "Eval loss 4.8763, accuracy 0.2088\n",
      "\n",
      "Epoch 120, lr = 0.001277\n",
      "Training loss 3.1064, accuracy 0.3324\n",
      "Eval loss 4.8736, accuracy 0.2078\n",
      "\n",
      "Epoch 121, lr = 0.001272\n",
      "Training loss 3.0957, accuracy 0.3331\n",
      "Eval loss 4.9012, accuracy 0.2082\n",
      "\n",
      "Epoch 122, lr = 0.001267\n",
      "Training loss 3.0884, accuracy 0.3349\n",
      "Eval loss 4.9044, accuracy 0.2070\n",
      "\n",
      "Epoch 123, lr = 0.001261\n",
      "Training loss 3.0841, accuracy 0.3362\n",
      "Eval loss 4.9213, accuracy 0.2058\n",
      "\n",
      "Epoch 124, lr = 0.001256\n",
      "Training loss 3.0722, accuracy 0.3374\n",
      "Eval loss 4.9279, accuracy 0.2057\n",
      "\n",
      "Epoch 125, lr = 0.001251\n",
      "Training loss 3.0782, accuracy 0.3376\n",
      "Eval loss 4.8851, accuracy 0.2061\n",
      "\n",
      "Epoch 126, lr = 0.001246\n",
      "Training loss 3.0740, accuracy 0.3381\n",
      "Eval loss 4.8990, accuracy 0.2058\n",
      "\n",
      "Epoch 127, lr = 0.001241\n",
      "Training loss 3.0698, accuracy 0.3352\n",
      "Eval loss 4.9139, accuracy 0.2085\n",
      "\n",
      "Epoch 128, lr = 0.001237\n",
      "Training loss 3.0595, accuracy 0.3360\n",
      "Eval loss 4.9230, accuracy 0.2089\n",
      "\n",
      "Epoch 129, lr = 0.001232\n",
      "Training loss 3.0593, accuracy 0.3416\n",
      "Eval loss 4.9159, accuracy 0.2030\n",
      "\n",
      "Epoch 130, lr = 0.001227\n",
      "Training loss 3.0513, accuracy 0.3385\n",
      "Eval loss 4.9099, accuracy 0.2083\n",
      "\n",
      "Epoch 131, lr = 0.001222\n",
      "Training loss 3.0474, accuracy 0.3394\n",
      "Eval loss 4.9339, accuracy 0.2068\n",
      "\n",
      "Epoch 132, lr = 0.001218\n",
      "Training loss 3.0559, accuracy 0.3430\n",
      "Eval loss 4.9450, accuracy 0.2023\n",
      "\n",
      "Epoch 133, lr = 0.001213\n",
      "Training loss 3.0501, accuracy 0.3418\n",
      "Eval loss 4.9074, accuracy 0.2045\n",
      "\n",
      "Epoch 134, lr = 0.001209\n",
      "Training loss 3.0326, accuracy 0.3413\n",
      "Eval loss 4.9331, accuracy 0.2073\n",
      "\n",
      "Epoch 135, lr = 0.001204\n",
      "Training loss 3.0325, accuracy 0.3447\n",
      "Eval loss 4.9349, accuracy 0.2047\n",
      "\n",
      "Epoch 136, lr = 0.001200\n",
      "Training loss 3.0281, accuracy 0.3452\n",
      "Eval loss 4.9374, accuracy 0.2042\n",
      "\n",
      "Epoch 137, lr = 0.001195\n",
      "Training loss 3.0323, accuracy 0.3453\n",
      "Eval loss 4.9202, accuracy 0.2047\n",
      "\n",
      "Epoch 138, lr = 0.001191\n",
      "Training loss 3.0181, accuracy 0.3472\n",
      "Eval loss 4.9508, accuracy 0.2034\n",
      "\n",
      "Epoch 139, lr = 0.001187\n",
      "Training loss 3.0126, accuracy 0.3453\n",
      "Eval loss 4.9387, accuracy 0.2065\n",
      "\n",
      "Epoch 140, lr = 0.001182\n",
      "Training loss 3.0154, accuracy 0.3475\n",
      "Eval loss 4.9327, accuracy 0.2046\n",
      "\n",
      "Epoch 141, lr = 0.001178\n",
      "Training loss 3.0150, accuracy 0.3446\n",
      "Eval loss 4.9283, accuracy 0.2065\n",
      "\n",
      "Epoch 142, lr = 0.001174\n",
      "Training loss 3.0004, accuracy 0.3455\n",
      "Eval loss 4.9634, accuracy 0.2068\n",
      "\n",
      "Epoch 143, lr = 0.001170\n",
      "Training loss 3.0067, accuracy 0.3478\n",
      "Eval loss 4.9447, accuracy 0.2041\n",
      "\n",
      "Epoch 144, lr = 0.001166\n",
      "Training loss 2.9989, accuracy 0.3457\n",
      "Eval loss 4.9355, accuracy 0.2077\n",
      "\n",
      "Epoch 145, lr = 0.001162\n",
      "Training loss 2.9980, accuracy 0.3479\n",
      "Eval loss 4.9548, accuracy 0.2058\n",
      "\n",
      "Epoch 146, lr = 0.001158\n",
      "Training loss 2.9783, accuracy 0.3514\n",
      "Eval loss 4.9707, accuracy 0.2040\n",
      "\n",
      "Epoch 147, lr = 0.001154\n",
      "Training loss 2.9819, accuracy 0.3512\n",
      "Eval loss 4.9414, accuracy 0.2065\n",
      "\n",
      "Epoch 148, lr = 0.001150\n",
      "Training loss 2.9926, accuracy 0.3508\n",
      "Eval loss 4.9620, accuracy 0.2032\n",
      "\n",
      "Epoch 149, lr = 0.001146\n",
      "Training loss 2.9800, accuracy 0.3527\n",
      "Eval loss 4.9612, accuracy 0.2033\n",
      "\n",
      "Epoch 150, lr = 0.001142\n",
      "Training loss 2.9737, accuracy 0.3532\n",
      "Eval loss 4.9993, accuracy 0.2014\n",
      "\n",
      "Epoch 151, lr = 0.001138\n",
      "Training loss 2.9778, accuracy 0.3536\n",
      "Eval loss 4.9451, accuracy 0.2038\n",
      "\n",
      "Epoch 152, lr = 0.001135\n",
      "Training loss 2.9777, accuracy 0.3520\n",
      "Eval loss 4.9266, accuracy 0.2074\n",
      "\n",
      "Epoch 153, lr = 0.001131\n",
      "Training loss 2.9658, accuracy 0.3528\n",
      "Eval loss 4.9669, accuracy 0.2050\n",
      "\n",
      "Epoch 154, lr = 0.001127\n",
      "Training loss 2.9586, accuracy 0.3507\n",
      "Eval loss 5.0068, accuracy 0.2071\n",
      "\n",
      "Epoch 155, lr = 0.001124\n",
      "Training loss 2.9610, accuracy 0.3566\n",
      "Eval loss 4.9624, accuracy 0.2026\n",
      "\n",
      "Epoch 156, lr = 0.001120\n",
      "Training loss 2.9598, accuracy 0.3521\n",
      "Eval loss 4.9686, accuracy 0.2067\n",
      "\n",
      "Epoch 157, lr = 0.001116\n",
      "Training loss 2.9683, accuracy 0.3559\n",
      "Eval loss 4.9526, accuracy 0.2017\n",
      "\n",
      "Epoch 158, lr = 0.001113\n",
      "Training loss 2.9419, accuracy 0.3546\n",
      "Eval loss 4.9983, accuracy 0.2060\n",
      "\n",
      "Epoch 159, lr = 0.001109\n",
      "Training loss 2.9479, accuracy 0.3559\n",
      "Eval loss 4.9748, accuracy 0.2049\n",
      "\n",
      "Epoch 160, lr = 0.001106\n",
      "Training loss 2.9598, accuracy 0.3523\n",
      "Eval loss 4.9479, accuracy 0.2069\n",
      "\n",
      "Epoch 161, lr = 0.001103\n",
      "Training loss 2.9459, accuracy 0.3578\n",
      "Eval loss 4.9626, accuracy 0.2038\n",
      "\n",
      "Epoch 162, lr = 0.001099\n",
      "Training loss 2.9412, accuracy 0.3562\n",
      "Eval loss 4.9589, accuracy 0.2068\n",
      "\n",
      "Epoch 163, lr = 0.001096\n",
      "Training loss 2.9384, accuracy 0.3583\n",
      "Eval loss 4.9532, accuracy 0.2056\n",
      "\n",
      "Epoch 164, lr = 0.001092\n",
      "Training loss 2.9238, accuracy 0.3581\n",
      "Eval loss 5.0060, accuracy 0.2053\n",
      "\n",
      "Epoch 165, lr = 0.001089\n",
      "Training loss 2.9330, accuracy 0.3595\n",
      "Eval loss 4.9794, accuracy 0.2036\n",
      "\n",
      "Epoch 166, lr = 0.001086\n",
      "Training loss 2.9231, accuracy 0.3583\n",
      "Eval loss 4.9980, accuracy 0.2046\n",
      "\n",
      "Epoch 167, lr = 0.001083\n",
      "Training loss 2.9206, accuracy 0.3582\n",
      "Eval loss 4.9985, accuracy 0.2060\n",
      "\n",
      "Epoch 168, lr = 0.001079\n",
      "Training loss 2.9190, accuracy 0.3623\n",
      "Eval loss 4.9917, accuracy 0.2034\n",
      "\n",
      "Epoch 169, lr = 0.001076\n",
      "Training loss 2.9345, accuracy 0.3627\n",
      "Eval loss 4.9882, accuracy 0.2008\n",
      "\n",
      "Epoch 170, lr = 0.001073\n",
      "Training loss 2.9082, accuracy 0.3609\n",
      "Eval loss 5.0127, accuracy 0.2051\n",
      "\n",
      "Epoch 171, lr = 0.001070\n",
      "Training loss 2.9096, accuracy 0.3600\n",
      "Eval loss 4.9888, accuracy 0.2054\n",
      "\n",
      "Epoch 172, lr = 0.001067\n",
      "Training loss 2.9007, accuracy 0.3629\n",
      "Eval loss 5.0153, accuracy 0.2032\n",
      "\n",
      "Epoch 173, lr = 0.001064\n",
      "Training loss 2.9054, accuracy 0.3638\n",
      "Eval loss 4.9862, accuracy 0.2038\n",
      "\n",
      "Epoch 174, lr = 0.001061\n",
      "Training loss 2.8960, accuracy 0.3625\n",
      "Eval loss 5.0166, accuracy 0.2048\n",
      "\n",
      "Epoch 175, lr = 0.001058\n",
      "Training loss 2.9040, accuracy 0.3625\n",
      "Eval loss 4.9985, accuracy 0.2035\n",
      "\n",
      "Epoch 176, lr = 0.001054\n",
      "Training loss 2.8973, accuracy 0.3656\n",
      "Eval loss 4.9946, accuracy 0.2023\n",
      "\n",
      "Epoch 177, lr = 0.001052\n",
      "Training loss 2.8914, accuracy 0.3669\n",
      "Eval loss 5.0069, accuracy 0.2018\n",
      "\n",
      "Epoch 178, lr = 0.001049\n",
      "Training loss 2.8929, accuracy 0.3625\n",
      "Eval loss 5.0085, accuracy 0.2054\n",
      "\n",
      "Epoch 179, lr = 0.001046\n",
      "Training loss 2.8830, accuracy 0.3677\n",
      "Eval loss 5.0274, accuracy 0.2024\n",
      "\n",
      "Epoch 180, lr = 0.001043\n",
      "Training loss 2.8878, accuracy 0.3682\n",
      "Eval loss 5.0060, accuracy 0.2018\n",
      "\n",
      "Epoch 181, lr = 0.001040\n",
      "Training loss 2.8839, accuracy 0.3618\n",
      "Eval loss 5.0217, accuracy 0.2073\n",
      "\n",
      "Epoch 182, lr = 0.001037\n",
      "Training loss 2.8846, accuracy 0.3671\n",
      "Eval loss 5.0137, accuracy 0.2019\n",
      "\n",
      "Epoch 183, lr = 0.001034\n",
      "Training loss 2.8788, accuracy 0.3683\n",
      "Eval loss 5.0101, accuracy 0.2021\n",
      "\n",
      "Epoch 184, lr = 0.001031\n",
      "Training loss 2.8806, accuracy 0.3672\n",
      "Eval loss 4.9977, accuracy 0.2032\n",
      "\n",
      "Epoch 185, lr = 0.001029\n",
      "Training loss 2.8710, accuracy 0.3674\n",
      "Eval loss 5.0283, accuracy 0.2041\n",
      "\n",
      "Epoch 186, lr = 0.001026\n",
      "Training loss 2.8759, accuracy 0.3677\n",
      "Eval loss 5.0253, accuracy 0.2019\n",
      "\n",
      "Epoch 187, lr = 0.001023\n",
      "Training loss 2.8704, accuracy 0.3666\n",
      "Eval loss 4.9995, accuracy 0.2057\n",
      "\n",
      "Epoch 188, lr = 0.001020\n",
      "Training loss 2.8603, accuracy 0.3715\n",
      "Eval loss 5.0424, accuracy 0.2012\n",
      "\n",
      "Epoch 189, lr = 0.001018\n",
      "Training loss 2.8705, accuracy 0.3702\n",
      "Eval loss 5.0311, accuracy 0.2004\n",
      "\n",
      "Epoch 190, lr = 0.001015\n",
      "Training loss 2.8545, accuracy 0.3706\n",
      "Eval loss 5.0411, accuracy 0.2026\n",
      "\n",
      "Epoch 191, lr = 0.001012\n",
      "Training loss 2.8586, accuracy 0.3708\n",
      "Eval loss 5.0252, accuracy 0.2029\n",
      "\n",
      "Epoch 192, lr = 0.001010\n",
      "Training loss 2.8612, accuracy 0.3705\n",
      "Eval loss 5.0175, accuracy 0.2032\n",
      "\n",
      "Epoch 193, lr = 0.001007\n",
      "Training loss 2.8532, accuracy 0.3713\n",
      "Eval loss 5.0387, accuracy 0.2022\n",
      "\n",
      "Epoch 194, lr = 0.001004\n",
      "Training loss 2.8489, accuracy 0.3702\n",
      "Eval loss 5.0524, accuracy 0.2040\n",
      "\n",
      "Epoch 195, lr = 0.001002\n",
      "Training loss 2.8525, accuracy 0.3690\n",
      "Eval loss 5.0224, accuracy 0.2055\n",
      "\n",
      "Epoch 196, lr = 0.000999\n",
      "Training loss 2.8335, accuracy 0.3739\n",
      "Eval loss 5.0712, accuracy 0.2018\n",
      "\n",
      "Epoch 197, lr = 0.000997\n",
      "Training loss 2.8433, accuracy 0.3720\n",
      "Eval loss 5.0394, accuracy 0.2035\n",
      "\n",
      "Epoch 198, lr = 0.000994\n",
      "Training loss 2.8360, accuracy 0.3718\n",
      "Eval loss 5.0377, accuracy 0.2058\n",
      "\n",
      "Epoch 199, lr = 0.000992\n",
      "Training loss 2.8373, accuracy 0.3747\n",
      "Eval loss 5.0346, accuracy 0.2024\n",
      "\n",
      "Epoch 200, lr = 0.000989\n",
      "Training loss 2.8471, accuracy 0.3749\n",
      "Eval loss 5.0408, accuracy 0.2000\n",
      "\n",
      "Epoch 201, lr = 0.000987\n",
      "Training loss 2.8370, accuracy 0.3702\n",
      "Eval loss 5.0514, accuracy 0.2051\n",
      "\n",
      "Epoch 202, lr = 0.000984\n",
      "Training loss 2.8452, accuracy 0.3742\n",
      "Eval loss 5.0158, accuracy 0.2015\n",
      "\n",
      "Epoch 203, lr = 0.000982\n",
      "Training loss 2.8320, accuracy 0.3756\n",
      "Eval loss 5.0484, accuracy 0.2019\n",
      "\n",
      "Epoch 204, lr = 0.000979\n",
      "Training loss 2.8360, accuracy 0.3749\n",
      "Eval loss 5.0322, accuracy 0.2025\n",
      "\n",
      "Epoch 205, lr = 0.000977\n",
      "Training loss 2.8400, accuracy 0.3758\n",
      "Eval loss 5.0249, accuracy 0.2006\n",
      "\n",
      "Epoch 206, lr = 0.000975\n",
      "Training loss 2.8196, accuracy 0.3729\n",
      "Eval loss 5.0666, accuracy 0.2035\n",
      "\n",
      "Epoch 207, lr = 0.000972\n",
      "Training loss 2.8162, accuracy 0.3766\n",
      "Eval loss 5.0596, accuracy 0.2036\n",
      "\n",
      "Epoch 208, lr = 0.000970\n",
      "Training loss 2.8246, accuracy 0.3746\n",
      "Eval loss 5.0414, accuracy 0.2036\n",
      "\n",
      "Epoch 209, lr = 0.000968\n",
      "Training loss 2.8237, accuracy 0.3728\n",
      "Eval loss 5.0548, accuracy 0.2048\n",
      "\n",
      "Epoch 210, lr = 0.000965\n",
      "Training loss 2.8281, accuracy 0.3787\n",
      "Eval loss 5.0474, accuracy 0.1992\n",
      "\n",
      "Epoch 211, lr = 0.000963\n",
      "Training loss 2.8200, accuracy 0.3781\n",
      "Eval loss 5.0578, accuracy 0.2006\n",
      "\n",
      "Epoch 212, lr = 0.000961\n",
      "Training loss 2.8039, accuracy 0.3773\n",
      "Eval loss 5.0754, accuracy 0.2037\n",
      "\n",
      "Epoch 213, lr = 0.000959\n",
      "Training loss 2.8198, accuracy 0.3786\n",
      "Eval loss 5.0397, accuracy 0.2010\n",
      "\n",
      "Epoch 214, lr = 0.000956\n",
      "Training loss 2.8137, accuracy 0.3777\n",
      "Eval loss 5.0393, accuracy 0.2024\n",
      "\n",
      "Epoch 215, lr = 0.000954\n",
      "Training loss 2.8030, accuracy 0.3796\n",
      "Eval loss 5.0559, accuracy 0.2023\n",
      "\n",
      "Epoch 216, lr = 0.000952\n",
      "Training loss 2.7999, accuracy 0.3799\n",
      "Eval loss 5.0780, accuracy 0.2018\n",
      "\n",
      "Epoch 217, lr = 0.000950\n",
      "Training loss 2.8033, accuracy 0.3802\n",
      "Eval loss 5.0621, accuracy 0.2012\n",
      "\n",
      "Epoch 218, lr = 0.000947\n",
      "Training loss 2.8049, accuracy 0.3765\n",
      "Eval loss 5.0609, accuracy 0.2042\n",
      "\n",
      "Epoch 219, lr = 0.000945\n",
      "Training loss 2.8065, accuracy 0.3795\n",
      "Eval loss 5.0385, accuracy 0.2014\n",
      "\n",
      "Epoch 220, lr = 0.000943\n",
      "Training loss 2.8000, accuracy 0.3778\n",
      "Eval loss 5.0552, accuracy 0.2046\n",
      "\n",
      "Epoch 221, lr = 0.000941\n",
      "Training loss 2.7929, accuracy 0.3801\n",
      "Eval loss 5.0602, accuracy 0.2040\n",
      "\n",
      "Epoch 222, lr = 0.000939\n",
      "Training loss 2.8004, accuracy 0.3806\n",
      "Eval loss 5.0419, accuracy 0.2020\n",
      "\n",
      "Epoch 223, lr = 0.000937\n",
      "Training loss 2.7900, accuracy 0.3804\n",
      "Eval loss 5.0808, accuracy 0.2027\n",
      "\n",
      "Epoch 224, lr = 0.000935\n",
      "Training loss 2.7918, accuracy 0.3799\n",
      "Eval loss 5.0489, accuracy 0.2036\n",
      "\n",
      "Epoch 225, lr = 0.000933\n",
      "Training loss 2.7874, accuracy 0.3835\n",
      "Eval loss 5.0588, accuracy 0.2007\n",
      "\n",
      "Epoch 226, lr = 0.000931\n",
      "Training loss 2.7975, accuracy 0.3842\n",
      "Eval loss 5.0754, accuracy 0.1975\n",
      "\n",
      "Epoch 227, lr = 0.000929\n",
      "Training loss 2.7940, accuracy 0.3835\n",
      "Eval loss 5.0623, accuracy 0.1986\n",
      "\n",
      "Epoch 228, lr = 0.000926\n",
      "Training loss 2.7895, accuracy 0.3842\n",
      "Eval loss 5.0767, accuracy 0.1997\n",
      "\n",
      "Epoch 229, lr = 0.000924\n",
      "Training loss 2.7912, accuracy 0.3842\n",
      "Eval loss 5.0709, accuracy 0.1993\n",
      "\n",
      "Epoch 230, lr = 0.000922\n",
      "Training loss 2.7727, accuracy 0.3825\n",
      "Eval loss 5.0769, accuracy 0.2035\n",
      "\n",
      "Epoch 231, lr = 0.000920\n",
      "Training loss 2.7767, accuracy 0.3860\n",
      "Eval loss 5.0894, accuracy 0.1988\n",
      "\n",
      "Epoch 232, lr = 0.000918\n",
      "Training loss 2.7799, accuracy 0.3850\n",
      "Eval loss 5.0663, accuracy 0.1996\n",
      "\n",
      "Epoch 233, lr = 0.000916\n",
      "Training loss 2.7723, accuracy 0.3818\n",
      "Eval loss 5.0755, accuracy 0.2037\n",
      "\n",
      "Epoch 234, lr = 0.000915\n",
      "Training loss 2.7645, accuracy 0.3846\n",
      "Eval loss 5.0843, accuracy 0.2031\n",
      "\n",
      "Epoch 235, lr = 0.000913\n",
      "Training loss 2.7746, accuracy 0.3872\n",
      "Eval loss 5.0799, accuracy 0.1984\n",
      "\n",
      "Epoch 236, lr = 0.000911\n",
      "Training loss 2.7692, accuracy 0.3870\n",
      "Eval loss 5.0813, accuracy 0.1989\n",
      "\n",
      "Epoch 237, lr = 0.000909\n",
      "Training loss 2.7747, accuracy 0.3849\n",
      "Eval loss 5.0676, accuracy 0.2015\n",
      "\n",
      "Epoch 238, lr = 0.000907\n",
      "Training loss 2.7627, accuracy 0.3850\n",
      "Eval loss 5.0824, accuracy 0.2025\n",
      "\n",
      "Epoch 239, lr = 0.000905\n",
      "Training loss 2.7686, accuracy 0.3869\n",
      "Eval loss 5.0842, accuracy 0.2006\n",
      "\n",
      "Epoch 240, lr = 0.000903\n",
      "Training loss 2.7499, accuracy 0.3888\n",
      "Eval loss 5.1098, accuracy 0.2004\n",
      "\n",
      "Epoch 241, lr = 0.000901\n",
      "Training loss 2.7628, accuracy 0.3875\n",
      "Eval loss 5.0756, accuracy 0.2004\n",
      "\n",
      "Epoch 242, lr = 0.000899\n",
      "Training loss 2.7521, accuracy 0.3869\n",
      "Eval loss 5.0961, accuracy 0.2025\n",
      "\n",
      "Epoch 243, lr = 0.000897\n",
      "Training loss 2.7541, accuracy 0.3865\n",
      "Eval loss 5.0838, accuracy 0.2020\n",
      "\n",
      "Epoch 244, lr = 0.000896\n",
      "Training loss 2.7670, accuracy 0.3883\n",
      "Eval loss 5.0913, accuracy 0.1986\n",
      "\n",
      "Epoch 245, lr = 0.000894\n",
      "Training loss 2.7595, accuracy 0.3896\n",
      "Eval loss 5.0704, accuracy 0.1993\n",
      "\n",
      "Epoch 246, lr = 0.000892\n",
      "Training loss 2.7585, accuracy 0.3905\n",
      "Eval loss 5.1109, accuracy 0.1968\n",
      "\n",
      "Epoch 247, lr = 0.000890\n",
      "Training loss 2.7623, accuracy 0.3872\n",
      "Eval loss 5.0497, accuracy 0.2017\n",
      "\n",
      "Epoch 248, lr = 0.000888\n",
      "Training loss 2.7465, accuracy 0.3879\n",
      "Eval loss 5.0755, accuracy 0.2033\n",
      "\n",
      "Epoch 249, lr = 0.000887\n",
      "Training loss 2.7498, accuracy 0.3886\n",
      "Eval loss 5.0864, accuracy 0.2007\n",
      "\n",
      "Epoch 250, lr = 0.000885\n",
      "Training loss 2.7522, accuracy 0.3884\n",
      "Eval loss 5.0740, accuracy 0.2022\n",
      "\n",
      "Epoch 251, lr = 0.000883\n",
      "Training loss 2.7439, accuracy 0.3905\n",
      "Eval loss 5.1034, accuracy 0.1987\n",
      "\n",
      "Epoch 252, lr = 0.000881\n",
      "Training loss 2.7426, accuracy 0.3917\n",
      "Eval loss 5.0925, accuracy 0.1979\n",
      "\n",
      "Epoch 253, lr = 0.000880\n",
      "Training loss 2.7471, accuracy 0.3913\n",
      "Eval loss 5.1141, accuracy 0.1963\n",
      "\n",
      "Epoch 254, lr = 0.000878\n",
      "Training loss 2.7388, accuracy 0.3910\n",
      "Eval loss 5.0837, accuracy 0.2008\n",
      "\n",
      "Epoch 255, lr = 0.000876\n",
      "Training loss 2.7626, accuracy 0.3915\n",
      "Eval loss 5.0630, accuracy 0.1964\n",
      "\n",
      "Epoch 256, lr = 0.000874\n",
      "Training loss 2.7421, accuracy 0.3927\n",
      "Eval loss 5.0870, accuracy 0.1975\n",
      "\n",
      "Epoch 257, lr = 0.000873\n",
      "Training loss 2.7323, accuracy 0.3918\n",
      "Eval loss 5.0975, accuracy 0.2007\n",
      "\n",
      "Epoch 258, lr = 0.000871\n",
      "Training loss 2.7400, accuracy 0.3866\n",
      "Eval loss 5.0805, accuracy 0.2034\n",
      "\n",
      "Epoch 259, lr = 0.000869\n",
      "Training loss 2.7209, accuracy 0.3902\n",
      "Eval loss 5.1067, accuracy 0.2036\n",
      "\n",
      "Epoch 260, lr = 0.000868\n",
      "Training loss 2.7318, accuracy 0.3910\n",
      "Eval loss 5.1096, accuracy 0.2010\n",
      "\n",
      "Epoch 261, lr = 0.000866\n",
      "Training loss 2.7463, accuracy 0.3930\n",
      "Eval loss 5.0642, accuracy 0.1999\n",
      "\n",
      "Epoch 262, lr = 0.000864\n",
      "Training loss 2.7337, accuracy 0.3891\n",
      "Eval loss 5.0934, accuracy 0.2028\n",
      "\n",
      "Epoch 263, lr = 0.000863\n",
      "Training loss 2.7382, accuracy 0.3927\n",
      "Eval loss 5.0881, accuracy 0.1989\n",
      "\n",
      "Epoch 264, lr = 0.000861\n",
      "Training loss 2.7319, accuracy 0.3924\n",
      "Eval loss 5.0802, accuracy 0.2011\n",
      "\n",
      "Epoch 265, lr = 0.000859\n",
      "Training loss 2.7272, accuracy 0.3910\n",
      "Eval loss 5.0933, accuracy 0.2022\n",
      "\n",
      "Epoch 266, lr = 0.000858\n",
      "Training loss 2.7200, accuracy 0.3895\n",
      "Eval loss 5.1136, accuracy 0.2028\n",
      "\n",
      "Epoch 267, lr = 0.000856\n",
      "Training loss 2.7193, accuracy 0.3923\n",
      "Eval loss 5.1051, accuracy 0.2013\n",
      "\n",
      "Epoch 268, lr = 0.000855\n",
      "Training loss 2.7222, accuracy 0.3940\n",
      "Eval loss 5.0819, accuracy 0.2011\n",
      "\n",
      "Epoch 269, lr = 0.000853\n",
      "Training loss 2.7237, accuracy 0.3915\n",
      "Eval loss 5.0757, accuracy 0.2032\n",
      "\n",
      "Epoch 270, lr = 0.000851\n",
      "Training loss 2.7176, accuracy 0.3921\n",
      "Eval loss 5.0946, accuracy 0.2023\n",
      "\n",
      "Epoch 271, lr = 0.000850\n",
      "Training loss 2.7176, accuracy 0.3927\n",
      "Eval loss 5.0971, accuracy 0.2012\n",
      "\n",
      "Epoch 272, lr = 0.000848\n",
      "Training loss 2.7287, accuracy 0.3958\n",
      "Eval loss 5.1064, accuracy 0.1958\n",
      "\n",
      "Epoch 273, lr = 0.000847\n",
      "Training loss 2.7185, accuracy 0.3899\n",
      "Eval loss 5.1053, accuracy 0.2034\n",
      "\n",
      "Epoch 274, lr = 0.000845\n",
      "Training loss 2.7058, accuracy 0.3954\n",
      "Eval loss 5.1290, accuracy 0.2000\n",
      "\n",
      "Epoch 275, lr = 0.000844\n",
      "Training loss 2.7104, accuracy 0.3955\n",
      "Eval loss 5.0993, accuracy 0.2008\n",
      "\n",
      "Epoch 276, lr = 0.000842\n",
      "Training loss 2.7043, accuracy 0.3927\n",
      "Eval loss 5.1346, accuracy 0.2024\n",
      "\n",
      "Epoch 277, lr = 0.000841\n",
      "Training loss 2.7110, accuracy 0.3971\n",
      "Eval loss 5.0986, accuracy 0.1996\n",
      "\n",
      "Epoch 278, lr = 0.000839\n",
      "Training loss 2.7097, accuracy 0.3962\n",
      "Eval loss 5.0933, accuracy 0.2003\n",
      "\n",
      "Epoch 279, lr = 0.000838\n",
      "Training loss 2.7093, accuracy 0.3947\n",
      "Eval loss 5.0980, accuracy 0.2018\n",
      "\n",
      "Epoch 280, lr = 0.000836\n",
      "Training loss 2.7016, accuracy 0.3950\n",
      "Eval loss 5.1023, accuracy 0.2015\n",
      "\n",
      "Epoch 281, lr = 0.000835\n",
      "Training loss 2.7057, accuracy 0.3956\n",
      "Eval loss 5.1030, accuracy 0.2004\n",
      "\n",
      "Epoch 282, lr = 0.000833\n",
      "Training loss 2.7110, accuracy 0.3941\n",
      "Eval loss 5.0784, accuracy 0.2019\n",
      "\n",
      "Epoch 283, lr = 0.000832\n",
      "Training loss 2.7122, accuracy 0.3928\n",
      "Eval loss 5.0650, accuracy 0.2031\n",
      "\n",
      "Epoch 284, lr = 0.000830\n",
      "Training loss 2.6985, accuracy 0.3937\n",
      "Eval loss 5.1250, accuracy 0.2014\n",
      "\n",
      "Epoch 285, lr = 0.000829\n",
      "Training loss 2.7020, accuracy 0.3978\n",
      "Eval loss 5.1160, accuracy 0.1989\n",
      "\n",
      "Epoch 286, lr = 0.000827\n",
      "Training loss 2.7061, accuracy 0.3970\n",
      "Eval loss 5.0877, accuracy 0.1998\n",
      "\n",
      "Epoch 287, lr = 0.000826\n",
      "Training loss 2.6945, accuracy 0.3988\n",
      "Eval loss 5.0957, accuracy 0.2003\n",
      "\n",
      "Epoch 288, lr = 0.000824\n",
      "Training loss 2.6988, accuracy 0.3942\n",
      "Eval loss 5.0900, accuracy 0.2033\n",
      "\n",
      "Epoch 289, lr = 0.000823\n",
      "Training loss 2.7022, accuracy 0.3983\n",
      "Eval loss 5.1036, accuracy 0.1988\n",
      "\n",
      "Epoch 290, lr = 0.000821\n",
      "Training loss 2.6977, accuracy 0.3991\n",
      "Eval loss 5.1142, accuracy 0.1981\n",
      "\n",
      "Epoch 291, lr = 0.000820\n",
      "Training loss 2.6924, accuracy 0.3958\n",
      "Eval loss 5.0986, accuracy 0.2024\n",
      "\n",
      "Epoch 292, lr = 0.000819\n",
      "Training loss 2.6857, accuracy 0.3981\n",
      "Eval loss 5.1317, accuracy 0.2000\n",
      "\n",
      "Epoch 293, lr = 0.000817\n",
      "Training loss 2.6848, accuracy 0.3959\n",
      "Eval loss 5.1305, accuracy 0.2026\n",
      "\n",
      "Epoch 294, lr = 0.000816\n",
      "Training loss 2.6948, accuracy 0.4004\n",
      "Eval loss 5.1123, accuracy 0.1983\n",
      "\n",
      "Epoch 295, lr = 0.000814\n",
      "Training loss 2.6935, accuracy 0.4008\n",
      "Eval loss 5.1328, accuracy 0.1958\n",
      "\n",
      "Epoch 296, lr = 0.000813\n",
      "Training loss 2.6898, accuracy 0.3963\n",
      "Eval loss 5.1087, accuracy 0.2022\n",
      "\n",
      "Epoch 297, lr = 0.000812\n",
      "Training loss 2.6851, accuracy 0.3973\n",
      "Eval loss 5.1019, accuracy 0.2015\n",
      "\n",
      "Epoch 298, lr = 0.000810\n",
      "Training loss 2.6930, accuracy 0.3996\n",
      "Eval loss 5.1007, accuracy 0.1994\n",
      "\n",
      "Epoch 299, lr = 0.000809\n",
      "Training loss 2.6827, accuracy 0.3994\n",
      "Eval loss 5.1116, accuracy 0.2005\n"
     ]
    }
   ],
   "source": [
    "training_loop(config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfe009d07ae4a5fb8d65a0b7a6c24c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 300, lr = 0.000808\n",
      "Training loss 2.6902, accuracy 0.3949\n",
      "Eval loss 5.0956, accuracy 0.2037\n",
      "\n",
      "Epoch 301, lr = 0.000806\n",
      "Training loss 2.6750, accuracy 0.4015\n",
      "Eval loss 5.1299, accuracy 0.1999\n",
      "\n",
      "Epoch 302, lr = 0.000805\n",
      "Training loss 2.6843, accuracy 0.3988\n",
      "Eval loss 5.0984, accuracy 0.2015\n",
      "\n",
      "Epoch 303, lr = 0.000804\n",
      "Training loss 2.6897, accuracy 0.3999\n",
      "Eval loss 5.0831, accuracy 0.1999\n",
      "\n",
      "Epoch 304, lr = 0.000802\n",
      "Training loss 2.6714, accuracy 0.4022\n",
      "Eval loss 5.1234, accuracy 0.2001\n",
      "\n",
      "Epoch 305, lr = 0.000801\n",
      "Training loss 2.6984, accuracy 0.4019\n",
      "Eval loss 5.1194, accuracy 0.1958\n",
      "\n",
      "Epoch 306, lr = 0.000800\n",
      "Training loss 2.6889, accuracy 0.3990\n",
      "Eval loss 5.0919, accuracy 0.2007\n",
      "\n",
      "Epoch 307, lr = 0.000798\n",
      "Training loss 2.6795, accuracy 0.4001\n",
      "Eval loss 5.0978, accuracy 0.2019\n",
      "\n",
      "Epoch 308, lr = 0.000797\n",
      "Training loss 2.6733, accuracy 0.3999\n",
      "Eval loss 5.1074, accuracy 0.2014\n",
      "\n",
      "Epoch 309, lr = 0.000796\n",
      "Training loss 2.6738, accuracy 0.3995\n",
      "Eval loss 5.1012, accuracy 0.2023\n",
      "\n",
      "Epoch 310, lr = 0.000795\n",
      "Training loss 2.6742, accuracy 0.4017\n",
      "Eval loss 5.1068, accuracy 0.1992\n",
      "\n",
      "Epoch 311, lr = 0.000793\n",
      "Training loss 2.6781, accuracy 0.4027\n",
      "Eval loss 5.1146, accuracy 0.1974\n",
      "\n",
      "Epoch 312, lr = 0.000792\n",
      "Training loss 2.6877, accuracy 0.4032\n",
      "Eval loss 5.1292, accuracy 0.1948\n",
      "\n",
      "Epoch 313, lr = 0.000791\n",
      "Training loss 2.6704, accuracy 0.4024\n",
      "Eval loss 5.1173, accuracy 0.1993\n",
      "\n",
      "Epoch 314, lr = 0.000789\n",
      "Training loss 2.6672, accuracy 0.3975\n",
      "Eval loss 5.1235, accuracy 0.2029\n",
      "\n",
      "Epoch 315, lr = 0.000788\n",
      "Training loss 2.6676, accuracy 0.4040\n",
      "Eval loss 5.1258, accuracy 0.1974\n",
      "\n",
      "Epoch 316, lr = 0.000787\n",
      "Training loss 2.6651, accuracy 0.4003\n",
      "Eval loss 5.1105, accuracy 0.2021\n",
      "\n",
      "Epoch 317, lr = 0.000786\n",
      "Training loss 2.6541, accuracy 0.4022\n",
      "Eval loss 5.1290, accuracy 0.2008\n",
      "\n",
      "Epoch 318, lr = 0.000784\n",
      "Training loss 2.6660, accuracy 0.4048\n",
      "Eval loss 5.1226, accuracy 0.1970\n",
      "\n",
      "Epoch 319, lr = 0.000783\n",
      "Training loss 2.6646, accuracy 0.4033\n",
      "Eval loss 5.1235, accuracy 0.1987\n",
      "\n",
      "Epoch 320, lr = 0.000782\n",
      "Training loss 2.6530, accuracy 0.4009\n",
      "Eval loss 5.1472, accuracy 0.2024\n",
      "\n",
      "Epoch 321, lr = 0.000781\n",
      "Training loss 2.6667, accuracy 0.4044\n",
      "Eval loss 5.0994, accuracy 0.1990\n",
      "\n",
      "Epoch 322, lr = 0.000780\n",
      "Training loss 2.6592, accuracy 0.3993\n",
      "Eval loss 5.1271, accuracy 0.2032\n",
      "\n",
      "Epoch 323, lr = 0.000778\n",
      "Training loss 2.6571, accuracy 0.4021\n",
      "Eval loss 5.1125, accuracy 0.2010\n",
      "\n",
      "Epoch 324, lr = 0.000777\n",
      "Training loss 2.6541, accuracy 0.4018\n",
      "Eval loss 5.1230, accuracy 0.2031\n",
      "\n",
      "Epoch 325, lr = 0.000776\n",
      "Training loss 2.6581, accuracy 0.4065\n",
      "Eval loss 5.1159, accuracy 0.1974\n",
      "\n",
      "Epoch 326, lr = 0.000775\n",
      "Training loss 2.6490, accuracy 0.4037\n",
      "Eval loss 5.1237, accuracy 0.2009\n",
      "\n",
      "Epoch 327, lr = 0.000774\n",
      "Training loss 2.6452, accuracy 0.4037\n",
      "Eval loss 5.1414, accuracy 0.2011\n",
      "\n",
      "Epoch 328, lr = 0.000772\n",
      "Training loss 2.6399, accuracy 0.4049\n",
      "Eval loss 5.1430, accuracy 0.2010\n",
      "\n",
      "Epoch 329, lr = 0.000771\n",
      "Training loss 2.6542, accuracy 0.4062\n",
      "Eval loss 5.1212, accuracy 0.1969\n",
      "\n",
      "Epoch 330, lr = 0.000770\n",
      "Training loss 2.6473, accuracy 0.4061\n",
      "Eval loss 5.1215, accuracy 0.1989\n",
      "\n",
      "Epoch 331, lr = 0.000769\n",
      "Training loss 2.6496, accuracy 0.4064\n",
      "Eval loss 5.1013, accuracy 0.1994\n",
      "\n",
      "Epoch 332, lr = 0.000768\n",
      "Training loss 2.6404, accuracy 0.4065\n",
      "Eval loss 5.1421, accuracy 0.1995\n",
      "\n",
      "Epoch 333, lr = 0.000767\n",
      "Training loss 2.6394, accuracy 0.4040\n",
      "Eval loss 5.1468, accuracy 0.2015\n",
      "\n",
      "Epoch 334, lr = 0.000765\n",
      "Training loss 2.6441, accuracy 0.4054\n",
      "Eval loss 5.1271, accuracy 0.1998\n",
      "\n",
      "Epoch 335, lr = 0.000764\n",
      "Training loss 2.6389, accuracy 0.4062\n",
      "Eval loss 5.1127, accuracy 0.2007\n",
      "\n",
      "Epoch 336, lr = 0.000763\n",
      "Training loss 2.6341, accuracy 0.4082\n",
      "Eval loss 5.1312, accuracy 0.2002\n",
      "\n",
      "Epoch 337, lr = 0.000762\n",
      "Training loss 2.6350, accuracy 0.4054\n",
      "Eval loss 5.1306, accuracy 0.2005\n",
      "\n",
      "Epoch 338, lr = 0.000761\n",
      "Training loss 2.6353, accuracy 0.4064\n",
      "Eval loss 5.1345, accuracy 0.2006\n",
      "\n",
      "Epoch 339, lr = 0.000760\n",
      "Training loss 2.6394, accuracy 0.4092\n",
      "Eval loss 5.1364, accuracy 0.1968\n",
      "\n",
      "Epoch 340, lr = 0.000759\n",
      "Training loss 2.6369, accuracy 0.4069\n",
      "Eval loss 5.1170, accuracy 0.2014\n",
      "\n",
      "Epoch 341, lr = 0.000758\n",
      "Training loss 2.6370, accuracy 0.4087\n",
      "Eval loss 5.1480, accuracy 0.1968\n",
      "\n",
      "Epoch 342, lr = 0.000756\n",
      "Training loss 2.6380, accuracy 0.4048\n",
      "Eval loss 5.1240, accuracy 0.2012\n",
      "\n",
      "Epoch 343, lr = 0.000755\n",
      "Training loss 2.6511, accuracy 0.4081\n",
      "Eval loss 5.1325, accuracy 0.1957\n",
      "\n",
      "Epoch 344, lr = 0.000754\n",
      "Training loss 2.6360, accuracy 0.4080\n",
      "Eval loss 5.1167, accuracy 0.1998\n",
      "\n",
      "Epoch 345, lr = 0.000753\n",
      "Training loss 2.6325, accuracy 0.4079\n",
      "Eval loss 5.1425, accuracy 0.1981\n",
      "\n",
      "Epoch 346, lr = 0.000752\n",
      "Training loss 2.6308, accuracy 0.4077\n",
      "Eval loss 5.1463, accuracy 0.2004\n",
      "\n",
      "Epoch 347, lr = 0.000751\n",
      "Training loss 2.6386, accuracy 0.4083\n",
      "Eval loss 5.1192, accuracy 0.1992\n",
      "\n",
      "Epoch 348, lr = 0.000750\n",
      "Training loss 2.6321, accuracy 0.4060\n",
      "Eval loss 5.1256, accuracy 0.2018\n",
      "\n",
      "Epoch 349, lr = 0.000749\n",
      "Training loss 2.6325, accuracy 0.4112\n",
      "Eval loss 5.1504, accuracy 0.1965\n",
      "\n",
      "Epoch 350, lr = 0.000748\n",
      "Training loss 2.6366, accuracy 0.4102\n",
      "Eval loss 5.1132, accuracy 0.1978\n",
      "\n",
      "Epoch 351, lr = 0.000747\n",
      "Training loss 2.6288, accuracy 0.4097\n",
      "Eval loss 5.1323, accuracy 0.1998\n",
      "\n",
      "Epoch 352, lr = 0.000746\n",
      "Training loss 2.6231, accuracy 0.4095\n",
      "Eval loss 5.1330, accuracy 0.2003\n",
      "\n",
      "Epoch 353, lr = 0.000745\n",
      "Training loss 2.6332, accuracy 0.4115\n",
      "Eval loss 5.1205, accuracy 0.1973\n",
      "\n",
      "Epoch 354, lr = 0.000744\n",
      "Training loss 2.6237, accuracy 0.4101\n",
      "Eval loss 5.1448, accuracy 0.1988\n",
      "\n",
      "Epoch 355, lr = 0.000742\n",
      "Training loss 2.6204, accuracy 0.4108\n",
      "Eval loss 5.1560, accuracy 0.1982\n",
      "\n",
      "Epoch 356, lr = 0.000741\n",
      "Training loss 2.6232, accuracy 0.4081\n",
      "Eval loss 5.1189, accuracy 0.2017\n",
      "\n",
      "Epoch 357, lr = 0.000740\n",
      "Training loss 2.6306, accuracy 0.4095\n",
      "Eval loss 5.1083, accuracy 0.2002\n",
      "\n",
      "Epoch 358, lr = 0.000739\n",
      "Training loss 2.6178, accuracy 0.4072\n",
      "Eval loss 5.1446, accuracy 0.2016\n",
      "\n",
      "Epoch 359, lr = 0.000738\n",
      "Training loss 2.6258, accuracy 0.4093\n",
      "Eval loss 5.1159, accuracy 0.1996\n",
      "\n",
      "Epoch 360, lr = 0.000737\n",
      "Training loss 2.6237, accuracy 0.4141\n",
      "Eval loss 5.1467, accuracy 0.1961\n",
      "\n",
      "Epoch 361, lr = 0.000736\n",
      "Training loss 2.6165, accuracy 0.4082\n",
      "Eval loss 5.1473, accuracy 0.2010\n",
      "\n",
      "Epoch 362, lr = 0.000735\n",
      "Training loss 2.6141, accuracy 0.4128\n",
      "Eval loss 5.1506, accuracy 0.1979\n",
      "\n",
      "Epoch 363, lr = 0.000734\n",
      "Training loss 2.6195, accuracy 0.4122\n",
      "Eval loss 5.1503, accuracy 0.1965\n",
      "\n",
      "Epoch 364, lr = 0.000733\n",
      "Training loss 2.6433, accuracy 0.4123\n",
      "Eval loss 5.1534, accuracy 0.1931\n",
      "\n",
      "Epoch 365, lr = 0.000732\n",
      "Training loss 2.6234, accuracy 0.4125\n",
      "Eval loss 5.1469, accuracy 0.1949\n",
      "\n",
      "Epoch 366, lr = 0.000731\n",
      "Training loss 2.6092, accuracy 0.4114\n",
      "Eval loss 5.1393, accuracy 0.2006\n",
      "\n",
      "Epoch 367, lr = 0.000730\n",
      "Training loss 2.6220, accuracy 0.4127\n",
      "Eval loss 5.1302, accuracy 0.1971\n",
      "\n",
      "Epoch 368, lr = 0.000729\n",
      "Training loss 2.6150, accuracy 0.4125\n",
      "Eval loss 5.1303, accuracy 0.1988\n",
      "\n",
      "Epoch 369, lr = 0.000728\n",
      "Training loss 2.6118, accuracy 0.4123\n",
      "Eval loss 5.1367, accuracy 0.1988\n",
      "\n",
      "Epoch 370, lr = 0.000727\n",
      "Training loss 2.6063, accuracy 0.4121\n",
      "Eval loss 5.1360, accuracy 0.2002\n",
      "\n",
      "Epoch 371, lr = 0.000726\n",
      "Training loss 2.6119, accuracy 0.4132\n",
      "Eval loss 5.1559, accuracy 0.1979\n",
      "\n",
      "Epoch 372, lr = 0.000725\n",
      "Training loss 2.6247, accuracy 0.4131\n",
      "Eval loss 5.1367, accuracy 0.1953\n",
      "\n",
      "Epoch 373, lr = 0.000724\n",
      "Training loss 2.6014, accuracy 0.4149\n",
      "Eval loss 5.1699, accuracy 0.1979\n",
      "\n",
      "Epoch 374, lr = 0.000723\n",
      "Training loss 2.6256, accuracy 0.4160\n",
      "Eval loss 5.1445, accuracy 0.1929\n",
      "\n",
      "Epoch 375, lr = 0.000722\n",
      "Training loss 2.6011, accuracy 0.4144\n",
      "Eval loss 5.1486, accuracy 0.1976\n",
      "\n",
      "Epoch 376, lr = 0.000721\n",
      "Training loss 2.5979, accuracy 0.4148\n",
      "Eval loss 5.1591, accuracy 0.1983\n",
      "\n",
      "Epoch 377, lr = 0.000720\n",
      "Training loss 2.6042, accuracy 0.4095\n",
      "Eval loss 5.1415, accuracy 0.2021\n",
      "\n",
      "Epoch 378, lr = 0.000720\n",
      "Training loss 2.6074, accuracy 0.4141\n",
      "Eval loss 5.1601, accuracy 0.1965\n",
      "\n",
      "Epoch 379, lr = 0.000719\n",
      "Training loss 2.6004, accuracy 0.4160\n",
      "Eval loss 5.1524, accuracy 0.1964\n",
      "\n",
      "Epoch 380, lr = 0.000718\n",
      "Training loss 2.5921, accuracy 0.4153\n",
      "Eval loss 5.1717, accuracy 0.1980\n",
      "\n",
      "Epoch 381, lr = 0.000717\n",
      "Training loss 2.5962, accuracy 0.4131\n",
      "Eval loss 5.1658, accuracy 0.1990\n",
      "\n",
      "Epoch 382, lr = 0.000716\n",
      "Training loss 2.6116, accuracy 0.4161\n",
      "Eval loss 5.1655, accuracy 0.1938\n",
      "\n",
      "Epoch 383, lr = 0.000715\n",
      "Training loss 2.6034, accuracy 0.4144\n",
      "Eval loss 5.1383, accuracy 0.1989\n",
      "\n",
      "Epoch 384, lr = 0.000714\n",
      "Training loss 2.5994, accuracy 0.4139\n",
      "Eval loss 5.1446, accuracy 0.1991\n",
      "\n",
      "Epoch 385, lr = 0.000713\n",
      "Training loss 2.5989, accuracy 0.4110\n",
      "Eval loss 5.1428, accuracy 0.2008\n",
      "\n",
      "Epoch 386, lr = 0.000712\n",
      "Training loss 2.5880, accuracy 0.4160\n",
      "Eval loss 5.1507, accuracy 0.1990\n",
      "\n",
      "Epoch 387, lr = 0.000711\n",
      "Training loss 2.5983, accuracy 0.4155\n",
      "Eval loss 5.1485, accuracy 0.1964\n",
      "\n",
      "Epoch 388, lr = 0.000710\n",
      "Training loss 2.5899, accuracy 0.4166\n",
      "Eval loss 5.1470, accuracy 0.1972\n",
      "\n",
      "Epoch 389, lr = 0.000709\n",
      "Training loss 2.5812, accuracy 0.4185\n",
      "Eval loss 5.1616, accuracy 0.1974\n",
      "\n",
      "Epoch 390, lr = 0.000708\n",
      "Training loss 2.5834, accuracy 0.4152\n",
      "Eval loss 5.1695, accuracy 0.2002\n",
      "\n",
      "Epoch 391, lr = 0.000707\n",
      "Training loss 2.5876, accuracy 0.4152\n",
      "Eval loss 5.1569, accuracy 0.1994\n",
      "\n",
      "Epoch 392, lr = 0.000707\n",
      "Training loss 2.5875, accuracy 0.4129\n",
      "Eval loss 5.1780, accuracy 0.2001\n",
      "\n",
      "Epoch 393, lr = 0.000706\n",
      "Training loss 2.5828, accuracy 0.4158\n",
      "Eval loss 5.1613, accuracy 0.1998\n",
      "\n",
      "Epoch 394, lr = 0.000705\n",
      "Training loss 2.5866, accuracy 0.4153\n",
      "Eval loss 5.1425, accuracy 0.1995\n",
      "\n",
      "Epoch 395, lr = 0.000704\n",
      "Training loss 2.5921, accuracy 0.4172\n",
      "Eval loss 5.1408, accuracy 0.1967\n",
      "\n",
      "Epoch 396, lr = 0.000703\n",
      "Training loss 2.5830, accuracy 0.4163\n",
      "Eval loss 5.1529, accuracy 0.1999\n",
      "\n",
      "Epoch 397, lr = 0.000702\n",
      "Training loss 2.5851, accuracy 0.4098\n",
      "Eval loss 5.1830, accuracy 0.2020\n",
      "\n",
      "Epoch 398, lr = 0.000701\n",
      "Training loss 2.5751, accuracy 0.4160\n",
      "Eval loss 5.1678, accuracy 0.2000\n",
      "\n",
      "Epoch 399, lr = 0.000700\n",
      "Training loss 2.5803, accuracy 0.4137\n",
      "Eval loss 5.1750, accuracy 0.2015\n"
     ]
    }
   ],
   "source": [
    "training_loop(100, config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td></td></tr><tr><td>step</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.00064</td></tr><tr><td>step</td><td>240518</td></tr><tr><td>train_acc</td><td>0.42933</td></tr><tr><td>train_loss</td><td>2.50929</td></tr><tr><td>val_acc</td><td>0.19778</td></tr><tr><td>val_loss</td><td>5.19866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-eon-7</strong> at: <a href='https://wandb.ai/lovis/basic-transformer/runs/qh607w2f' target=\"_blank\">https://wandb.ai/lovis/basic-transformer/runs/qh607w2f</a><br/>Synced 6 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230420_102236-qh607w2f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(transformer.state_dict(), run_data_path + \"/model.pt\")\n",
    "model_artifact.add_file(run_data_path + \"/model.pt\")\n",
    "wandb.log_artifact(model_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30]) torch.Size([64, 30])\n",
      "True: <unknown> since i saw you last that , i fear me , will never out of my bones . i shall not fear fly - blowing . sebastian . why\n",
      "Prediction: <unknown> , he saw him in , you with wood , , i be be of my <unknown> today come have be be my . <unknown> me come . i\n",
      "True: heavens bless my lord from fell aufidius ! volumnia . hell beat aufidius head below his knee and tread upon his neck . re - enter gentlewoman , with valeria\n",
      "Prediction: i , my breast of hence asleep ! enter . i make him out against . <unknown> . <unknown> upon him horns . volumnia - enter volumnia caius with a\n",
      "True: emperor ; and say long live our emperor saturnine ! a long flourish till they come down saturninus . titus andronicus , for thy favours done to us in our\n",
      "Prediction: <unknown> , and , you is thou general , , exeunt friend flourish . aaron have to , . o , , farewell my brothers thou , slay all the\n",
      "True: you ; yet be leaden footed , till his great rage be off him . <unknown> , when he broke his <unknown> and <unknown> against the horses of the sun\n",
      "Prediction: the , and i gone , , and long <unknown> date yields <unknown> , , don . <unknown> he will his pate , <unknown> , the gates of his gates\n",
      "True: scroop , hastings and all are brought to the correction of your law . there is not now a rebels sword <unknown> , but peace puts forth her <unknown> everywhere\n",
      "Prediction: of . archbishop , other lieutenant won to richmond duke of king brother . king is no lords in messenger mind in at and we , on the subjects .\n",
      "True: thy tongue will not confess thy error . plantagenet . hath not thy rose a canker , somerset ? somerset . hath not thy rose a thorn , plantagenet ?\n",
      "Prediction: i <unknown> is not be . name . pisanio . i bolingbroke any father ? white made but , plantagenet . no <unknown> thy face <unknown> white , a ,\n",
      "True: be more <unknown> at , by breaking through the foul and ugly <unknown> of vapours that did seem to <unknown> him . if all the year were playing <unknown> ,\n",
      "Prediction: be <unknown> <unknown> . <unknown> and the time the time <unknown> <unknown> sun of the . may possess to drown with with but you the world of touchd in ,\n",
      "True: if thou didst ever hold me in thy heart , absent thee from <unknown> awhile , and in this harsh world draw thy breath in pain , to tell my\n",
      "Prediction: i i shouldst deny see , down thy masters , thou boy with thy , . kill take thy case fray thou me sword . thy of and <unknown> thee\n",
      "True: most precious queen and children are even now to be <unknown> <unknown> . say to me , when <unknown> thou the prince florizel , my son ? kings are no\n",
      "Prediction: <unknown> <unknown> foe . <unknown> , , in returnd the found , , but , the what laertes i lay <unknown> priest , , i father , mamillius , not\n",
      "True: am as able and as fit as thou to serve and to deserve my mistress grace ; and that my sword upon thee shall approve , and plead my passions\n",
      "Prediction: am a <unknown> to <unknown> a . i art say . die be a kindness . . and therefore , assurance will my , be thy and nothing my dukedom\n",
      "True: with things dying , i with things new - born . heres a sight for thee . look thee , a bearing - cloth for a <unknown> child ! look\n",
      "Prediction: , <unknown> that . and would <unknown> shallow <unknown> made simplicity but a cup to you , beatrice how , boy thart of colourd , thee <unknown> eye , desdemona\n",
      "True: of suffolk , well quickly <unknown> duke humphrey from his seat . cardinal . this weighty business will not brook delay ; ill to the duke of suffolk presently .\n",
      "Prediction: of yorks , earl <unknown> of the of of the brother , warwick . i gentleman cause , be be the , i go the king of york be continue\n",
      "True: of gold . charles . now let us on , my lords , and join our powers , and seek how we may <unknown> the foe . exeunt . scene\n",
      "Prediction: , the , i . i , us wait our my lord ; and let with <unknown> with as make to we may , our sea . exeunt . scene\n",
      "True: not thou <unknown> , corrupted , and exempt from ancient gentry ? his trespass yet lives guilty in thy blood ; and , till thou be restored , thou art\n",
      "Prediction: thou . , . thou for but <unknown> from thy <unknown> . o name is , not of his breast . for thou being he <unknown> <unknown> , thou art\n",
      "True: load , and turn him off , like to the empty ass , to shake his ears , and <unknown> in commons . octavius . you may do your will\n",
      "Prediction: <unknown> , and <unknown> to to , as a a <unknown> sea , with try his naked with and to his his . charles . o shall not that pleasures\n",
      "True: <unknown> of your shameful heads ; and bid that strumpet , your <unknown> dam , like to the earth , swallow her own increase . this is the feast that\n",
      "Prediction: <unknown> . <unknown> <unknown> <unknown> , and so them come , joind gentle blessings , joind bright a <unknown> , should up up tears . o <unknown> the man of\n",
      "True: a beast . you were also , jupiter , a swan , for the love of <unknown> . o <unknown> love ! how near the god drew to the complexion\n",
      "Prediction: , <unknown> . i have born <unknown> and , and goodly . and <unknown> hunt of a ! thersites , , , o art is world of up hide world\n",
      "True: return his sworn and mortal foe . matter of marriage was the charge he gave me , but dreadful war shall answer his demand . had he none else to\n",
      "Prediction: am to welcome freedom <unknown> stroke . talbot is youth , <unknown> <unknown> of <unknown> me , and i <unknown> of be as bitter . and i been else ,\n",
      "True: he may <unknown> his dotage with their powers , and hold our lives in mercy . oswald , i say ! albany . well , you may fear too far\n",
      "Prediction: and was not his mouth . a <unknown> . and with his <unknown> in vain of but . i will , regan . i , well shall be me .\n",
      "True: so , indeed ? he beats biondello . biondello . help , help , help ! heres a madman will murder me . exit . pedant . help , son\n",
      "Prediction: ? ? and , i hath me . pedant . ay , sir ! sir ! baptista a dish . not him . tranio grumio baptista . how , ho\n",
      "True: it is othellos pleasure , our noble and valiant general , that upon certain tidings now arrived , <unknown> the mere perdition of the <unknown> fleet , every man put\n",
      "Prediction: i is not disposition to sir purpose general our friends , to we our <unknown> seemed we , to , <unknown> judge of nations time , , the feeble that\n",
      "True: eyes , which heaven shall take in nature of a fee . ay , with these crystal beads heaven shall be <unknown> to do him justice , and revenge on\n",
      "Prediction: virginity , and , doth be from me , our part , but , marry a <unknown> tears of peep be sworn . be this good . and , <unknown>\n",
      "True: no <unknown> ; all men idle , all ; and women too , but innocent and pure ; no sovereignty , sebastian . yet he would be king ont .\n",
      "Prediction: and more but but <unknown> , , <unknown> women whores <unknown> , , like <unknown> , horrible , and <unknown> , for . i wise hath say blind , ,\n",
      "True: him , but , like lesser lights , did <unknown> their crowns to his <unknown> : where now his sons like a glow - worm in the night , the\n",
      "Prediction: , well and not by a , , <unknown> gently the <unknown> and the youth ; and is , <unknown> were a <unknown> - <unknown> <unknown> <unknown> semblance , and\n",
      "True: tis not my meaning to <unknown> one title of your honour out . to you , my lord , i come , what lord you will , from the most\n",
      "Prediction: and a an fault . be . . in my graces , of king shallow , sir lord , i am to and i hamlet do do my hence queen\n",
      "True: by my will . princess of france . why , will shall break it ; will , and nothing else . king . your ladyship is ignorant what it is\n",
      "Prediction: and my troth , i of france . gall , then you i away now and you as so else , king . we ignorance is fair ; we is\n",
      "True: try . i have a kind of self <unknown> with you ; but an unkind self , that itself will leave to be anothers fool . i would be gone\n",
      "Prediction: <unknown> it viola am been suit of fighting - charity strange , and i if lady , that you , seem you be so vassal , enter am not talbots\n",
      "True: ? what news from her ? enter valentine . valentine . so please my lord , i might not be admitted , but from her <unknown> do return this answer\n",
      "Prediction: , o is ? venice ? second malvolio and valentine . my am your lord , she am entreat know afraid , for that my will , i her glove\n",
      "True: of leonato . don john . a very forward march - <unknown> ! how came you to this ? borachio . being <unknown> for a <unknown> , as i was\n",
      "Prediction: , <unknown> . borachio pedro . i <unknown> <unknown> man on maker . borachio bravely it in this ? borachio . i learnd on the <unknown> man i i would\n",
      "True: and drowsy humour ; for no pulse shall keep his native progress , but <unknown> . no <unknown> , no breath shall <unknown> thou <unknown> , the roses in thy\n",
      "Prediction: <unknown> <unknown> <unknown> of and i man but blow his business pride warm nor i vengeance enter flight shall no <unknown> , <unknown> , yield , though sun of my\n",
      "True: . to be once in doubt is once to be resolvd : exchange me for a <unknown> when i shall turn the business of my soul to such <unknown> and\n",
      "Prediction: more romeo ferdinand revenged more love he to more be calld . i for with my company , i am be to crown to my speech to death a as\n",
      "True: you , sir ! tranio . and you , sir ! you are welcome . travel you far on , or are you at the farthest ? pedant . sir\n",
      "Prediction: me . and , exit . i you , sir , sir slaves more , i . in off . and else you ? your hearing ? you . i\n",
      "True: tender over his occasions , true , so <unknown> , so nurse - like ; let his virtue join with my request , which ill make bold your highness cannot\n",
      "Prediction: <unknown> of her substance . and and and <unknown> and so <unknown> , <unknown> , and him <unknown> be with him tongue , and is believe him to majesty ,\n",
      "True: you , allegiance . this sorrow that i have by right is yours ; and all the pleasures you usurp are mine . richard . the curse my noble father\n",
      "Prediction: the . and , clarence is , i have <unknown> my and yours , and this the worlds of have is caesars . warwick . and gates of fortune lord\n",
      "True: lost a noble and renowned brother , in his love toward her ever most kind and natural ; with him the <unknown> and <unknown> of her fortune , her marriage\n",
      "Prediction: is her hundred jewel princely son , and love revenue with him life <unknown> <unknown> of <unknown> vow till a i <unknown> of patient of his honour , and uncle\n",
      "True: is but needful : mercy is not itself that oft looks so ; pardon is still the nurse of second woe . but yet , poor claudio ! there is\n",
      "Prediction: is a a . and , as an in i <unknown> upon easy for me the the lesser , love sin . dorset , , for <unknown> , leonato is\n",
      "True: coldly . you say he dind at home , the goldsmith here denies that saying . sirrah , what say you ? dromio of ephesus . sir , he <unknown>\n",
      "Prediction: <unknown> as i have so was for night , and hose is . you rebellion  benedick , get says you ? romeo of ephesus . i , stop <unknown>\n",
      "True: <unknown> so <unknown> it : no love toward others in that bosom sits that on himself such <unknown> shame <unknown> . <unknown> for shame deny that thou <unknown> love to\n",
      "Prediction: <unknown> of high that not i more can thee , love <unknown> doth , they their doth large should should , therefore , the , not which diest , ,\n",
      "True: , hath laid that in a dozen passes between you and him , he shall not exceed you three hits . he hath laid on twelve for nine . and\n",
      "Prediction: , and made his handkerchief the <unknown> year here the and me . as and have find the . hundred you hamlet hath a a you hundred his hundred first\n",
      "True: <unknown> made . pluck off a little . i would not be a young count in your way for more than blushing comes to . if your back cannot vouchsafe\n",
      "Prediction: <unknown> . , emilia down the <unknown> fire hamlet have not yield mad cuckold man . rome beard . ever <unknown> you in . dot knocking you lordship were <unknown>\n",
      "True: store , harsh , <unknown> , and rude , <unknown> perish : look whom she best <unknown> , she gave thee more ; which bounteous gift thou shouldst in bounty\n",
      "Prediction: the of and and <unknown> , quick <unknown> , full , . <unknown> you i comes <unknown> , <unknown> comes me <unknown> <unknown> for , , , <unknown> have semblance\n",
      "True: was first . in war was never lion <unknown> more fierce , in peace was never gentle lamb more mild , than was that young and princely gentleman . his\n",
      "Prediction: and not <unknown> the faith , i <unknown> and up than than nor his and his <unknown> than than than than nor when <unknown> <unknown> claudio loving harry born richard\n",
      "True: up your tears , and stick your <unknown> on this fair corse , and , as the custom is , and in her best array bear her to church ;\n",
      "Prediction: , thy eyelids , and your your tears <unknown> your <unknown> lady . and <unknown> like you rest of , <unknown> the the <unknown> <unknown> , it babe seek .\n",
      "True: ? northumberland . first , to thy sacred state wish i all happiness . the next news is : i have to london sent the heads of salisbury , <unknown>\n",
      "Prediction: , king . ay , my see reverence name , i may the to hotspur king date is chancellor the have heard see , to earl of this , who\n",
      "True: o , <unknown> fortune ! to master <unknown> sword , made <unknown> town with <unknown> fires bright and britons <unknown> with courage . cloten . come , theres no more\n",
      "Prediction: , th i , ! o ha mortimer <unknown> , thou a of <unknown> <unknown> , , , <unknown> beggars , <unknown> , gloucester . what , come no <unknown>\n",
      "True: him have time to see his friends his foes , and merry fools to mock at him resort ; let him have time to mark how slow time goes in\n",
      "Prediction: me depart them . do the argument . <unknown> . and , <unknown> have see him athens . with and him be them to see him he he is ,\n",
      "True: return , and force their <unknown> courtesy . lear . my wits begin to turn . come on , my boy . how dost , my boy ? art cold\n",
      "Prediction: i , and bestow , <unknown> to to exeunt . i lord do to speak . lear , , then lord ; gloucester came thou mad masters ? gloucester thou\n",
      "True: is almost <unknown> , and her <unknown> in <unknown> golden characters express a general praise to her , and care in us at whose expense tis done . cleon .\n",
      "Prediction: , ended <unknown> to and the kinsman <unknown> arms . dew . . sound doom . kiss own and <unknown> no spite . liberty fields . <unknown> . enter .\n",
      "True: is : he hath the jewel of my life in hold , his youngest daughter , beautiful bianca , and her <unknown> from me and other more , suitors to\n",
      "Prediction: , to but is a <unknown> of the cause , me , and lieutenant sons , and , , and <unknown> brother ; her ; her . than i ,\n",
      "True: speak so loud . repair thy wit , good youth , or it will fall to <unknown> ruin . i stand here for law . duke . this letter from\n",
      "Prediction: <unknown> of , ? i to tongue to sweet youth , i ill is be into thee , of mowbray am for , thee , exeunt . i deed is\n",
      "True: best at a beast , my lord , that eer i saw . lysander . this lion is a very fox for his valour . theseus . true ; and\n",
      "Prediction: wisely . once good . and lord , and would i heard it lafew . well is doth thisbe very <unknown> , love good ; pandarus . well , and\n",
      "True: : those <unknown> who , not yet two summers younger , must have <unknown> to delight the taste , would now be glad of bread and beg for it :\n",
      "Prediction: , and lips are , as yet , hours <unknown> <unknown> hath <unknown> their . <unknown> on <unknown> of and have have as to it to lead . it .\n",
      "True: and cast up their caps o , me alone ! make you a sword of me ? if these shows be not outward , which of you but is four\n",
      "Prediction: , <unknown> the the heads and th and <unknown> . o a fast hole , all , i i <unknown> be off so , you should these shall contempt not\n",
      "True: who it is , for neer till now was i a child to fear i know not what . martius . lord bassianus lies <unknown> in blood , all on\n",
      "Prediction: , is is ? and i <unknown> i . it , <unknown> . thee it would not where . montague . i , , ! , france , and these\n",
      "True: , say you , with him ? servant . ay , the most <unknown> piece of earth , i think , that eer the sun <unknown> bright on . paulina\n",
      "Prediction: of nay that so and all i holofernes . ay , madam noble renowned advancd of arthur is that warrant , is you i image was forth and the o\n",
      "True: the belly answer ? menenius . i will tell you ; if youll bestow a small - of what you have little - patience awhile , <unknown> hear the <unknown>\n",
      "Prediction: drive <unknown> be , dromio . ay will not you , you he hear me fool <unknown> <unknown> them i have said grace but , , to , whether <unknown>\n",
      "True: thee , scarcely <unknown> , in the <unknown> ; where , for a monument upon thy bones , and eer - <unknown> <unknown> , the <unknown> <unknown> and <unknown> water\n",
      "Prediction: the up and tell with and <unknown> very of and thou being the <unknown> , the head , thou on i <unknown> hour thee i <unknown> of of <unknown> of\n",
      "True: . olivia . o , say so , and so be ! exeunt . scene ii . a room in olivias house . enter maria and clown . maria .\n",
      "Prediction: not i . i , i you , i hear , it malvolio viola act ii . the room in the house . enter olivia , maria . olivia .\n",
      "True: two of the <unknown> companions in the world . the <unknown> of these <unknown> heavens fall on their heads like dew ! for they are worthy to <unknown> heaven with\n",
      "Prediction: the <unknown> us clock . . the afternoon . mrs <unknown> of the <unknown> are bids , the throats , <unknown> , now now are all things ope , ,\n",
      "True: 5 . another part of the same . enter the <unknown> with the <unknown> of their <unknown> , in a <unknown> <unknown> , & c . <unknown> and odours bring away\n",
      "Prediction: ii ii the part of the island . scene agamemnon rest with the pregnant . the <unknown> . with a basket , . c . king . <unknown> , you\n",
      "True: exquisite form , their <unknown> great ; and i am something curious , being strange , to have them in safe <unknown> . may it please you to take them\n",
      "Prediction: <unknown> lady . and baser <unknown> <unknown> and , am sure <unknown> , as <unknown> , i make you <unknown> some <unknown> , alonso i be you , make me\n",
      "True: you shall find many , nay , almost any . prospero . aside . honest lord , thou hast said well ; for some of you there present are worse\n",
      "Prediction: of have not me a and , not spent grow i . o . i ariel , how art <unknown> anything . and i of them are are gainst you\n",
      "True: hid , <unknown> to <unknown> and girdle with embracing flames the waist of <unknown> fair love , lucrece the chaste . haply that name of chaste <unknown> set this <unknown>\n",
      "Prediction: , , and , the , <unknown> <unknown> <unknown> . : <unknown> <unknown> many , ladies : and cheeks <unknown> hand then she she of fancy hue tempted on precious\n",
      "True: <unknown> for use of the project gutenberg trademark . if you do not charge anything for <unknown> of this ebook , <unknown> with the trademark license is very easy .\n",
      "Prediction: me , the of <unknown> <unknown> gutenberg  <unknown> project you will fight <unknown> the in the to <unknown> work in <unknown> in project project <unknown> <unknown> <unknown> <unknown> to\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for _, data in enumerate(val_dl):\n",
    "    x, y = data[0].to(device), data[1].to(device)\n",
    "    output = transformer(x)\n",
    "\n",
    "    # Flatten batch and sequence dimension\n",
    "    loss = loss_fn(output.view(-1, output.shape[-1]), y.view(-1))\n",
    "    \n",
    "    pred = nn.functional.softmax(output, dim=-1)\n",
    "    pred = pred.argmax(dim=-1)\n",
    "    print(pred.shape, y.shape)\n",
    "    for i in range(pred.shape[0]):\n",
    "      print(\"True:\", \" \".join([index_to_word[word] for word in y[i, :].tolist()]))\n",
    "      print(\"Prediction:\", \" \".join([index_to_word[word] for word in pred[i, :].tolist()]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = config\n",
    "def generate_next_token(tokens=None):\n",
    "    if tokens == None:\n",
    "        tokens = [len(index_to_word)-1]\n",
    "    x = torch.LongTensor([tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        y = transformer(x)\n",
    "    # Don't allow the model to generate <unknown> tokens\n",
    "    y = y[:, :, :y.shape[2]-1]\n",
    "    pred = y.argmax(dim=-1).view(-1)\n",
    "    next_word = pred[len(tokens)-1].item()\n",
    "    return next_word\n",
    "\n",
    "def print_sentence(words):\n",
    "    print(\" \".join([index_to_word[word] for word in words]))\n",
    "\n",
    "generate_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if thou didst ever hold me in thy heart , absent cell , that i should win thee with thy loves , and make me say thou art a soldier\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(start=None):\n",
    "    if start == None:\n",
    "        sentence = []\n",
    "    else:\n",
    "        words = start.split(\" \")\n",
    "        sentence = [word_to_index[x] for x in words]\n",
    "    \n",
    "    while len(sentence) < config[\"max_input_length\"]:\n",
    "        next_word = generate_next_token(sentence)\n",
    "        sentence += [next_word]\n",
    "    \n",
    "    print_sentence(sentence)\n",
    "\n",
    "generate_sentence(\"if thou didst\")\n",
    "generate_sentence(\"you shall not pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6084])\n",
      "[9, 9, 9, 9, 48, 1121, 592, 9, 9, 13]\n",
      "[',', ',', ',', ',', 'to', 'our', 'again', ',', ',', 'for']\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "test = \"and bring him if . if thou issueless shalt hap\"\n",
    "test = [word_to_index[x] for x in test.split(\" \")]\n",
    "\n",
    "x = torch.LongTensor([test]).to(device)\n",
    "output = transformer(x)\n",
    "pred = F.softmax(output[:, -1, :].view(-1).detach().cpu(), dim=0)\n",
    "dist = torch.distributions.categorical.Categorical(probs=pred)\n",
    "\n",
    "print(pred.shape)\n",
    "pred = dist.sample([10]).tolist()\n",
    "print(pred)\n",
    "print([index_to_word[word] for word in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
