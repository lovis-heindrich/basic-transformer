{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from src.model import Transformer, TransformerConfig\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"models\"\n",
    "\n",
    "with open(model_folder+\"/config.json\", \"r\") as f:\n",
    "    settings = json.loads(f.read())\n",
    "\n",
    "with open(model_folder+\"/word_data.json\", \"r\") as f:\n",
    "    word_data = json.loads(f.read())\n",
    "\n",
    "word_to_index = {k:int(v) for k, v in word_data[\"word_to_index\"].items()}\n",
    "index_to_word = {int(k):v for k, v in word_data[\"index_to_word\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import seaborn as sns\n",
    "\n",
    "# initial_lr = 0.001\n",
    "# max_lr = 0.004\n",
    "# min_lr = 0.0001\n",
    "# optimizer = torch.optim.Adam([torch.Tensor([[1, 2]]), torch.Tensor([[1]])], lr=initial_lr)\n",
    "\n",
    "# total_epochs = 200\n",
    "# warmup_steps = 40\n",
    "# scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=initial_lr/max_lr, total_iters=warmup_steps)\n",
    "# scheduler2 = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = total_epochs-warmup_steps, eta_min = min_lr)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[warmup_steps])\n",
    "\n",
    "# lrs = []\n",
    "# for i in range(total_epochs):\n",
    "#     lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "#     lrs.append(lr)\n",
    "#     scheduler.step()\n",
    "\n",
    "# sns.lineplot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "config = TransformerConfig(vocab_size=settings[\"vocabulary_size\"], max_input_length=settings[\"max_input_length\"], num_heads=settings[\"num_heads\"], num_blocks=settings[\"num_blocks\"], embedding_size=settings[\"embedding_size\"])\n",
    "transformer = Transformer(config)\n",
    "transformer.load_state_dict(torch.load(model_folder+\"/model.pt\", map_location=torch.device(device)))\n",
    "transformer.to(device)\n",
    "transformer.train()\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(tokens=None, sample=False, temp=1, top_k=1):\n",
    "    if sample:\n",
    "        transformer.eval()\n",
    "    else: \n",
    "        transformer.train()\n",
    "    if tokens == None:\n",
    "        tokens = [len(index_to_word)-1]\n",
    "    x = torch.LongTensor([tokens]).to(device)\n",
    "    with torch.no_grad():\n",
    "        y = transformer(x)/temp\n",
    "        # Retrieve prediction of last token only\n",
    "        # Don't allow the model to generate <unknown> tokens\n",
    "        y = y[:, -1, :y.shape[2]-1].view(-1).detach().cpu()\n",
    "    #print(\"Probability of sampling a top 100 word:\", torch.sum(torch.topk(torch.nn.functional.softmax(y, dim=0), 100).values).item())\n",
    "    if not sample:\n",
    "        y = torch.nn.functional.softmax(y, dim=0)\n",
    "        pred = y.argmax(dim=-1).view(-1)\n",
    "    else:\n",
    "        #torch.topk(a.flatten(), 3).indices\n",
    "        if top_k < 1:\n",
    "            lowest_indices = torch.topk(y, int(len(y)*(1-top_k)), largest=False).indices\n",
    "            y[lowest_indices] = float(\"-inf\")\n",
    "        y = torch.nn.functional.softmax(y, dim=0)\n",
    "        #print(\"Probability of sampling a top 100 word:\", torch.sum(torch.topk(y, 100).values).item())\n",
    "        dist = torch.distributions.categorical.Categorical(probs=y)\n",
    "        pred = dist.sample([1])\n",
    "    next_word = pred.item()\n",
    "    return next_word\n",
    "\n",
    "def print_sentence(words):\n",
    "    print(\" \".join([index_to_word[word] for word in words]))\n",
    "\n",
    "def generate_sentence(start=None, sample=False, length=50, temp=1, top_k=0.8):\n",
    "    if start == None:\n",
    "        sentence = []\n",
    "    else:\n",
    "        words = start.split(\" \")\n",
    "        sentence = [word_to_index[x] for x in words]\n",
    "    \n",
    "    while len(sentence)<length:\n",
    "        if len(sentence) < settings[\"max_input_length\"]:\n",
    "            input_sentence = sentence\n",
    "        else:\n",
    "            input_sentence = sentence[-settings[\"max_input_length\"]:]\n",
    "        next_word = generate_next_token(input_sentence, sample, temp, top_k)\n",
    "        sentence += [next_word]\n",
    "    \n",
    "    print_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is man ? bassanio . i weep , sir , his life is a married dog , and god made thee merry report ! bassanio . he needs must\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    generate_sentence(\"what is\", length=30, sample=True, temp=0.05, top_k=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i long to live no more hard than have i my suit , and these bitter slaves are but till i thought she had any man made and duty to\n",
      "i warrant you , be satisfied . king henry . madam , i say tomorrow is it .â€” pardon me , sovereign please your highness defiance . queen margaret .\n",
      "i say , though best time , yet , ill write it for you . pistol . where is your fellow ? give me too much of man . bullcalf\n",
      "i am never able to shake the matter fort . lady macbeth . as i am , had thou hadst little fear between his master . enter lady macbeth .\n",
      "i die straight how to love you . richard . who can be gracious here ? anne . ay , madam . anne . nothing but give me fortune .\n",
      "i must acquaint you presently . exit . brabantio . look , here comes my kindred . enter roderigo and roderigo . welcome , iago . how now , roderigo\n",
      "i now , not use love ; see , hear me speak . i am noble , and his bastard daughter , young titus . now , father , for\n",
      "i wish you wise but same . be dog , and by the good gods , theres three women . you are valiant . gloucester . art thou quickly ,\n",
      "i be my comfort . when i wakd , i may play the place freely after these women tomorrow . now , o , pray , i will call you\n",
      "i and will undertake these letters to live these your katherine . baptista . for what ? biondello . dost thou know , sir ? biondello . ay , sir\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    generate_sentence(\"i\", length=30, sample=True, temp=0.05, top_k=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and i will be sworn to me . i am sorry i shall be glad i have done . i will not be my father . king . i have a woman . rosaline . i will not be satisfied . berowne . i will not be sworn to wed\n",
      "and all the rest . enter a messenger . messenger . my lord , the king , my lord of norfolk , i have sent for you , and i will not leave you to give me leave to be a man . king . i will not be patient\n",
      "and the king , and the rest , and the rest . king henry . i am sorry that we have power to have mercy for thee . king henry . then , farewell . exeunt . scene iii . london . the palace . enter king henry , warwick\n",
      "and the king . king henry . i am sorry that i have . king john . and i will not be satisfied . king henry . o , i will have thee to be sworn . exeunt . scene ii . a plain near the field . enter king\n",
      "and the king . king henry . i will , my lord of gloucester , to know the news . king richard . i will not stay tonight . exeunt king henry and derby . king henry . i will not come to me . i will not be patient\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    generate_sentence(\"and\", False, 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples\n",
    "\n",
    "- [if thou didst] not , thou art a fool . i am a fool , and thou art a fool . thou art a fool . clown . i am\n",
    "- [i] have a woman , and i have a good heart . but i will be a man of mine , and i will not be satisfied .\n",
    "- [i] am a man , and i am sure of it . i have said , and i will not be sworn to thee . i am a king ,\n",
    "- [you are] a merciful old wench . i am not able to wait upon my friend . i am glad to have merry a launcelet , if i had a\n",
    "- [you are] a beauteous blossom , and i will encounter thee with my nan . i am a proper friend . anne . i would not play a fool ,\n",
    "- i am not drunk for certain , nor i am not of any part of my sex . i am sorry for neither\n",
    "- [you are] in the field . enter a messenger . messenger . news , madam , news ! cleopatra . what news ? messenger . the news ? messenger . the king is gone . messenger . the lord mortimer is coming . antony . the noble antony is come\n",
    "- [like the sky] , and the wind and the moon . o , what a one that is in the other ?\n",
    "- [i] am a gentleman of love , and a man of all the world ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
