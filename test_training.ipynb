{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.model import Transformer, TransformerConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import trange\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data if not already downloaded\n",
    "path = \"./data\"\n",
    "filename = \"/shakespeare.txt\"\n",
    "if not os.path.isfile(path+filename):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    r = requests.get(\"https://www.gutenberg.org/files/100/100-0.txt\", stream = True)\n",
    "    with open(path+filename, \"w\") as f:\n",
    "        f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1987861 words, 5462587 characters\n",
      "[(' ', 702111), (', ', 67006), ('.\\n', 31293), ('the', 25606), ('\\n', 23889), ('I', 23693), ('. ', 20640), (',\\n', 20451), ('and', 20184), ('to', 17467)]\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "words = re.split(r\"\\b\", data)\n",
    "print(f\"Data size: {len(words)} words, {len(data)} characters\")\n",
    "word_counts = Counter(words)\n",
    "print(word_counts.most_common(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 726000 words\n",
      "Replaced 73711 words\n",
      "Data size: 1261861 words, 5462587 characters\n",
      "[(', ', 87465), ('. ', 69343), ('the', 30320), ('and', 28480), ('i', 24032), ('to', 20971), ('of', 18870), ('’', 16787), ('a', 16341), ('you', 14704), ('; ', 13455), ('my', 13185), ('in', 12464), ('that', 12258), ('is', 9923), ('not', 9085), ('with', 8545), ('s', 8464), ('for', 8302), ('me', 8283)]\n"
     ]
    }
   ],
   "source": [
    "# Reduce data amount by manually preprocessing some tokens\n",
    "\n",
    "# Remove space\n",
    "initial_length = len(words)\n",
    "to_remove = [\" \", \"\\n\"]\n",
    "words = [word.lower() for word in words if word not in to_remove]\n",
    "print(f\"Removed {initial_length - len(words)} words\")\n",
    "\n",
    "# Simplify punctuation\n",
    "to_replace = {\".\\n\\n\\n\": \". \", \".\\n\\n\": \". \", \".\\n\": \". \", \",\\n\\n\\n\": \", \", \",\\n\\n\": \", \", \",\\n\": \", \", \";\\n\\n\\n\": \"; \", \";\\n\\n\": \"; \", \";\\n\": \"; \"}\n",
    "replaced = 0\n",
    "for i, word in enumerate(words):\n",
    "    if word in to_replace.keys():\n",
    "        words[i] = to_replace[word]\n",
    "        replaced += 1\n",
    "print(f\"Replaced {replaced} words\")\n",
    "print(f\"Data size: {len(words)} words, {len(data)} characters\")\n",
    "\n",
    "word_counts = Counter(words)\n",
    "print(word_counts.most_common(n=20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9623 words that occur >= 5 times\n",
      "18600 words that occur < 5 times\n",
      "Example low occurence words: ['\\ufeff', 'restrictions', 'included', 'online', 'january', '1994', ' #', '100', ']\\n[', 'recently']\n"
     ]
    }
   ],
   "source": [
    "# Filter for words with <n occurrences\n",
    "n = 5\n",
    "high_occurence_words = []\n",
    "low_occurence_words = []\n",
    "for key, value in word_counts.items():\n",
    "    if value >= n:\n",
    "        high_occurence_words.append(key)\n",
    "    else:\n",
    "        low_occurence_words.append(key)\n",
    "print(f\"{len(high_occurence_words)} words that occur >= {n} times\")\n",
    "print(f\"{len(low_occurence_words)} words that occur < {n} times\")\n",
    "print(\"Example low occurence words:\", low_occurence_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example common word embedding: 21: and\n",
      "Example uncommon word embedding: 9623: <unknown>\n",
      "Vocabulary size: 9624\n"
     ]
    }
   ],
   "source": [
    "# Create embedding\n",
    "word_to_index = {word:i for i, word in enumerate(high_occurence_words)}\n",
    "index_to_word = {i:word for i, word in enumerate(high_occurence_words)}\n",
    "last_index = len(word_to_index)\n",
    "for word in low_occurence_words:\n",
    "    word_to_index[word] = last_index\n",
    "    index_to_word[last_index] = \"<unknown>\"\n",
    "print(f\"Example common word embedding: {word_to_index['and']}: {index_to_word[word_to_index['and']]}\")\n",
    "print(f\"Example uncommon word embedding: {word_to_index['restrictions']}: {index_to_word[word_to_index['restrictions']]}\")\n",
    "print(f\"Vocabulary size: {len(index_to_word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'our', '<unknown>', '<unknown>', 'to', 'hear', 'about', 'new', 'ebooks', '. ']\n",
      "['our', '<unknown>', '<unknown>', 'to', 'hear', 'about', 'new', 'ebooks', '. ', '<unknown>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "tokenized_data = [word_to_index[word] for word in words]\n",
    "labels = tokenized_data[1:] + [len(index_to_word) - 1]\n",
    "\n",
    "print([index_to_word[x] for x in tokenized_data[-10:]])\n",
    "print([index_to_word[x] for x in labels[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n",
      "Total tokens per batch 3840\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(index_to_word)\n",
    "total_steps = 100000\n",
    "max_input_length = 30\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "val_split = 0.2\n",
    "num_blocks = 4\n",
    "num_heads = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "# Paper used 25000 tokens per batch\n",
    "print(\"Total tokens per batch\", batch_size*max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30]) torch.Size([128, 30])\n",
      "['were', 'post', '—\\n', 'the', 'man', 'i', '’ ', 'th', '’ ', 'moon']\n",
      "['post', '—\\n', 'the', 'man', 'i', '’ ', 'th', '’ ', 'moon', '’']\n"
     ]
    }
   ],
   "source": [
    "# Create pytorch dataset\n",
    "cutoff = len(tokenized_data) % max_input_length\n",
    "data_x = tokenized_data[:-cutoff]\n",
    "data_y = labels[:-cutoff]\n",
    "data_x = torch.LongTensor(data_x).view(-1, max_input_length)\n",
    "data_y = torch.LongTensor(data_y).view(-1, max_input_length)\n",
    "ds = TensorDataset(data_x, data_y)\n",
    "val_size = int(len(ds)*val_split)\n",
    "train_ds, val_ds = torch.utils.data.random_split(ds, [len(ds)-val_size, val_size])\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Check if order is intact\n",
    "for _, data in enumerate(train_dl):\n",
    "    x, y = data[0], data[1]\n",
    "    print(x.shape, y.shape)\n",
    "    print([index_to_word[word] for word in x[5, :10].tolist()])\n",
    "    print([index_to_word[word] for word in y[5, :10].tolist()])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30]) torch.Size([128, 30]) torch.Size([128, 30, 9624])\n",
      "tensor(43.9588, grad_fn=<NllLossBackward>)\n",
      "torch.Size([128, 30, 9624])\n",
      "['giddy', 'in', 'spirit', ', ', 'still', 'gazing', 'in', 'a', 'doubt', 'whether']\n",
      "['in', 'spirit', ', ', 'still', 'gazing', 'in', 'a', 'doubt', 'whether', 'those']\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(TransformerConfig(vocab_size=vocabulary_size, max_input_length=max_input_length, num_heads=num_heads, num_blocks=num_blocks, embedding_size=embedding_size), apply_softmax=False)\n",
    "transformer.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "steps_per_epoch = len(train_dl)\n",
    "num_epochs = total_steps // steps_per_epoch\n",
    "\n",
    "def transformer_lr(step, d_model=embedding_size, warmup_steps=4000):\n",
    "    if step==0:\n",
    "        return transformer_lr(1, d_model, warmup_steps)\n",
    "    return (d_model ** -0.5)*min(step**-0.5, step*(warmup_steps**-0.5))\n",
    "\n",
    "initial_lr = transformer_lr(1)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "lr_per_epoch = lambda epoch: transformer_lr(epoch*steps_per_epoch) / initial_lr\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=[lr_per_epoch])\n",
    "\n",
    "for _, data in enumerate(train_dl):\n",
    "    x, y = data[0].to(device), data[1].to(device)\n",
    "    output = transformer(x)\n",
    "    print(x.shape, y.shape, output.shape)\n",
    "\n",
    "    # Flatten batch and sequence dimension\n",
    "    loss = loss_fn(output.view(-1, output.shape[-1]), y.view(-1))\n",
    "    print(loss)\n",
    "    \n",
    "    pred = nn.functional.softmax(output, dim=-1)\n",
    "    print(pred.shape)\n",
    "    print([index_to_word[word] for word in x[5, :10].tolist()])\n",
    "    print([index_to_word[word] for word in y[5, :10].tolist()])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2026752\n"
     ]
    }
   ],
   "source": [
    "params = 0\n",
    "for param in transformer.parameters():\n",
    "    params += param.nelement()\n",
    "print(\"Total parameters:\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(dl):\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for _, data in enumerate(dl):\n",
    "            x, y = data[0].to(device), data[1].to(device)\n",
    "            output = transformer(x)\n",
    "            \n",
    "            # Flatten batch and sequence dimension\n",
    "            loss = loss_fn(output.view(-1, output.shape[-1]), y.view(-1)).item()\n",
    "            total_loss += loss\n",
    "            pred = nn.functional.softmax(output, dim=-1)\n",
    "            pred = pred.argmax(dim=-1)\n",
    "            accuracy = (pred == y).float().mean().item()\n",
    "            total_acc += accuracy\n",
    "    return total_loss / len(dl), total_acc / len(dl)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=5):\n",
    "    stat_columns = [\"Epoch\", \"Step\", \"Lr\", \"TrainLoss\", \"TrainAcc\", \"ValLoss\", \"ValAcc\"]\n",
    "    stats = []\n",
    "    # Basic training loop\n",
    "    for e in trange(epochs):\n",
    "        transformer.train()\n",
    "        for i, data in enumerate(train_dl):\n",
    "            x, y = data[0].to(device), data[1].to(device)\n",
    "            output = transformer(x)\n",
    "            \n",
    "            # Flatten batch and sequence dimension\n",
    "            loss = loss_fn(output.view(-1, output.shape[-1]), y.view(-1))\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        train_loss, train_acc = eval(train_dl)\n",
    "        val_loss, val_acc = eval(val_dl)\n",
    "        lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "        step = e*steps_per_epoch\n",
    "        print(f\"\\nEpoch {e}, lr = {lr:.6f}\")\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Training loss {train_loss:.4f}, accuracy {train_acc:.4f}\")\n",
    "        print(f\"Eval loss {val_loss:.4f}, accuracy {val_acc:.4f}\")\n",
    "        stats.append([e, step, lr, train_loss, train_acc, val_loss, val_acc])\n",
    "    stat_df = pd.DataFrame(stats, columns=stat_columns)\n",
    "    return stat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train(num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"vocabulary_size\": vocabulary_size,\n",
    "    \"max_input_length\": max_input_length,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"embedding_size\": embedding_size,\n",
    "    \"word_to_index\": word_to_index,\n",
    "    \"index_to_word\": index_to_word,\n",
    "    \"num_blocks\": num_blocks,\n",
    "    \"num_heads\": num_heads\n",
    "}\n",
    "\n",
    "def save_model():\n",
    "    path = \"./models\"\n",
    "    model_filename = \"/transformer_model.pt\"\n",
    "    settings_filename = \"/transformer_settings.pkl\"\n",
    "    stats_filename = \"/transformer_stats.csv\"\n",
    "    if not os.path.isfile(path+model_filename):\n",
    "        print(\"Saving model\")\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        torch.save(transformer.state_dict(), path+model_filename)\n",
    "        with open(path + settings_filename, \"wb\") as f:\n",
    "            pickle.dump(params, f)\n",
    "        train_stats.to_csv()\n",
    "    else:\n",
    "        print(\"Model already exists, manually delete it to save current model instead.\")\n",
    "\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42.2960)\n",
      "tensor([5.5613e-20, 2.1305e-18, 6.7830e-15,  ..., 1.3166e-10, 4.7985e-13,\n",
      "        7.7562e-19])\n",
      "Example\n",
      "True: give him the ring ,  and bring him if thou canst unto antonio ’ s house .  away ,  make\n",
      "Prediction: beheaded directly cool pestilent pestilent pestilent directly pestilent pestilent pestilent pestilent pestilent pestilent pestilent pestilent pestilent pestilent pestilent pestilent stoop\n",
      "\n",
      "True: glowing :\n",
      " whereas reproof ,  obedient and in order ,  fits kings ,  as they are men ,  for they\n",
      "Prediction: times times times times times times times getting getting vexed times getting times times times vexed doors vexed bold doors\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for _, data in enumerate(train_dl):\n",
    "    x, y = data[0].to(device), data[1].to(device)\n",
    "    output = transformer(x)\n",
    "\n",
    "    # Flatten batch and sequence dimension\n",
    "    loss = loss_fn(output.view(-1, output.shape[-1]), y.view(-1))\n",
    "    print(loss)\n",
    "    \n",
    "    pred = nn.functional.softmax(output, dim=-1)\n",
    "    print(pred[0, 0, :])\n",
    "    pred = pred.argmax(dim=-1)\n",
    "    print(\"Example\")\n",
    "    print(\"True:\", \" \".join([index_to_word[word] for word in y[1, :20].tolist()]))\n",
    "    print(\"Prediction:\", \" \".join([index_to_word[word] for word in pred[1, :20].tolist()]))\n",
    "    print(\"\")\n",
    "    print(\"True:\", \" \".join([index_to_word[word] for word in y[10, :20].tolist()]))\n",
    "    print(\"Prediction:\", \" \".join([index_to_word[word] for word in pred[10, :20].tolist()]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
