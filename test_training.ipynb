{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlovisheindrich\u001b[0m (\u001b[33mlovis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from src.model import Transformer, TransformerConfig\n",
    "from src.load_data import load_data, download_data, create_word_dicts, create_dataset\n",
    "from src.train import train, eval\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "from tqdm.notebook import trange\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import wandb\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "wandb.login()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 1195401 words\n",
      "6083 words that occur >= 10 times\n",
      "22188 words that occur < 10 times\n",
      "Vocabulary size: 6084\n"
     ]
    }
   ],
   "source": [
    "download_data()\n",
    "words = load_data()\n",
    "word_to_index, index_to_word = create_word_dicts(words, min_occurrences=10)\n",
    "\n",
    "data = {\n",
    "    \"words\": words,\n",
    "    \"word_to_index\": word_to_index,\n",
    "    \"index_to_word\": index_to_word\n",
    "}\n",
    "\n",
    "# Initialize run data directory\n",
    "run_data_path = \"./run_data\"\n",
    "if os.path.exists(run_data_path):\n",
    "    shutil.rmtree(run_data_path)\n",
    "os.mkdir(run_data_path)\n",
    "os.mkdir(run_data_path+\"/checkpoints\")\n",
    "\n",
    "with open(run_data_path+\"/word_data.json\", 'w') as outfile:\n",
    "    outfile.write(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\heind\\Documents\\workspace\\basic-transformer\\wandb\\run-20230420_102236-qh607w2f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lovis/basic-transformer/runs/qh607w2f' target=\"_blank\">devoted-eon-7</a></strong> to <a href='https://wandb.ai/lovis/basic-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lovis/basic-transformer' target=\"_blank\">https://wandb.ai/lovis/basic-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lovis/basic-transformer/runs/qh607w2f' target=\"_blank\">https://wandb.ai/lovis/basic-transformer/runs/qh607w2f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens per batch 1920\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"vocabulary_size\": len(index_to_word),\n",
    "    \"max_input_length\": 30,\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_size\": 256,\n",
    "    \"num_blocks\": 4,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_epochs\": 300,\n",
    "    \"val_split\": 0.2,\n",
    "    \"warmup_steps\": 4000,\n",
    "    \"lr_scale\": 5\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=\"basic-transformer\",\n",
    "    config=config,\n",
    "    settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "word_artifact = wandb.Artifact('word_dicts', 'dataset')\n",
    "word_artifact.add_file(local_path=run_data_path+\"/word_data.json\")\n",
    "wandb.log_artifact(word_artifact)\n",
    "\n",
    "model_artifact = wandb.Artifact('models', 'model')\n",
    "\n",
    "\n",
    "# Paper used 25000 tokens per batch\n",
    "print(\"Total tokens per batch\", config[\"batch_size\"]*config[\"max_input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl = create_dataset(words, word_to_index, index_to_word, batch_size=config[\"batch_size\"], val_split=config[\"val_split\"], max_input_length=config[\"max_input_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4720128\n",
      "Total training steps: 149700\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(TransformerConfig(vocab_size=config[\"vocabulary_size\"], max_input_length=config[\"max_input_length\"], num_heads=config[\"num_heads\"], num_blocks=config[\"num_blocks\"], embedding_size=config[\"embedding_size\"]), apply_softmax=False)\n",
    "transformer.to(device)\n",
    "wandb.watch(transformer, log_freq=1000)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "steps_per_epoch = len(train_dl)\n",
    "\n",
    "def transformer_lr(step, d_model=config[\"embedding_size\"], warmup_steps=config[\"warmup_steps\"], lr_scale=config[\"lr_scale\"]):\n",
    "    if step==0:\n",
    "        return transformer_lr(1, d_model, warmup_steps)\n",
    "    return lr_scale*((d_model) ** -0.5)*min(step**-0.5, step*(warmup_steps**-1.5))\n",
    "\n",
    "initial_lr = transformer_lr(1)\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "lr_per_epoch = lambda epoch: transformer_lr(epoch*steps_per_epoch) / initial_lr\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_per_epoch)\n",
    "\n",
    "transformer_params = 0\n",
    "for param in transformer.parameters():\n",
    "    transformer_params += param.nelement()\n",
    "print(\"Total parameters:\", transformer_params)\n",
    "print(\"Total training steps:\", steps_per_epoch*config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_schedule():\n",
    "    optim = torch.optim.Adam(transformer.parameters(), lr=initial_lr, betas=(0.9, 0.98), eps=1e-09)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=lr_per_epoch)\n",
    "    lrs = []\n",
    "    for i in range(300):\n",
    "        lrs.append(lr_scheduler.optimizer.param_groups[0]['lr'])\n",
    "        lr_scheduler.step()\n",
    "    import seaborn as sns\n",
    "    print(initial_lr, max(lrs))\n",
    "    sns.lineplot(lrs)\n",
    "\n",
    "#plot_lr_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(num_epochs, start_epoch=0, model_checkpoint_freq=100):\n",
    "    for e in trange(num_epochs):\n",
    "\n",
    "        train(transformer, loss_fn=loss_fn, optim=optim, device=device, dl=train_dl)\n",
    "\n",
    "        train_loss, train_acc = eval(transformer=transformer, loss_fn=loss_fn, device=device, dl=train_dl)\n",
    "        val_loss, val_acc = eval(transformer=transformer, loss_fn=loss_fn, device=device, dl=val_dl)\n",
    "\n",
    "        lr = lr_scheduler.optimizer.param_groups[0]['lr']\n",
    "        step = (e+start_epoch)*steps_per_epoch\n",
    "        print(f\"\\nEpoch {e+start_epoch}, lr = {lr:.6f}\")\n",
    "        lr_scheduler.step()\n",
    "        print(f\"Training loss {train_loss:.4f}, accuracy {train_acc:.4f}\")\n",
    "        print(f\"Eval loss {val_loss:.4f}, accuracy {val_acc:.4f}\")\n",
    "        if (e+1)%model_checkpoint_freq == 0:\n",
    "            checkpoint_name = f\"/checkpoints/checkpoint_{e+1}.pt\"\n",
    "            torch.save(transformer.state_dict(), run_data_path + checkpoint_name)\n",
    "            model_artifact.add_file(run_data_path + checkpoint_name, name =checkpoint_name)\n",
    "        wandb.log({\"train_acc\": train_acc, \"train_loss\": train_loss, \"val_acc\": val_acc, \"val_loss\": val_loss, \"learning_rate\": lr, \"step\": step})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4dce5e179444efb37ca9a804cd5050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, lr = 0.000001\n",
      "Training loss 32.7505, accuracy 0.0100\n",
      "Eval loss 32.8000, accuracy 0.0099\n",
      "\n",
      "Epoch 1, lr = 0.000616\n",
      "Training loss 7.0642, accuracy 0.0778\n",
      "Eval loss 7.0852, accuracy 0.0783\n",
      "\n",
      "Epoch 2, lr = 0.001233\n",
      "Training loss 5.8961, accuracy 0.0835\n",
      "Eval loss 5.9437, accuracy 0.0811\n",
      "\n",
      "Epoch 3, lr = 0.001849\n",
      "Training loss 5.1550, accuracy 0.1728\n",
      "Eval loss 5.2229, accuracy 0.1691\n",
      "\n",
      "Epoch 4, lr = 0.002466\n",
      "Training loss 4.9541, accuracy 0.1790\n",
      "Eval loss 5.0377, accuracy 0.1743\n",
      "\n",
      "Epoch 5, lr = 0.003082\n",
      "Training loss 4.8494, accuracy 0.1804\n",
      "Eval loss 4.9352, accuracy 0.1753\n",
      "\n",
      "Epoch 6, lr = 0.003698\n",
      "Training loss 4.7860, accuracy 0.1880\n",
      "Eval loss 4.8760, accuracy 0.1836\n",
      "\n",
      "Epoch 7, lr = 0.004315\n",
      "Training loss 4.7628, accuracy 0.1873\n",
      "Eval loss 4.8662, accuracy 0.1824\n",
      "\n",
      "Epoch 8, lr = 0.004931\n",
      "Training loss 4.7258, accuracy 0.1882\n",
      "Eval loss 4.8356, accuracy 0.1833\n",
      "\n",
      "Epoch 9, lr = 0.004663\n",
      "Training loss 4.6630, accuracy 0.1950\n",
      "Eval loss 4.7848, accuracy 0.1891\n",
      "\n",
      "Epoch 10, lr = 0.004424\n",
      "Training loss 4.6081, accuracy 0.1997\n",
      "Eval loss 4.7488, accuracy 0.1931\n",
      "\n",
      "Epoch 11, lr = 0.004218\n",
      "Training loss 4.5604, accuracy 0.2024\n",
      "Eval loss 4.7145, accuracy 0.1942\n",
      "\n",
      "Epoch 12, lr = 0.004038\n",
      "Training loss 4.5036, accuracy 0.2063\n",
      "Eval loss 4.6800, accuracy 0.1972\n",
      "\n",
      "Epoch 13, lr = 0.003880\n",
      "Training loss 4.4638, accuracy 0.2078\n",
      "Eval loss 4.6575, accuracy 0.1978\n",
      "\n",
      "Epoch 14, lr = 0.003739\n",
      "Training loss 4.4284, accuracy 0.2134\n",
      "Eval loss 4.6333, accuracy 0.2013\n",
      "\n",
      "Epoch 15, lr = 0.003612\n",
      "Training loss 4.3840, accuracy 0.2167\n",
      "Eval loss 4.6123, accuracy 0.2043\n",
      "\n",
      "Epoch 16, lr = 0.003497\n",
      "Training loss 4.3545, accuracy 0.2186\n",
      "Eval loss 4.5951, accuracy 0.2051\n",
      "\n",
      "Epoch 17, lr = 0.003393\n",
      "Training loss 4.3188, accuracy 0.2203\n",
      "Eval loss 4.5875, accuracy 0.2054\n",
      "\n",
      "Epoch 18, lr = 0.003297\n",
      "Training loss 4.2853, accuracy 0.2239\n",
      "Eval loss 4.5755, accuracy 0.2069\n",
      "\n",
      "Epoch 19, lr = 0.003209\n",
      "Training loss 4.2580, accuracy 0.2252\n",
      "Eval loss 4.5676, accuracy 0.2075\n",
      "\n",
      "Epoch 20, lr = 0.003128\n",
      "Training loss 4.2351, accuracy 0.2277\n",
      "Eval loss 4.5616, accuracy 0.2098\n",
      "\n",
      "Epoch 21, lr = 0.003053\n",
      "Training loss 4.2121, accuracy 0.2285\n",
      "Eval loss 4.5539, accuracy 0.2079\n",
      "\n",
      "Epoch 22, lr = 0.002983\n",
      "Training loss 4.1781, accuracy 0.2314\n",
      "Eval loss 4.5383, accuracy 0.2102\n",
      "\n",
      "Epoch 23, lr = 0.002917\n",
      "Training loss 4.1550, accuracy 0.2324\n",
      "Eval loss 4.5364, accuracy 0.2107\n",
      "\n",
      "Epoch 24, lr = 0.002856\n",
      "Training loss 4.1364, accuracy 0.2340\n",
      "Eval loss 4.5382, accuracy 0.2107\n",
      "\n",
      "Epoch 25, lr = 0.002798\n",
      "Training loss 4.1130, accuracy 0.2359\n",
      "Eval loss 4.5328, accuracy 0.2118\n",
      "\n",
      "Epoch 26, lr = 0.002744\n",
      "Training loss 4.0954, accuracy 0.2368\n",
      "Eval loss 4.5351, accuracy 0.2106\n",
      "\n",
      "Epoch 27, lr = 0.002692\n",
      "Training loss 4.0710, accuracy 0.2389\n",
      "Eval loss 4.5386, accuracy 0.2126\n",
      "\n",
      "Epoch 28, lr = 0.002644\n",
      "Training loss 4.0489, accuracy 0.2405\n",
      "Eval loss 4.5365, accuracy 0.2124\n",
      "\n",
      "Epoch 29, lr = 0.002598\n",
      "Training loss 4.0232, accuracy 0.2432\n",
      "Eval loss 4.5431, accuracy 0.2141\n",
      "\n",
      "Epoch 30, lr = 0.002554\n",
      "Training loss 4.0135, accuracy 0.2435\n",
      "Eval loss 4.5314, accuracy 0.2122\n",
      "\n",
      "Epoch 31, lr = 0.002513\n",
      "Training loss 3.9913, accuracy 0.2445\n",
      "Eval loss 4.5363, accuracy 0.2124\n",
      "\n",
      "Epoch 32, lr = 0.002473\n",
      "Training loss 3.9686, accuracy 0.2466\n",
      "Eval loss 4.5514, accuracy 0.2113\n",
      "\n",
      "Epoch 33, lr = 0.002435\n",
      "Training loss 3.9498, accuracy 0.2474\n",
      "Eval loss 4.5358, accuracy 0.2135\n",
      "\n",
      "Epoch 34, lr = 0.002399\n",
      "Training loss 3.9348, accuracy 0.2496\n",
      "Eval loss 4.5452, accuracy 0.2121\n",
      "\n",
      "Epoch 35, lr = 0.002365\n",
      "Training loss 3.9139, accuracy 0.2517\n",
      "Eval loss 4.5420, accuracy 0.2136\n",
      "\n",
      "Epoch 36, lr = 0.002332\n",
      "Training loss 3.8964, accuracy 0.2518\n",
      "Eval loss 4.5497, accuracy 0.2140\n",
      "\n",
      "Epoch 37, lr = 0.002300\n",
      "Training loss 3.8728, accuracy 0.2541\n",
      "Eval loss 4.5492, accuracy 0.2129\n",
      "\n",
      "Epoch 38, lr = 0.002269\n",
      "Training loss 3.8619, accuracy 0.2557\n",
      "Eval loss 4.5474, accuracy 0.2129\n",
      "\n",
      "Epoch 39, lr = 0.002240\n",
      "Training loss 3.8420, accuracy 0.2566\n",
      "Eval loss 4.5508, accuracy 0.2137\n",
      "\n",
      "Epoch 40, lr = 0.002212\n",
      "Training loss 3.8322, accuracy 0.2575\n",
      "Eval loss 4.5520, accuracy 0.2141\n",
      "\n",
      "Epoch 41, lr = 0.002185\n",
      "Training loss 3.8150, accuracy 0.2589\n",
      "Eval loss 4.5693, accuracy 0.2138\n",
      "\n",
      "Epoch 42, lr = 0.002159\n",
      "Training loss 3.8030, accuracy 0.2594\n",
      "Eval loss 4.5721, accuracy 0.2139\n",
      "\n",
      "Epoch 43, lr = 0.002133\n",
      "Training loss 3.7903, accuracy 0.2599\n",
      "Eval loss 4.5770, accuracy 0.2138\n",
      "\n",
      "Epoch 44, lr = 0.002109\n",
      "Training loss 3.7686, accuracy 0.2621\n",
      "Eval loss 4.5709, accuracy 0.2139\n",
      "\n",
      "Epoch 45, lr = 0.002085\n",
      "Training loss 3.7488, accuracy 0.2631\n",
      "Eval loss 4.5954, accuracy 0.2119\n",
      "\n",
      "Epoch 46, lr = 0.002063\n",
      "Training loss 3.7366, accuracy 0.2648\n",
      "Eval loss 4.5821, accuracy 0.2145\n",
      "\n",
      "Epoch 47, lr = 0.002041\n",
      "Training loss 3.7250, accuracy 0.2659\n",
      "Eval loss 4.5943, accuracy 0.2112\n",
      "\n",
      "Epoch 48, lr = 0.002019\n",
      "Training loss 3.7119, accuracy 0.2665\n",
      "Eval loss 4.5995, accuracy 0.2128\n",
      "\n",
      "Epoch 49, lr = 0.001998\n",
      "Training loss 3.7030, accuracy 0.2679\n",
      "Eval loss 4.5924, accuracy 0.2119\n",
      "\n",
      "Epoch 50, lr = 0.001978\n",
      "Training loss 3.6800, accuracy 0.2696\n",
      "Eval loss 4.6038, accuracy 0.2132\n",
      "\n",
      "Epoch 51, lr = 0.001959\n",
      "Training loss 3.6678, accuracy 0.2702\n",
      "Eval loss 4.6193, accuracy 0.2142\n",
      "\n",
      "Epoch 52, lr = 0.001940\n",
      "Training loss 3.6659, accuracy 0.2700\n",
      "Eval loss 4.6066, accuracy 0.2143\n",
      "\n",
      "Epoch 53, lr = 0.001922\n",
      "Training loss 3.6403, accuracy 0.2725\n",
      "Eval loss 4.6304, accuracy 0.2125\n",
      "\n",
      "Epoch 54, lr = 0.001904\n",
      "Training loss 3.6360, accuracy 0.2729\n",
      "Eval loss 4.6304, accuracy 0.2128\n",
      "\n",
      "Epoch 55, lr = 0.001886\n",
      "Training loss 3.6218, accuracy 0.2740\n",
      "Eval loss 4.6271, accuracy 0.2118\n",
      "\n",
      "Epoch 56, lr = 0.001869\n",
      "Training loss 3.6127, accuracy 0.2744\n",
      "Eval loss 4.6217, accuracy 0.2130\n",
      "\n",
      "Epoch 57, lr = 0.001853\n",
      "Training loss 3.6044, accuracy 0.2754\n",
      "Eval loss 4.6200, accuracy 0.2137\n",
      "\n",
      "Epoch 58, lr = 0.001837\n",
      "Training loss 3.5797, accuracy 0.2769\n",
      "Eval loss 4.6527, accuracy 0.2139\n",
      "\n",
      "Epoch 59, lr = 0.001821\n",
      "Training loss 3.5707, accuracy 0.2786\n",
      "Eval loss 4.6614, accuracy 0.2112\n",
      "\n",
      "Epoch 60, lr = 0.001806\n",
      "Training loss 3.5625, accuracy 0.2804\n",
      "Eval loss 4.6697, accuracy 0.2097\n",
      "\n",
      "Epoch 61, lr = 0.001791\n",
      "Training loss 3.5538, accuracy 0.2805\n",
      "Eval loss 4.6477, accuracy 0.2119\n",
      "\n",
      "Epoch 62, lr = 0.001777\n",
      "Training loss 3.5343, accuracy 0.2822\n",
      "Eval loss 4.6622, accuracy 0.2122\n",
      "\n",
      "Epoch 63, lr = 0.001763\n",
      "Training loss 3.5215, accuracy 0.2835\n",
      "Eval loss 4.6779, accuracy 0.2119\n",
      "\n",
      "Epoch 64, lr = 0.001749\n",
      "Training loss 3.5099, accuracy 0.2847\n",
      "Eval loss 4.6798, accuracy 0.2113\n",
      "\n",
      "Epoch 65, lr = 0.001735\n",
      "Training loss 3.5040, accuracy 0.2848\n",
      "Eval loss 4.6722, accuracy 0.2117\n",
      "\n",
      "Epoch 66, lr = 0.001722\n",
      "Training loss 3.4970, accuracy 0.2864\n",
      "Eval loss 4.6797, accuracy 0.2097\n",
      "\n",
      "Epoch 67, lr = 0.001709\n",
      "Training loss 3.4921, accuracy 0.2870\n",
      "Eval loss 4.6863, accuracy 0.2095\n",
      "\n",
      "Epoch 68, lr = 0.001696\n",
      "Training loss 3.4736, accuracy 0.2867\n",
      "Eval loss 4.7079, accuracy 0.2138\n",
      "\n",
      "Epoch 69, lr = 0.001684\n",
      "Training loss 3.4581, accuracy 0.2908\n",
      "Eval loss 4.7051, accuracy 0.2101\n",
      "\n",
      "Epoch 70, lr = 0.001672\n",
      "Training loss 3.4550, accuracy 0.2895\n",
      "Eval loss 4.6894, accuracy 0.2122\n",
      "\n",
      "Epoch 71, lr = 0.001660\n",
      "Training loss 3.4410, accuracy 0.2913\n",
      "Eval loss 4.7129, accuracy 0.2103\n",
      "\n",
      "Epoch 72, lr = 0.001649\n",
      "Training loss 3.4252, accuracy 0.2928\n",
      "Eval loss 4.7337, accuracy 0.2095\n",
      "\n",
      "Epoch 73, lr = 0.001637\n",
      "Training loss 3.4201, accuracy 0.2935\n",
      "Eval loss 4.7265, accuracy 0.2112\n",
      "\n",
      "Epoch 74, lr = 0.001626\n",
      "Training loss 3.4062, accuracy 0.2949\n",
      "Eval loss 4.7367, accuracy 0.2096\n",
      "\n",
      "Epoch 75, lr = 0.001615\n",
      "Training loss 3.4080, accuracy 0.2941\n",
      "Eval loss 4.7259, accuracy 0.2121\n",
      "\n",
      "Epoch 76, lr = 0.001605\n",
      "Training loss 3.3980, accuracy 0.2957\n",
      "Eval loss 4.7465, accuracy 0.2100\n",
      "\n",
      "Epoch 77, lr = 0.001594\n",
      "Training loss 3.3882, accuracy 0.2964\n",
      "Eval loss 4.7352, accuracy 0.2108\n",
      "\n",
      "Epoch 78, lr = 0.001584\n",
      "Training loss 3.3793, accuracy 0.2969\n",
      "Eval loss 4.7213, accuracy 0.2132\n",
      "\n",
      "Epoch 79, lr = 0.001574\n",
      "Training loss 3.3724, accuracy 0.2967\n",
      "Eval loss 4.7502, accuracy 0.2126\n",
      "\n",
      "Epoch 80, lr = 0.001564\n",
      "Training loss 3.3622, accuracy 0.3007\n",
      "Eval loss 4.7579, accuracy 0.2076\n",
      "\n",
      "Epoch 81, lr = 0.001554\n",
      "Training loss 3.3585, accuracy 0.3005\n",
      "Eval loss 4.7319, accuracy 0.2110\n",
      "\n",
      "Epoch 82, lr = 0.001545\n",
      "Training loss 3.3309, accuracy 0.3025\n",
      "Eval loss 4.7664, accuracy 0.2118\n",
      "\n",
      "Epoch 83, lr = 0.001536\n",
      "Training loss 3.3405, accuracy 0.3009\n",
      "Eval loss 4.7725, accuracy 0.2111\n",
      "\n",
      "Epoch 84, lr = 0.001526\n",
      "Training loss 3.3325, accuracy 0.3045\n",
      "Eval loss 4.7513, accuracy 0.2097\n",
      "\n",
      "Epoch 85, lr = 0.001517\n",
      "Training loss 3.3170, accuracy 0.3047\n",
      "Eval loss 4.7642, accuracy 0.2102\n",
      "\n",
      "Epoch 86, lr = 0.001509\n",
      "Training loss 3.3109, accuracy 0.3041\n",
      "Eval loss 4.7777, accuracy 0.2111\n",
      "\n",
      "Epoch 87, lr = 0.001500\n",
      "Training loss 3.2974, accuracy 0.3088\n",
      "Eval loss 4.7854, accuracy 0.2092\n",
      "\n",
      "Epoch 88, lr = 0.001491\n",
      "Training loss 3.2933, accuracy 0.3066\n",
      "Eval loss 4.7919, accuracy 0.2108\n",
      "\n",
      "Epoch 89, lr = 0.001483\n",
      "Training loss 3.2837, accuracy 0.3089\n",
      "Eval loss 4.8011, accuracy 0.2074\n",
      "\n",
      "Epoch 90, lr = 0.001475\n",
      "Training loss 3.2810, accuracy 0.3101\n",
      "Eval loss 4.7830, accuracy 0.2091\n",
      "\n",
      "Epoch 91, lr = 0.001466\n",
      "Training loss 3.2760, accuracy 0.3120\n",
      "Eval loss 4.7968, accuracy 0.2075\n",
      "\n",
      "Epoch 92, lr = 0.001458\n",
      "Training loss 3.2640, accuracy 0.3107\n",
      "Eval loss 4.8029, accuracy 0.2104\n",
      "\n",
      "Epoch 93, lr = 0.001451\n",
      "Training loss 3.2657, accuracy 0.3118\n",
      "Eval loss 4.7885, accuracy 0.2075\n",
      "\n",
      "Epoch 94, lr = 0.001443\n",
      "Training loss 3.2503, accuracy 0.3117\n",
      "Eval loss 4.8140, accuracy 0.2108\n",
      "\n",
      "Epoch 95, lr = 0.001435\n",
      "Training loss 3.2476, accuracy 0.3122\n",
      "Eval loss 4.8152, accuracy 0.2091\n",
      "\n",
      "Epoch 96, lr = 0.001428\n",
      "Training loss 3.2478, accuracy 0.3154\n",
      "Eval loss 4.7874, accuracy 0.2083\n",
      "\n",
      "Epoch 97, lr = 0.001420\n",
      "Training loss 3.2358, accuracy 0.3168\n",
      "Eval loss 4.8144, accuracy 0.2074\n",
      "\n",
      "Epoch 98, lr = 0.001413\n",
      "Training loss 3.2256, accuracy 0.3149\n",
      "Eval loss 4.8299, accuracy 0.2097\n",
      "\n",
      "Epoch 99, lr = 0.001406\n",
      "Training loss 3.2200, accuracy 0.3186\n",
      "Eval loss 4.8003, accuracy 0.2082\n",
      "\n",
      "Epoch 100, lr = 0.001399\n",
      "Training loss 3.2226, accuracy 0.3198\n",
      "Eval loss 4.8271, accuracy 0.2055\n",
      "\n",
      "Epoch 101, lr = 0.001392\n",
      "Training loss 3.2037, accuracy 0.3174\n",
      "Eval loss 4.8419, accuracy 0.2105\n",
      "\n",
      "Epoch 102, lr = 0.001385\n",
      "Training loss 3.1962, accuracy 0.3218\n",
      "Eval loss 4.8494, accuracy 0.2064\n",
      "\n",
      "Epoch 103, lr = 0.001378\n",
      "Training loss 3.1952, accuracy 0.3201\n",
      "Eval loss 4.8199, accuracy 0.2089\n",
      "\n",
      "Epoch 104, lr = 0.001372\n",
      "Training loss 3.1871, accuracy 0.3194\n",
      "Eval loss 4.8477, accuracy 0.2098\n",
      "\n",
      "Epoch 105, lr = 0.001365\n",
      "Training loss 3.1785, accuracy 0.3227\n",
      "Eval loss 4.8452, accuracy 0.2092\n",
      "\n",
      "Epoch 106, lr = 0.001359\n",
      "Training loss 3.1663, accuracy 0.3236\n",
      "Eval loss 4.8713, accuracy 0.2090\n",
      "\n",
      "Epoch 107, lr = 0.001352\n",
      "Training loss 3.1726, accuracy 0.3228\n",
      "Eval loss 4.8419, accuracy 0.2098\n",
      "\n",
      "Epoch 108, lr = 0.001346\n",
      "Training loss 3.1639, accuracy 0.3226\n",
      "Eval loss 4.8479, accuracy 0.2094\n",
      "\n",
      "Epoch 109, lr = 0.001340\n",
      "Training loss 3.1564, accuracy 0.3236\n",
      "Eval loss 4.8624, accuracy 0.2092\n",
      "\n",
      "Epoch 110, lr = 0.001334\n",
      "Training loss 3.1514, accuracy 0.3267\n",
      "Eval loss 4.8607, accuracy 0.2073\n",
      "\n",
      "Epoch 111, lr = 0.001328\n",
      "Training loss 3.1519, accuracy 0.3258\n",
      "Eval loss 4.8487, accuracy 0.2090\n",
      "\n",
      "Epoch 112, lr = 0.001322\n",
      "Training loss 3.1369, accuracy 0.3271\n",
      "Eval loss 4.8582, accuracy 0.2085\n",
      "\n",
      "Epoch 113, lr = 0.001316\n",
      "Training loss 3.1358, accuracy 0.3288\n",
      "Eval loss 4.8636, accuracy 0.2069\n",
      "\n",
      "Epoch 114, lr = 0.001310\n",
      "Training loss 3.1283, accuracy 0.3291\n",
      "Eval loss 4.8818, accuracy 0.2060\n",
      "\n",
      "Epoch 115, lr = 0.001305\n",
      "Training loss 3.1318, accuracy 0.3284\n",
      "Eval loss 4.8506, accuracy 0.2068\n",
      "\n",
      "Epoch 116, lr = 0.001299\n",
      "Training loss 3.1230, accuracy 0.3289\n",
      "Eval loss 4.8732, accuracy 0.2088\n",
      "\n",
      "Epoch 117, lr = 0.001293\n",
      "Training loss 3.1366, accuracy 0.3296\n",
      "Eval loss 4.8345, accuracy 0.2085\n",
      "\n",
      "Epoch 118, lr = 0.001288\n",
      "Training loss 3.1067, accuracy 0.3325\n",
      "Eval loss 4.8809, accuracy 0.2069\n",
      "\n",
      "Epoch 119, lr = 0.001282\n",
      "Training loss 3.1060, accuracy 0.3321\n",
      "Eval loss 4.8763, accuracy 0.2088\n",
      "\n",
      "Epoch 120, lr = 0.001277\n",
      "Training loss 3.1064, accuracy 0.3324\n",
      "Eval loss 4.8736, accuracy 0.2078\n",
      "\n",
      "Epoch 121, lr = 0.001272\n",
      "Training loss 3.0957, accuracy 0.3331\n",
      "Eval loss 4.9012, accuracy 0.2082\n",
      "\n",
      "Epoch 122, lr = 0.001267\n",
      "Training loss 3.0884, accuracy 0.3349\n",
      "Eval loss 4.9044, accuracy 0.2070\n",
      "\n",
      "Epoch 123, lr = 0.001261\n",
      "Training loss 3.0841, accuracy 0.3362\n",
      "Eval loss 4.9213, accuracy 0.2058\n",
      "\n",
      "Epoch 124, lr = 0.001256\n",
      "Training loss 3.0722, accuracy 0.3374\n",
      "Eval loss 4.9279, accuracy 0.2057\n",
      "\n",
      "Epoch 125, lr = 0.001251\n",
      "Training loss 3.0782, accuracy 0.3376\n",
      "Eval loss 4.8851, accuracy 0.2061\n",
      "\n",
      "Epoch 126, lr = 0.001246\n",
      "Training loss 3.0740, accuracy 0.3381\n",
      "Eval loss 4.8990, accuracy 0.2058\n",
      "\n",
      "Epoch 127, lr = 0.001241\n",
      "Training loss 3.0698, accuracy 0.3352\n",
      "Eval loss 4.9139, accuracy 0.2085\n",
      "\n",
      "Epoch 128, lr = 0.001237\n",
      "Training loss 3.0595, accuracy 0.3360\n",
      "Eval loss 4.9230, accuracy 0.2089\n",
      "\n",
      "Epoch 129, lr = 0.001232\n",
      "Training loss 3.0593, accuracy 0.3416\n",
      "Eval loss 4.9159, accuracy 0.2030\n",
      "\n",
      "Epoch 130, lr = 0.001227\n",
      "Training loss 3.0513, accuracy 0.3385\n",
      "Eval loss 4.9099, accuracy 0.2083\n",
      "\n",
      "Epoch 131, lr = 0.001222\n",
      "Training loss 3.0474, accuracy 0.3394\n",
      "Eval loss 4.9339, accuracy 0.2068\n",
      "\n",
      "Epoch 132, lr = 0.001218\n",
      "Training loss 3.0559, accuracy 0.3430\n",
      "Eval loss 4.9450, accuracy 0.2023\n",
      "\n",
      "Epoch 133, lr = 0.001213\n",
      "Training loss 3.0501, accuracy 0.3418\n",
      "Eval loss 4.9074, accuracy 0.2045\n",
      "\n",
      "Epoch 134, lr = 0.001209\n",
      "Training loss 3.0326, accuracy 0.3413\n",
      "Eval loss 4.9331, accuracy 0.2073\n",
      "\n",
      "Epoch 135, lr = 0.001204\n",
      "Training loss 3.0325, accuracy 0.3447\n",
      "Eval loss 4.9349, accuracy 0.2047\n",
      "\n",
      "Epoch 136, lr = 0.001200\n",
      "Training loss 3.0281, accuracy 0.3452\n",
      "Eval loss 4.9374, accuracy 0.2042\n",
      "\n",
      "Epoch 137, lr = 0.001195\n",
      "Training loss 3.0323, accuracy 0.3453\n",
      "Eval loss 4.9202, accuracy 0.2047\n",
      "\n",
      "Epoch 138, lr = 0.001191\n",
      "Training loss 3.0181, accuracy 0.3472\n",
      "Eval loss 4.9508, accuracy 0.2034\n",
      "\n",
      "Epoch 139, lr = 0.001187\n",
      "Training loss 3.0126, accuracy 0.3453\n",
      "Eval loss 4.9387, accuracy 0.2065\n",
      "\n",
      "Epoch 140, lr = 0.001182\n",
      "Training loss 3.0154, accuracy 0.3475\n",
      "Eval loss 4.9327, accuracy 0.2046\n",
      "\n",
      "Epoch 141, lr = 0.001178\n",
      "Training loss 3.0150, accuracy 0.3446\n",
      "Eval loss 4.9283, accuracy 0.2065\n",
      "\n",
      "Epoch 142, lr = 0.001174\n",
      "Training loss 3.0004, accuracy 0.3455\n",
      "Eval loss 4.9634, accuracy 0.2068\n",
      "\n",
      "Epoch 143, lr = 0.001170\n",
      "Training loss 3.0067, accuracy 0.3478\n",
      "Eval loss 4.9447, accuracy 0.2041\n",
      "\n",
      "Epoch 144, lr = 0.001166\n",
      "Training loss 2.9989, accuracy 0.3457\n",
      "Eval loss 4.9355, accuracy 0.2077\n",
      "\n",
      "Epoch 145, lr = 0.001162\n",
      "Training loss 2.9980, accuracy 0.3479\n",
      "Eval loss 4.9548, accuracy 0.2058\n",
      "\n",
      "Epoch 146, lr = 0.001158\n",
      "Training loss 2.9783, accuracy 0.3514\n",
      "Eval loss 4.9707, accuracy 0.2040\n",
      "\n",
      "Epoch 147, lr = 0.001154\n",
      "Training loss 2.9819, accuracy 0.3512\n",
      "Eval loss 4.9414, accuracy 0.2065\n",
      "\n",
      "Epoch 148, lr = 0.001150\n",
      "Training loss 2.9926, accuracy 0.3508\n",
      "Eval loss 4.9620, accuracy 0.2032\n",
      "\n",
      "Epoch 149, lr = 0.001146\n",
      "Training loss 2.9800, accuracy 0.3527\n",
      "Eval loss 4.9612, accuracy 0.2033\n",
      "\n",
      "Epoch 150, lr = 0.001142\n",
      "Training loss 2.9737, accuracy 0.3532\n",
      "Eval loss 4.9993, accuracy 0.2014\n",
      "\n",
      "Epoch 151, lr = 0.001138\n",
      "Training loss 2.9778, accuracy 0.3536\n",
      "Eval loss 4.9451, accuracy 0.2038\n",
      "\n",
      "Epoch 152, lr = 0.001135\n",
      "Training loss 2.9777, accuracy 0.3520\n",
      "Eval loss 4.9266, accuracy 0.2074\n",
      "\n",
      "Epoch 153, lr = 0.001131\n",
      "Training loss 2.9658, accuracy 0.3528\n",
      "Eval loss 4.9669, accuracy 0.2050\n",
      "\n",
      "Epoch 154, lr = 0.001127\n",
      "Training loss 2.9586, accuracy 0.3507\n",
      "Eval loss 5.0068, accuracy 0.2071\n",
      "\n",
      "Epoch 155, lr = 0.001124\n",
      "Training loss 2.9610, accuracy 0.3566\n",
      "Eval loss 4.9624, accuracy 0.2026\n",
      "\n",
      "Epoch 156, lr = 0.001120\n",
      "Training loss 2.9598, accuracy 0.3521\n",
      "Eval loss 4.9686, accuracy 0.2067\n",
      "\n",
      "Epoch 157, lr = 0.001116\n",
      "Training loss 2.9683, accuracy 0.3559\n",
      "Eval loss 4.9526, accuracy 0.2017\n",
      "\n",
      "Epoch 158, lr = 0.001113\n",
      "Training loss 2.9419, accuracy 0.3546\n",
      "Eval loss 4.9983, accuracy 0.2060\n",
      "\n",
      "Epoch 159, lr = 0.001109\n",
      "Training loss 2.9479, accuracy 0.3559\n",
      "Eval loss 4.9748, accuracy 0.2049\n",
      "\n",
      "Epoch 160, lr = 0.001106\n",
      "Training loss 2.9598, accuracy 0.3523\n",
      "Eval loss 4.9479, accuracy 0.2069\n",
      "\n",
      "Epoch 161, lr = 0.001103\n",
      "Training loss 2.9459, accuracy 0.3578\n",
      "Eval loss 4.9626, accuracy 0.2038\n",
      "\n",
      "Epoch 162, lr = 0.001099\n",
      "Training loss 2.9412, accuracy 0.3562\n",
      "Eval loss 4.9589, accuracy 0.2068\n",
      "\n",
      "Epoch 163, lr = 0.001096\n",
      "Training loss 2.9384, accuracy 0.3583\n",
      "Eval loss 4.9532, accuracy 0.2056\n",
      "\n",
      "Epoch 164, lr = 0.001092\n",
      "Training loss 2.9238, accuracy 0.3581\n",
      "Eval loss 5.0060, accuracy 0.2053\n",
      "\n",
      "Epoch 165, lr = 0.001089\n",
      "Training loss 2.9330, accuracy 0.3595\n",
      "Eval loss 4.9794, accuracy 0.2036\n",
      "\n",
      "Epoch 166, lr = 0.001086\n",
      "Training loss 2.9231, accuracy 0.3583\n",
      "Eval loss 4.9980, accuracy 0.2046\n",
      "\n",
      "Epoch 167, lr = 0.001083\n",
      "Training loss 2.9206, accuracy 0.3582\n",
      "Eval loss 4.9985, accuracy 0.2060\n",
      "\n",
      "Epoch 168, lr = 0.001079\n",
      "Training loss 2.9190, accuracy 0.3623\n",
      "Eval loss 4.9917, accuracy 0.2034\n",
      "\n",
      "Epoch 169, lr = 0.001076\n",
      "Training loss 2.9345, accuracy 0.3627\n",
      "Eval loss 4.9882, accuracy 0.2008\n",
      "\n",
      "Epoch 170, lr = 0.001073\n",
      "Training loss 2.9082, accuracy 0.3609\n",
      "Eval loss 5.0127, accuracy 0.2051\n",
      "\n",
      "Epoch 171, lr = 0.001070\n",
      "Training loss 2.9096, accuracy 0.3600\n",
      "Eval loss 4.9888, accuracy 0.2054\n",
      "\n",
      "Epoch 172, lr = 0.001067\n",
      "Training loss 2.9007, accuracy 0.3629\n",
      "Eval loss 5.0153, accuracy 0.2032\n",
      "\n",
      "Epoch 173, lr = 0.001064\n",
      "Training loss 2.9054, accuracy 0.3638\n",
      "Eval loss 4.9862, accuracy 0.2038\n",
      "\n",
      "Epoch 174, lr = 0.001061\n",
      "Training loss 2.8960, accuracy 0.3625\n",
      "Eval loss 5.0166, accuracy 0.2048\n",
      "\n",
      "Epoch 175, lr = 0.001058\n",
      "Training loss 2.9040, accuracy 0.3625\n",
      "Eval loss 4.9985, accuracy 0.2035\n",
      "\n",
      "Epoch 176, lr = 0.001054\n",
      "Training loss 2.8973, accuracy 0.3656\n",
      "Eval loss 4.9946, accuracy 0.2023\n",
      "\n",
      "Epoch 177, lr = 0.001052\n",
      "Training loss 2.8914, accuracy 0.3669\n",
      "Eval loss 5.0069, accuracy 0.2018\n",
      "\n",
      "Epoch 178, lr = 0.001049\n",
      "Training loss 2.8929, accuracy 0.3625\n",
      "Eval loss 5.0085, accuracy 0.2054\n",
      "\n",
      "Epoch 179, lr = 0.001046\n",
      "Training loss 2.8830, accuracy 0.3677\n",
      "Eval loss 5.0274, accuracy 0.2024\n",
      "\n",
      "Epoch 180, lr = 0.001043\n",
      "Training loss 2.8878, accuracy 0.3682\n",
      "Eval loss 5.0060, accuracy 0.2018\n",
      "\n",
      "Epoch 181, lr = 0.001040\n",
      "Training loss 2.8839, accuracy 0.3618\n",
      "Eval loss 5.0217, accuracy 0.2073\n",
      "\n",
      "Epoch 182, lr = 0.001037\n",
      "Training loss 2.8846, accuracy 0.3671\n",
      "Eval loss 5.0137, accuracy 0.2019\n",
      "\n",
      "Epoch 183, lr = 0.001034\n",
      "Training loss 2.8788, accuracy 0.3683\n",
      "Eval loss 5.0101, accuracy 0.2021\n",
      "\n",
      "Epoch 184, lr = 0.001031\n",
      "Training loss 2.8806, accuracy 0.3672\n",
      "Eval loss 4.9977, accuracy 0.2032\n",
      "\n",
      "Epoch 185, lr = 0.001029\n",
      "Training loss 2.8710, accuracy 0.3674\n",
      "Eval loss 5.0283, accuracy 0.2041\n",
      "\n",
      "Epoch 186, lr = 0.001026\n",
      "Training loss 2.8759, accuracy 0.3677\n",
      "Eval loss 5.0253, accuracy 0.2019\n",
      "\n",
      "Epoch 187, lr = 0.001023\n",
      "Training loss 2.8704, accuracy 0.3666\n",
      "Eval loss 4.9995, accuracy 0.2057\n",
      "\n",
      "Epoch 188, lr = 0.001020\n",
      "Training loss 2.8603, accuracy 0.3715\n",
      "Eval loss 5.0424, accuracy 0.2012\n",
      "\n",
      "Epoch 189, lr = 0.001018\n",
      "Training loss 2.8705, accuracy 0.3702\n",
      "Eval loss 5.0311, accuracy 0.2004\n",
      "\n",
      "Epoch 190, lr = 0.001015\n",
      "Training loss 2.8545, accuracy 0.3706\n",
      "Eval loss 5.0411, accuracy 0.2026\n",
      "\n",
      "Epoch 191, lr = 0.001012\n",
      "Training loss 2.8586, accuracy 0.3708\n",
      "Eval loss 5.0252, accuracy 0.2029\n",
      "\n",
      "Epoch 192, lr = 0.001010\n",
      "Training loss 2.8612, accuracy 0.3705\n",
      "Eval loss 5.0175, accuracy 0.2032\n",
      "\n",
      "Epoch 193, lr = 0.001007\n",
      "Training loss 2.8532, accuracy 0.3713\n",
      "Eval loss 5.0387, accuracy 0.2022\n",
      "\n",
      "Epoch 194, lr = 0.001004\n",
      "Training loss 2.8489, accuracy 0.3702\n",
      "Eval loss 5.0524, accuracy 0.2040\n",
      "\n",
      "Epoch 195, lr = 0.001002\n",
      "Training loss 2.8525, accuracy 0.3690\n",
      "Eval loss 5.0224, accuracy 0.2055\n",
      "\n",
      "Epoch 196, lr = 0.000999\n",
      "Training loss 2.8335, accuracy 0.3739\n",
      "Eval loss 5.0712, accuracy 0.2018\n",
      "\n",
      "Epoch 197, lr = 0.000997\n",
      "Training loss 2.8433, accuracy 0.3720\n",
      "Eval loss 5.0394, accuracy 0.2035\n",
      "\n",
      "Epoch 198, lr = 0.000994\n",
      "Training loss 2.8360, accuracy 0.3718\n",
      "Eval loss 5.0377, accuracy 0.2058\n",
      "\n",
      "Epoch 199, lr = 0.000992\n",
      "Training loss 2.8373, accuracy 0.3747\n",
      "Eval loss 5.0346, accuracy 0.2024\n",
      "\n",
      "Epoch 200, lr = 0.000989\n",
      "Training loss 2.8471, accuracy 0.3749\n",
      "Eval loss 5.0408, accuracy 0.2000\n",
      "\n",
      "Epoch 201, lr = 0.000987\n",
      "Training loss 2.8370, accuracy 0.3702\n",
      "Eval loss 5.0514, accuracy 0.2051\n",
      "\n",
      "Epoch 202, lr = 0.000984\n",
      "Training loss 2.8452, accuracy 0.3742\n",
      "Eval loss 5.0158, accuracy 0.2015\n",
      "\n",
      "Epoch 203, lr = 0.000982\n",
      "Training loss 2.8320, accuracy 0.3756\n",
      "Eval loss 5.0484, accuracy 0.2019\n",
      "\n",
      "Epoch 204, lr = 0.000979\n",
      "Training loss 2.8360, accuracy 0.3749\n",
      "Eval loss 5.0322, accuracy 0.2025\n",
      "\n",
      "Epoch 205, lr = 0.000977\n",
      "Training loss 2.8400, accuracy 0.3758\n",
      "Eval loss 5.0249, accuracy 0.2006\n",
      "\n",
      "Epoch 206, lr = 0.000975\n",
      "Training loss 2.8196, accuracy 0.3729\n",
      "Eval loss 5.0666, accuracy 0.2035\n",
      "\n",
      "Epoch 207, lr = 0.000972\n",
      "Training loss 2.8162, accuracy 0.3766\n",
      "Eval loss 5.0596, accuracy 0.2036\n",
      "\n",
      "Epoch 208, lr = 0.000970\n",
      "Training loss 2.8246, accuracy 0.3746\n",
      "Eval loss 5.0414, accuracy 0.2036\n",
      "\n",
      "Epoch 209, lr = 0.000968\n",
      "Training loss 2.8237, accuracy 0.3728\n",
      "Eval loss 5.0548, accuracy 0.2048\n",
      "\n",
      "Epoch 210, lr = 0.000965\n",
      "Training loss 2.8281, accuracy 0.3787\n",
      "Eval loss 5.0474, accuracy 0.1992\n",
      "\n",
      "Epoch 211, lr = 0.000963\n",
      "Training loss 2.8200, accuracy 0.3781\n",
      "Eval loss 5.0578, accuracy 0.2006\n",
      "\n",
      "Epoch 212, lr = 0.000961\n",
      "Training loss 2.8039, accuracy 0.3773\n",
      "Eval loss 5.0754, accuracy 0.2037\n",
      "\n",
      "Epoch 213, lr = 0.000959\n",
      "Training loss 2.8198, accuracy 0.3786\n",
      "Eval loss 5.0397, accuracy 0.2010\n",
      "\n",
      "Epoch 214, lr = 0.000956\n",
      "Training loss 2.8137, accuracy 0.3777\n",
      "Eval loss 5.0393, accuracy 0.2024\n",
      "\n",
      "Epoch 215, lr = 0.000954\n",
      "Training loss 2.8030, accuracy 0.3796\n",
      "Eval loss 5.0559, accuracy 0.2023\n",
      "\n",
      "Epoch 216, lr = 0.000952\n",
      "Training loss 2.7999, accuracy 0.3799\n",
      "Eval loss 5.0780, accuracy 0.2018\n",
      "\n",
      "Epoch 217, lr = 0.000950\n",
      "Training loss 2.8033, accuracy 0.3802\n",
      "Eval loss 5.0621, accuracy 0.2012\n",
      "\n",
      "Epoch 218, lr = 0.000947\n",
      "Training loss 2.8049, accuracy 0.3765\n",
      "Eval loss 5.0609, accuracy 0.2042\n",
      "\n",
      "Epoch 219, lr = 0.000945\n",
      "Training loss 2.8065, accuracy 0.3795\n",
      "Eval loss 5.0385, accuracy 0.2014\n",
      "\n",
      "Epoch 220, lr = 0.000943\n",
      "Training loss 2.8000, accuracy 0.3778\n",
      "Eval loss 5.0552, accuracy 0.2046\n",
      "\n",
      "Epoch 221, lr = 0.000941\n",
      "Training loss 2.7929, accuracy 0.3801\n",
      "Eval loss 5.0602, accuracy 0.2040\n",
      "\n",
      "Epoch 222, lr = 0.000939\n",
      "Training loss 2.8004, accuracy 0.3806\n",
      "Eval loss 5.0419, accuracy 0.2020\n",
      "\n",
      "Epoch 223, lr = 0.000937\n",
      "Training loss 2.7900, accuracy 0.3804\n",
      "Eval loss 5.0808, accuracy 0.2027\n",
      "\n",
      "Epoch 224, lr = 0.000935\n",
      "Training loss 2.7918, accuracy 0.3799\n",
      "Eval loss 5.0489, accuracy 0.2036\n",
      "\n",
      "Epoch 225, lr = 0.000933\n",
      "Training loss 2.7874, accuracy 0.3835\n",
      "Eval loss 5.0588, accuracy 0.2007\n",
      "\n",
      "Epoch 226, lr = 0.000931\n",
      "Training loss 2.7975, accuracy 0.3842\n",
      "Eval loss 5.0754, accuracy 0.1975\n",
      "\n",
      "Epoch 227, lr = 0.000929\n",
      "Training loss 2.7940, accuracy 0.3835\n",
      "Eval loss 5.0623, accuracy 0.1986\n",
      "\n",
      "Epoch 228, lr = 0.000926\n",
      "Training loss 2.7895, accuracy 0.3842\n",
      "Eval loss 5.0767, accuracy 0.1997\n",
      "\n",
      "Epoch 229, lr = 0.000924\n",
      "Training loss 2.7912, accuracy 0.3842\n",
      "Eval loss 5.0709, accuracy 0.1993\n",
      "\n",
      "Epoch 230, lr = 0.000922\n",
      "Training loss 2.7727, accuracy 0.3825\n",
      "Eval loss 5.0769, accuracy 0.2035\n",
      "\n",
      "Epoch 231, lr = 0.000920\n",
      "Training loss 2.7767, accuracy 0.3860\n",
      "Eval loss 5.0894, accuracy 0.1988\n",
      "\n",
      "Epoch 232, lr = 0.000918\n",
      "Training loss 2.7799, accuracy 0.3850\n",
      "Eval loss 5.0663, accuracy 0.1996\n",
      "\n",
      "Epoch 233, lr = 0.000916\n",
      "Training loss 2.7723, accuracy 0.3818\n",
      "Eval loss 5.0755, accuracy 0.2037\n",
      "\n",
      "Epoch 234, lr = 0.000915\n",
      "Training loss 2.7645, accuracy 0.3846\n",
      "Eval loss 5.0843, accuracy 0.2031\n",
      "\n",
      "Epoch 235, lr = 0.000913\n",
      "Training loss 2.7746, accuracy 0.3872\n",
      "Eval loss 5.0799, accuracy 0.1984\n",
      "\n",
      "Epoch 236, lr = 0.000911\n",
      "Training loss 2.7692, accuracy 0.3870\n",
      "Eval loss 5.0813, accuracy 0.1989\n",
      "\n",
      "Epoch 237, lr = 0.000909\n",
      "Training loss 2.7747, accuracy 0.3849\n",
      "Eval loss 5.0676, accuracy 0.2015\n",
      "\n",
      "Epoch 238, lr = 0.000907\n",
      "Training loss 2.7627, accuracy 0.3850\n",
      "Eval loss 5.0824, accuracy 0.2025\n",
      "\n",
      "Epoch 239, lr = 0.000905\n",
      "Training loss 2.7686, accuracy 0.3869\n",
      "Eval loss 5.0842, accuracy 0.2006\n",
      "\n",
      "Epoch 240, lr = 0.000903\n",
      "Training loss 2.7499, accuracy 0.3888\n",
      "Eval loss 5.1098, accuracy 0.2004\n",
      "\n",
      "Epoch 241, lr = 0.000901\n",
      "Training loss 2.7628, accuracy 0.3875\n",
      "Eval loss 5.0756, accuracy 0.2004\n",
      "\n",
      "Epoch 242, lr = 0.000899\n",
      "Training loss 2.7521, accuracy 0.3869\n",
      "Eval loss 5.0961, accuracy 0.2025\n",
      "\n",
      "Epoch 243, lr = 0.000897\n",
      "Training loss 2.7541, accuracy 0.3865\n",
      "Eval loss 5.0838, accuracy 0.2020\n",
      "\n",
      "Epoch 244, lr = 0.000896\n",
      "Training loss 2.7670, accuracy 0.3883\n",
      "Eval loss 5.0913, accuracy 0.1986\n",
      "\n",
      "Epoch 245, lr = 0.000894\n",
      "Training loss 2.7595, accuracy 0.3896\n",
      "Eval loss 5.0704, accuracy 0.1993\n",
      "\n",
      "Epoch 246, lr = 0.000892\n",
      "Training loss 2.7585, accuracy 0.3905\n",
      "Eval loss 5.1109, accuracy 0.1968\n",
      "\n",
      "Epoch 247, lr = 0.000890\n",
      "Training loss 2.7623, accuracy 0.3872\n",
      "Eval loss 5.0497, accuracy 0.2017\n",
      "\n",
      "Epoch 248, lr = 0.000888\n",
      "Training loss 2.7465, accuracy 0.3879\n",
      "Eval loss 5.0755, accuracy 0.2033\n",
      "\n",
      "Epoch 249, lr = 0.000887\n",
      "Training loss 2.7498, accuracy 0.3886\n",
      "Eval loss 5.0864, accuracy 0.2007\n",
      "\n",
      "Epoch 250, lr = 0.000885\n",
      "Training loss 2.7522, accuracy 0.3884\n",
      "Eval loss 5.0740, accuracy 0.2022\n",
      "\n",
      "Epoch 251, lr = 0.000883\n",
      "Training loss 2.7439, accuracy 0.3905\n",
      "Eval loss 5.1034, accuracy 0.1987\n",
      "\n",
      "Epoch 252, lr = 0.000881\n",
      "Training loss 2.7426, accuracy 0.3917\n",
      "Eval loss 5.0925, accuracy 0.1979\n",
      "\n",
      "Epoch 253, lr = 0.000880\n",
      "Training loss 2.7471, accuracy 0.3913\n",
      "Eval loss 5.1141, accuracy 0.1963\n",
      "\n",
      "Epoch 254, lr = 0.000878\n",
      "Training loss 2.7388, accuracy 0.3910\n",
      "Eval loss 5.0837, accuracy 0.2008\n",
      "\n",
      "Epoch 255, lr = 0.000876\n",
      "Training loss 2.7626, accuracy 0.3915\n",
      "Eval loss 5.0630, accuracy 0.1964\n",
      "\n",
      "Epoch 256, lr = 0.000874\n",
      "Training loss 2.7421, accuracy 0.3927\n",
      "Eval loss 5.0870, accuracy 0.1975\n",
      "\n",
      "Epoch 257, lr = 0.000873\n",
      "Training loss 2.7323, accuracy 0.3918\n",
      "Eval loss 5.0975, accuracy 0.2007\n",
      "\n",
      "Epoch 258, lr = 0.000871\n",
      "Training loss 2.7400, accuracy 0.3866\n",
      "Eval loss 5.0805, accuracy 0.2034\n",
      "\n",
      "Epoch 259, lr = 0.000869\n",
      "Training loss 2.7209, accuracy 0.3902\n",
      "Eval loss 5.1067, accuracy 0.2036\n",
      "\n",
      "Epoch 260, lr = 0.000868\n",
      "Training loss 2.7318, accuracy 0.3910\n",
      "Eval loss 5.1096, accuracy 0.2010\n",
      "\n",
      "Epoch 261, lr = 0.000866\n",
      "Training loss 2.7463, accuracy 0.3930\n",
      "Eval loss 5.0642, accuracy 0.1999\n",
      "\n",
      "Epoch 262, lr = 0.000864\n",
      "Training loss 2.7337, accuracy 0.3891\n",
      "Eval loss 5.0934, accuracy 0.2028\n",
      "\n",
      "Epoch 263, lr = 0.000863\n",
      "Training loss 2.7382, accuracy 0.3927\n",
      "Eval loss 5.0881, accuracy 0.1989\n",
      "\n",
      "Epoch 264, lr = 0.000861\n",
      "Training loss 2.7319, accuracy 0.3924\n",
      "Eval loss 5.0802, accuracy 0.2011\n",
      "\n",
      "Epoch 265, lr = 0.000859\n",
      "Training loss 2.7272, accuracy 0.3910\n",
      "Eval loss 5.0933, accuracy 0.2022\n",
      "\n",
      "Epoch 266, lr = 0.000858\n",
      "Training loss 2.7200, accuracy 0.3895\n",
      "Eval loss 5.1136, accuracy 0.2028\n",
      "\n",
      "Epoch 267, lr = 0.000856\n",
      "Training loss 2.7193, accuracy 0.3923\n",
      "Eval loss 5.1051, accuracy 0.2013\n",
      "\n",
      "Epoch 268, lr = 0.000855\n",
      "Training loss 2.7222, accuracy 0.3940\n",
      "Eval loss 5.0819, accuracy 0.2011\n",
      "\n",
      "Epoch 269, lr = 0.000853\n",
      "Training loss 2.7237, accuracy 0.3915\n",
      "Eval loss 5.0757, accuracy 0.2032\n",
      "\n",
      "Epoch 270, lr = 0.000851\n",
      "Training loss 2.7176, accuracy 0.3921\n",
      "Eval loss 5.0946, accuracy 0.2023\n",
      "\n",
      "Epoch 271, lr = 0.000850\n",
      "Training loss 2.7176, accuracy 0.3927\n",
      "Eval loss 5.0971, accuracy 0.2012\n",
      "\n",
      "Epoch 272, lr = 0.000848\n",
      "Training loss 2.7287, accuracy 0.3958\n",
      "Eval loss 5.1064, accuracy 0.1958\n",
      "\n",
      "Epoch 273, lr = 0.000847\n",
      "Training loss 2.7185, accuracy 0.3899\n",
      "Eval loss 5.1053, accuracy 0.2034\n",
      "\n",
      "Epoch 274, lr = 0.000845\n",
      "Training loss 2.7058, accuracy 0.3954\n",
      "Eval loss 5.1290, accuracy 0.2000\n",
      "\n",
      "Epoch 275, lr = 0.000844\n",
      "Training loss 2.7104, accuracy 0.3955\n",
      "Eval loss 5.0993, accuracy 0.2008\n",
      "\n",
      "Epoch 276, lr = 0.000842\n",
      "Training loss 2.7043, accuracy 0.3927\n",
      "Eval loss 5.1346, accuracy 0.2024\n",
      "\n",
      "Epoch 277, lr = 0.000841\n",
      "Training loss 2.7110, accuracy 0.3971\n",
      "Eval loss 5.0986, accuracy 0.1996\n",
      "\n",
      "Epoch 278, lr = 0.000839\n",
      "Training loss 2.7097, accuracy 0.3962\n",
      "Eval loss 5.0933, accuracy 0.2003\n"
     ]
    }
   ],
   "source": [
    "training_loop(config[\"num_epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁▃▆█</td></tr><tr><td>step</td><td>▁▃▆█</td></tr><tr><td>train_acc</td><td>▁▂▆█</td></tr><tr><td>train_loss</td><td>█▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▆█</td></tr><tr><td>val_loss</td><td>█▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.00185</td></tr><tr><td>step</td><td>1497</td></tr><tr><td>train_acc</td><td>0.172</td></tr><tr><td>train_loss</td><td>5.13684</td></tr><tr><td>val_acc</td><td>0.16939</td></tr><tr><td>val_loss</td><td>5.18249</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-butterfly-6</strong> at: <a href='https://wandb.ai/lovis/basic-transformer/runs/bil713w8' target=\"_blank\">https://wandb.ai/lovis/basic-transformer/runs/bil713w8</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230420_101212-bil713w8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(transformer.state_dict(), run_data_path + \"/model.pt\")\n",
    "model_artifact.add_file(run_data_path + \"/model.pt\")\n",
    "wandb.log_artifact(model_artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 30]) torch.Size([64, 30])\n",
      "True: doth not wish you joy ! gonzalo . be it so . amen ! re - enter ariel with the master and boatswain <unknown> following . o look , sir\n",
      "Prediction: i appear know his joy . but . i it so . amen . amen - enter ariel with the master and boatswain . . . i , , sir\n",
      "True: yet take this again - and yet i thank you - meaning henceforth to trouble you no more . speed . aside and yet you will ; and yet another\n",
      "Prediction: <unknown> i the <unknown> , <unknown> yet i will you - meaning henceforth to trouble you no more . exeunt . sir ay yet i will not for yet another\n",
      "True: <unknown> : that <unknown> must rise <unknown> that <unknown> him . you know the <unknown> <unknown> the duke has ? iailor . very well . daughter . she is <unknown>\n",
      "Prediction: <unknown> , <unknown> i , be , , <unknown> the from enter have , gentleman of of <unknown> has the iailor . yes well ; daughter . i is <unknown>\n",
      "True: look to taste the due meet for rebellion and such acts as yours . most <unknown> did you these arms <unknown> , fondly brought here and <unknown> sent hence .\n",
      "Prediction: and you the the <unknown> of with rebellion , the acts as these . now <unknown> we you eer three , , and <unknown> up in <unknown> to from ,\n",
      "True: ?— sparrow !— james , theres toys abroad . anon ill tell thee more . exit <unknown> . madam , i was not old sir <unknown> son . sir robert\n",
      "Prediction: . what . james , anon toys abroad . anon , tell thee , . anon . . enter , i tell a old sir <unknown> son . sir robert\n",
      "True: so ; and therefore certainly it were not good she knew his love , lest she make sport at it . hero . why , you speak truth . i\n",
      "Prediction: you , and so , i was not so , knew how love . and she be use at him . troilus . it , then speak well . margaret\n",
      "True: but now i see this one is one too much , and that we have a curse in having her . out on her , <unknown> . nurse . god\n",
      "Prediction: and i i have it <unknown> of the of , <unknown> and i we have a hand . having the . valentine on her ! you ! enter . lady\n",
      "True: ever i will stay if the first hour i shrink and run away . here on my knee i beg mortality , rather than life <unknown> with infamy . talbot\n",
      "Prediction: i i saw stay till ever issue hour come shrink , run away . enter , my knee i have mortality , to than eer doth me infamy . aaron\n",
      "True: say , there was he <unknown> , there <unknown> ins rouse , there falling out at <unknown> : or perchance , ‘ i saw him enter such a house of sale\n",
      "Prediction: , , you is no that , there was ins head , there falling out out the <unknown> there perchance , ‘ there saw him <unknown> , a friend of sale\n",
      "True: page to gardiner queen katherine , wife to king henry , afterwards <unknown> griffith , gentleman <unknown> to queen katherine patience , woman to queen katherine queens gentleman <unknown> <unknown>\n",
      "Prediction: , . gardiner queen katherine , mother to gardiner henry <unknown> mother <unknown> griffith , gentleman , , queen katherine , , gentleman to queen katherine gentleman , , ,\n",
      "True: that if again this apparition come he may approve our eyes and speak to it . horatio . tush , tush , twill not appear . barnardo . sit down\n",
      "Prediction: and i thou be was of to may approve the <unknown> . hear of him . enter . i , tush ! tush not be . hamlet . say down\n",
      "True: pieces of the broken <unknown> were placed the heads of edmund , duke of somerset and william de la pole , first duke of suffolk . this was my dream\n",
      "Prediction: <unknown> of the world empire , placed in <unknown> of the , duke of alençon , william de la pole . one duke of suffolk . buckingham was my dream\n",
      "True: of venice greet you . gives him a packet . othello . i kiss the instrument of their pleasures . opens the packet and reads . desdemona . and whats\n",
      "Prediction: , the , aufidius , gratiano him a packet . othello . o will the instrument of this pleasures . iago him packet . reads . othello . come ,\n",
      "True: , whats the matter ? mrs . page . your husbands coming hither , woman , with all the officers in windsor , to search for a gentleman that he\n",
      "Prediction: , i the matter , mrs . page . o husbands coming hither , woman ; i all the officers of windsor , and the for a <unknown> of thinks\n",
      "True: the way ? emilia . never . othello . to fetch her fan , her gloves , her mask , nor nothing ? emilia . never , my lord .\n",
      "Prediction: , <unknown> of second . i . othello . o fetch her fan , then gloves , her mask , her nothing in emilia . nor , nor lord .\n",
      "True: the odds by <unknown> four . now will i begin your moral , and do you follow with my lenvoy . the fox , the ape , and the humble\n",
      "Prediction: , <unknown> . <unknown> . . the , i shake the moral , and do the follow the an lenvoy . armado fox is sir ape , and the humble\n",
      "True: , <unknown> him their oaths , give him their heirs as pages , followd him even at the heels in golden <unknown> . he presently , as greatness knows itself\n",
      "Prediction: , and , from <unknown> , and him leave heirs , pages , their them to in the heels of the heels . and that , and if is itself\n",
      "True: in the channel . hostess . throw me in the channel ? ill throw thee in the channel . wilt thou , wilt thou , thou <unknown> rogue ? murder\n",
      "Prediction: , the <unknown> . enter . i me in the channel , ill see thee in the channel , ill thou , wilt thou , wilt <unknown> ? ? murder\n",
      "True: with our own tongues ; therefore follow me , and ill direct you how you shall go by him . all . content , content . exeunt citizens menenius .\n",
      "Prediction: . the power tongues . and , him . and take direct you . you shall , . him . menenius . content . content . exeunt citizens menenius ,\n",
      "True: her forth to beasts and birds to prey . her life was beastly and <unknown> of pity , and being dead , let birds on her take pity . exeunt\n",
      "Prediction: the in . <unknown> , birds , prey . enter father is beastly , <unknown> ; her , and <unknown> dead , her her on her <unknown> pity . enter\n",
      "True: have not yet set down this day of triumph . tomorrow , in my judgement , is too sudden , for i myself am not so well provided as else\n",
      "Prediction: are <unknown> <unknown> heard down the day . mine . king , to the mind , we too late , and i have have glad <unknown> simple provided to i\n",
      "True: <unknown> breast of this most <unknown> marriage feast . the cat , with eyne of burning coal , now <unknown> fore the <unknown> hole ; and <unknown> sing at the\n",
      "Prediction: <unknown> of of all day bloody country , , this king , the eyne of burning coal , <unknown> <unknown> with the mountain of , and , , , the\n",
      "True: some pains in writing , he beggd mine , and neither man nor master would take aught but the two rings . portia . what ring gave you , my\n",
      "Prediction: it pains to the . and shall it , and i mine nor master hath have it but that <unknown> rings . portia . i ring was you , my\n",
      "True: lazy - <unknown> clouds and sails upon the bosom of the air . juliet . o romeo , romeo , wherefore art thou romeo ? deny thy father and refuse\n",
      "Prediction: <unknown> - <unknown> bridegroom , <unknown> of the bosom of the earth , the . o , , romeo , thou hast thou romeo ? deny thy father , refuse\n",
      "True: terms it , shall lose the royalty of englands throne . buckingham . withdraw yourself awhile . ill go with you . exeunt richard and buckingham . stanley . we\n",
      "Prediction: is , . and lose it hearts of englands throne . northumberland . i yourself awhile . buckingham tell with you . exit richard and buckingham . richard . my\n",
      "True: good brother . titinius and messala . good night , lord brutus . brutus . farewell , everyone . exeunt cassius , titinius and messala . give me the gown\n",
      "Prediction: and sir . i . messala , brutus night , titinius brutus . brutus . welcome , everyone . exit . and titinius and messala . brutus me the gown\n",
      "True: this barbarous brawl : he that stirs next to carve for his own rage holds his soul light ; he dies upon his motion . silence that dreadful bell ,\n",
      "Prediction: the <unknown> <unknown> , the <unknown> slew his next be on his own <unknown> , his hand , , and <unknown> , his face , he . <unknown> bell ,\n",
      "True: <unknown> it ! <unknown> men indeed , most often , do so near the bottom run by their own fear or <unknown> . sebastian . prithee , say on :\n",
      "Prediction: <unknown> , . i , , , and often have <unknown> not . the <unknown> of away the own suffer , <unknown> <unknown> but . i , say thou :\n",
      "True: is content to spare thee yet ; and , <unknown> as thou art , will lead thee on to gather from thee . haply thou mayst inform something to save\n",
      "Prediction: , <unknown> . be the . . and , as , thou art , thou lead thee to to some from me . <unknown> thou mayst know thyself to do\n",
      "True: , nor proud me no <unknown> , but <unknown> your fine joints gainst thursday next to go with paris to saint <unknown> church , or i will drag thee on\n",
      "Prediction: , and the <unknown> , <unknown> , but i me <unknown> joints , the <unknown> . <unknown> , me back saint <unknown> church , and i will drag thee on\n",
      "True: it you , as much in private , and ill bid adieu . they converse apart katharine . what , was your <unknown> made without a tongue ? longaville .\n",
      "Prediction: your your , sir i as private , the i leave adieu . exit converse apart katharine . now , all it patience in ? a tongue ? longaville .\n",
      "True: the cave . stay , come not in . but that it eats our <unknown> , i should think here were a fairy . guiderius . whats the matter ,\n",
      "Prediction: the <unknown> of king , and , to . king if the eats the cousin , and should think thou were a fairy , i . i the matter ?\n",
      "True: pound of this poor merchants flesh , thou wilt not only loose the <unknown> , but , touchd with human gentleness and love , forgive a moiety of the principal\n",
      "Prediction: <unknown> of flesh land merchants flesh , that <unknown> not have <unknown> <unknown> <unknown> of but i <unknown> with human gentleness and <unknown> , make me moiety of the principal\n",
      "True: ; this to my lord of westmoreland . exit bardolph . go , peto , to horse , to horse , for thou and i have thirty miles to ride\n",
      "Prediction: . the dagger the son of westmoreland . king bardolph . bardolph , peto , to horse , to horse , to thou shalt i will thirty miles to ride\n",
      "True: <unknown> his senseless sword , and when it bows standst up . thou art left , marcius ; a <unknown> <unknown> , as big as thou art , were not\n",
      "Prediction: , , <unknown> <unknown> , and with he <unknown> him thou , enter art a , thou ; and <unknown> , , to a as a art , and a\n",
      "True: eat some part of my leek , or i will <unknown> his pate four days . bite , i pray you ; it is good for your green wound and\n",
      "Prediction: , up pains of it leek , and ill will ride it <unknown> in days . prince , i pray you . i is a . my <unknown> wound .\n",
      "True: . <unknown> <unknown> so himself himself forsook , and died to kiss his shadow in the brook . “ torches are made to light , jewels to wear , <unknown> to\n",
      "Prediction: , exit , , , , , , and <unknown> in be his eyes . the <unknown> . “ <unknown> are painted to beg , and to wear , and <unknown>\n",
      "True: , will sting your hearts . twas men i <unknown> , and you will give them me ; i take it kindly , yet be well assured you put sharp\n",
      "Prediction: , and <unknown> your hearts , i not that that , and i must give them leave leave and will it , , and i <unknown> assured you know sharp\n",
      "True: ; and he that will <unknown> with me for a thousand marks , let him lend me the money , and have at him ! for the box of the\n",
      "Prediction: , and i that <unknown> not his all , the thousand marks of and him not me a <unknown> . and he him him a first he box of the\n",
      "True: remember the first time ever caesar put it on ; twas on a summers evening , in his tent , that day he <unknown> the <unknown> . look , in\n",
      "Prediction: am the <unknown> time , thou found on on ; and on the summers evening , and the tent , and ever he <unknown> , <unknown> of casca , here\n",
      "True: thus far <unknown> you : but i could afflict you further . leontes . do , paulina ; for this affliction has a taste as sweet as any <unknown> comfort\n",
      "Prediction: <unknown> far been , . i i have find you further . valentine . you so paulina , i my is has a taste of well as any <unknown> does\n",
      "True: fate . desdemona . the heavens forbid but that our loves and comforts should increase even as our days do grow ! othello . amen to that , sweet powers\n",
      "Prediction: , , i . i gods forbid it that the loves are comforts are so our as our thoughts !— grow ! othello . o , the , and powers\n",
      "True: ? o theft most base , that we have stoln what we do fear to keep ! but thieves unworthy of a thing so stoln that in their country did\n",
      "Prediction: , i , , base , that ever have <unknown> our we have fear to save . the , unworthy now our little that <unknown> from even the <unknown> doth\n",
      "True: we were free . <unknown> ambition , which <unknown> so much that it did almost stretch the sides o th world , against all colour here did put the yoke\n",
      "Prediction: and shall <unknown> and enter . , and <unknown> to much that even did almost stretch the very of th world , and the colour of we <unknown> the yoke\n",
      "True: , appears he hath had good ancestors . arviragus . how angel - like he sings ! guiderius . but his neat <unknown> ! he cut our roots in characters\n",
      "Prediction: , and in is not intelligence ancestors , he . he ? is like is sings ! arviragus . the how <unknown> beard ! arviragus is his roots off characters\n",
      "True: one <unknown> head , son to the queen , after his own report ; who calld me traitor , <unknown> , and swore with his own single hand hed take\n",
      "Prediction: the of , , and to king queen , and the own report , and , me traitor , and , and swore with his <unknown> report hand hed take\n",
      "True: now , spirit ! whither wander you ? fairy over hill , over <unknown> , thorough bush , thorough <unknown> , over park , over pale , thorough flood ,\n",
      "Prediction: now , malvolio ! whither wander you ? malvolio over park , thorough park , thorough bush , thorough pale , thorough pale , thorough pale , thorough flood ,\n",
      "True: <unknown> of <unknown> . to send donations or determine the <unknown> of <unknown> for any particular state visit <unknown> . gutenberg . <unknown> <unknown> <unknown> while we cannot and do\n",
      "Prediction: <unknown> , the <unknown> <unknown> <unknown> donations in determine of terms of the <unknown> the <unknown> state of the . gutenberg . <unknown> is <unknown> <unknown> the have <unknown> <unknown>\n",
      "True: why , what an ass am i ! this is most brave , that i , the son of a dear father murderd , <unknown> to my revenge by heaven\n",
      "Prediction: i , what a <unknown> am i ? falstaff is not brave , that is am that son of a father father murderd son he , my mother , heaven\n",
      "True: how to choose a man ? care i for the limb , the <unknown> , the <unknown> , bulk , and big <unknown> of a man ? give me the\n",
      "Prediction: and many make the man ? clown i for my limb of i <unknown> , the <unknown> , the , and big <unknown> , the <unknown> , or me thy\n",
      "True: ? is t not enough to break into my garden and , like a thief , to come to rob my grounds , <unknown> my walls in spite of me\n",
      "Prediction: , dromio it not enough to be into thy tent , <unknown> as a <unknown> , to a to seek me grounds , to me <unknown> , my of me\n",
      "True: but the sack that thou hast drunk me would have bought me lights as good cheap at the dearest <unknown> in europe . i have <unknown> that <unknown> of yours\n",
      "Prediction: i , <unknown> is <unknown> hast , , is have bought me lights ; a cheap as the very <unknown> of europe . prince am not the title to thine\n",
      "True: ill follow , sir . but first , ant please the gods , ill hide my master from the flies , as deep as these poor <unknown> can dig ;\n",
      "Prediction: i be , my . exeunt if , ant please you gods , i be you sons from the flies , and deep as steel poor <unknown> are dig the\n",
      "True: <unknown> me , made me neglect my studies , lose my time , war with good counsel , set the world at nought ; made wit with <unknown> weak ,\n",
      "Prediction: thou me , i me neglect of studies with and my life , and , <unknown> counsel , and on form in nought . and wit with <unknown> <unknown> and\n",
      "True: but toys : renown and grace is dead ; the wine of life is drawn , and the mere <unknown> is left this vault to brag of . enter malcolm\n",
      "Prediction: the a , the is <unknown> is <unknown> , and wine is the is spent in and the wine air are not to vault of brag of . the romeo\n",
      "True: and what was he ? forsooth , a great <unknown> , one michael cassio , a florentine , a fellow almost damnd in a fair wife , that never set\n",
      "Prediction: i , say the ? what , he <unknown> <unknown> , a michael cassio , a florentine , a florentine of damnd in him white wench , to thinks set\n",
      "True: master in the door ; who askd them once or twice what they had in their basket . i <unknown> for fear lest the lunatic knave would have <unknown> it\n",
      "Prediction: <unknown> , <unknown> <unknown> . and , them for more twice oer they would before their office . enter knew the fear , the lunatic gods had have <unknown> the\n",
      "True: . a many of our bodies shall no doubt find native graves , upon the which , i trust , shall witness live in brass of this days work ;\n",
      "Prediction: , enter plague of you bodies are be doubt find native courage , the the which , i trust , shall never with in a of this days days .\n",
      "True: don john . fie , fie ! they are not to be namd , my lord , not to be spoke of ; there is not chastity enough in language\n",
      "Prediction: i pedro . what , fie , whats are not to be namd , masters lord , not to be spoke of ; but is no chastity enough for language\n",
      "True: posthumus . this is but a custom in your tongue ; you bear a <unknown> purpose , i hope . iachimo . i am the master of my speeches ,\n",
      "Prediction: i . i is the a custom that my tongue . and shall it brave for too and can , iachimo . i am not dukes of my speeches ,\n",
      "True: thou wilt write to antony ? ventidius . ill humbly signify what in his name , that <unknown> word of war , we have <unknown> ; how , with his\n",
      "Prediction: i art be to me , eros . ay dot signify to i the preparation , which , with of war . i may <unknown> thee and may sir his\n",
      "True: does within , to th <unknown> of her lord . on her left breast a mole <unknown> - spotted , like the crimson drops i th bottom of a <unknown>\n",
      "Prediction: , <unknown> the and make <unknown> of my own . king what , breast , mole <unknown> , spotted , to a crimson drops of th bottom of a <unknown>\n",
      "True: ™ trademark , and any other party <unknown> a project gutenberg ™ electronic work under this agreement , <unknown> all <unknown> to you for <unknown> , <unknown> and <unknown> ,\n",
      "Prediction: ™ trademark , and any other party from with project gutenberg ™ electronic work under the agreement , you all <unknown> with <unknown> and the and <unknown> and <unknown> ,\n",
      "True: you , it is thought you are false knaves . borachio . sir , i say to you we are none . dogberry . well , stand aside . fore\n",
      "Prediction: the . and is the you are not knaves . jaques . i , i have i you . are . . dogberry . i , i aside . dogberry\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for _, data in enumerate(train_dl):\n",
    "    x, y = data[0].to(device), data[1].to(device)\n",
    "    output = transformer(x)\n",
    "\n",
    "    # Flatten batch and sequence dimension\n",
    "    loss = loss_fn(output.view(-1, output.shape[-1]), y.view(-1))\n",
    "    \n",
    "    pred = nn.functional.softmax(output, dim=-1)\n",
    "    pred = pred.argmax(dim=-1)\n",
    "    print(pred.shape, y.shape)\n",
    "    for i in range(pred.shape[0]):\n",
    "      print(\"True:\", \" \".join([index_to_word[word] for word in y[i, :].tolist()]))\n",
    "      print(\"Prediction:\", \" \".join([index_to_word[word] for word in pred[i, :].tolist()]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings = params\n",
    "def generate_next_token(tokens=None):\n",
    "    if tokens == None:\n",
    "        tokens = []\n",
    "    last_token = len(index_to_word)-1\n",
    "    x = tokens + [last_token]\n",
    "    x = torch.LongTensor([x]).to(device)\n",
    "    with torch.no_grad():\n",
    "        y = transformer(x)\n",
    "    # Don't allow the model to generate <unknown> tokens\n",
    "    y = y[:, :, :y.shape[2]-1]\n",
    "    pred = y.argmax(dim=-1).view(-1)\n",
    "    next_word = pred[len(tokens)].item()\n",
    "    return next_word\n",
    "\n",
    "def print_sentence(words):\n",
    "    print(\" \".join([index_to_word[word] for word in words]))\n",
    "\n",
    "generate_next_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then being asked , , , , knowing , knowing , knowing , knowing , in , , , , , , , , , , , , , ,\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(start=None):\n",
    "    if start == None:\n",
    "        sentence = []\n",
    "    else:\n",
    "        words = start.split(\" \")\n",
    "        sentence = [word_to_index[x] for x in words]\n",
    "    \n",
    "    while len(sentence) < max_input_length:\n",
    "        next_word = generate_next_token(sentence)\n",
    "        sentence += [next_word]\n",
    "    \n",
    "    print_sentence(sentence)\n",
    "\n",
    "generate_sentence(\"then being asked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6084])\n",
      "[9, 19, 3899, 0, 27, 9, 19, 9, 9, 9]\n",
      "[',', 'and', 'betimes', 'the', 'with', ',', 'and', ',', ',', ',']\n"
     ]
    }
   ],
   "source": [
    "transformer.eval()\n",
    "test = \"and bring him if . if thou issueless shalt hap\"\n",
    "test = [word_to_index[x] for x in test.split(\" \")]\n",
    "\n",
    "x = torch.LongTensor([test]).to(device)\n",
    "output = transformer(x)\n",
    "pred = F.softmax(output[:, -1, :].view(-1).detach().cpu(), dim=0)\n",
    "dist = torch.distributions.categorical.Categorical(probs=pred)\n",
    "\n",
    "print(pred.shape)\n",
    "pred = dist.sample([10]).tolist()\n",
    "print(pred)\n",
    "print([index_to_word[word] for word in pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
