# basic-transformer

## Training

The current model has 2026752 parameters and took around 2 hours to train using a free Google Colab GPU.

### Training data
The training data can be downloaded [here](https://www.gutenberg.org/files/100/100-0.txt). The file should be placed under ```data/shakespeare.txt```.

## Used resources

I created this repository while loosely following along the recommended steps of Jacob Hilton's transformers curriculum. While implementing, I mainly refered to the original transformer paper and the illustrated transformer blogpost. While I didn't look at the code samples while implementing, I've read through the given implementation in the Natural language processing with transformers book before.

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [GPT-2 Architecture graphic](https://en.wikipedia.org/wiki/GPT-2#/media/File:Full_GPT_architecture.png)
- [Jacob Hilton's Deep Learning Curriculum](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/1-Transformers.md)
- [Natural Language Processing with Transformers book](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) 